---
layout: post
title: MaskNet介绍
description: 
modified: 2022-04-23
tags: 
---

weibo在《MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask》中抽出了MaskNet。

# 摘要

点击率（CTR）预估已成为许多现实世界应用中最基本的任务之一，对于排序模型来说，有效捕捉复杂的高阶特征非常重要。浅层前馈网络在许多最先进的深度神经网络（DNN）模型中被广泛使用，例如FNN、DeepFM和xDeepFM，以隐式捕捉高阶特征交互。然而，一些研究已经证明，**加法特征交互（addictive feature interaction）**，特别是前馈神经网络，在捕捉常见特征交互方面是低效的。

为了解决这个问题，我们通过提出**实例引导掩码（instance-guided mask）**，它引入了特定的乘法操作到DNN排序系统中，该掩码在特征嵌入和前馈层上执行**逐元素乘积（element-wise product）**，由输入实例引导。

我们还通过在本文中提出MaskBlock，将DNN模型中的前馈层转变为加法和乘法特征交互的混合体。MaskBlock结合了层归一化（layer normalization）、实例引导掩码（instance-guided mask）和前馈层，并且是设计新排序模型的基本构建块，可在不同配置下使用。本文中由MaskBlock组成的模型称为MaskNet，提出了**两种新的MaskNet模型**，以展示MaskBlock作为组成高性能排序系统的基本构建块的有效性。

在三个真实世界数据集上的实验结果表明，我们提出的MaskNet模型显著优于DeepFM和xDeepFM等最先进的模型，这意味着MaskBlock是组成新的高性能排序系统的有效基本构建单元。

# 1 引言

点击率（CTR）预测是预测用户点击推荐item的概率。它在个性化广告和推荐系统中扮演着重要角色。许多模型已被提出来解决这个问题，例如逻辑回归（LR）[16]、多项式-2（Poly2）[17]、基于树的模型[7]、基于张量的模型[12]、贝叶斯模型[5]和领域感知分解机（FFMs）[11]。近年来，使用深度神经网络（DNN）进行CTR估计也成为该领域的研究趋势，一些基于深度学习的模型被引入，如分解机支持的神经网络（FNN）[24]、注意力分解机（AFM）[3]、宽与深（W&D）[22]、DeepFM[6]、xDeepFM[13]等。

特征交叉对于CTR任务至关重要，对于排序模型来说，有效捕捉这些复杂特征非常重要。大多数DNN排序模型，如FNN、W&D、DeepFM和xDeepFM，使用浅层MLP层以隐式方式建模高阶交互，并且它是当前最先进的排序系统的重要组成部分。

然而，Alex Beutel等人[2]已经证明，**加法特征交互，特别是前馈神经网络，在捕捉常见特征交叉方面是低效的**。他们提出了一种简单但有效的方法，称为"**隐式交叉（latent cross）**"，这是RNN模型中上下文嵌入和神经网络隐藏状态之间的一种乘法交叉。最近，Rendle等人的工作[18]也表明，**一个精心配置的点积基线在协同过滤中大大优于MLP层**。虽然MLP理论上可以近似任何函数，但他们表明，使用MLP学习点积并非易事，并且对于一个相当大的嵌入维度，以高准确度学习点积需要大量的模型容量和许多训练数据。他们的工作还证明了**MLP层在建模复杂特征交互方面的低效性**。

受到"隐式交叉"[2]和Rendle的工作[18]的启发，我们关注以下问题：我们能否通过引入特定的乘法操作来改进DNN排序系统，使其有效地捕捉复杂的特征交互？

为了克服前馈层捕捉复杂特征交叉的低效性问题，我们在本文中引入了一种特殊的乘法操作到DNN排序系统中。首先，我们提出了一个**实例引导掩码**，在特征嵌入和前馈层上执行逐元素乘（element-wise product）。实例引导掩码利用**从输入实例收集的全局信息**，以统一的方式动态突出特征嵌入和隐藏层中的信息元素。

采用实例引导掩码有两个主要优点：

- 首先，掩码和隐藏层或特征嵌入层之间的**逐元素乘积**以统一的方式将乘法操作引入DNN排序系统，更有效地捕捉复杂特征交叉。
- 其次，这是一种由输入实例引导的**细粒度位级注意力（finegained b bitwise attention）**，既可以减弱特征嵌入和MLP层中的噪声影响，同时突出DNN排序系统中的信息信号。

通过结合实例引导掩码、前馈层和**层归一化(layer norm)**，我们提出了MaskBlock，将通常使用的前馈层转变为加法和乘法特征交互的混合体。

- 实例引导掩码（instance-guided mask）引入了乘法交叉，
- 前馈层（feed-forward layers）聚合了掩蔽信息，以更好地捕捉重要的特征交互
- 层归一化（layer normalization）可以简化网络的优化

MaskBlock可以被视为在某些配置下设计新排序模型的基本构建块。本文中由MaskBlock组成的模型称为MaskNet，提出了两种新的MaskNet模型，以展示MaskBlock作为组成高性能排序系统的基本构建块的有效性。

我们工作的成果总结如下：
- (1) 在这项工作中，我们提出了一个**实例引导掩码**，在DNN模型的特征嵌入和前馈层上执行逐元素乘积。实例引导掩码中包含的全局上下文信息被动态地整合到特征嵌入和前馈层中，以突出重要元素。
- (2) 我们提出了一个**名为MaskBlock的基本构建块**，它由三个关键组件组成：实例引导掩码、前馈层和层归一化模块。通过这种方式，我们将标准DNN模型中广泛使用的前馈层转变为加法和乘法特征交互的混合体。
- (3) 我们还提出了一个**新的排序框架，名为MaskNet**，利用MaskBlock作为基本构建单元来组成新的排序系统。更具体地说，本文设计了基于MaskBlock的串行MaskNet模型和并行MaskNet模型。串行排序模型逐块堆叠MaskBlock，而并行排序模型在共享特征嵌入层上并行放置多个MaskBlocks。
- (4) 在三个真实世界数据集上进行了广泛的实验，实验结果表明我们提出的两个MaskNet模型显著优于现有最先进模型。结果暗示MaskBlock确实通过实例引导掩码将乘法操作引入DNN模型，从而增强了DNN模型捕捉复杂特征交互的能力。

本文的其余部分组织如下。第2节介绍了与我们提出的模型相关的一些相关工作。第3节详细介绍了我们提出的模型。第4节展示了三个真实世界数据集上的实验结果并进行了讨论。第5节总结了本文的工作。

# 2.相关工作

略

# 3 我们提出的模型

在本节中，我们首先描述特征嵌入层。然后，将介绍我们提出的实例引导掩码、MaskBlock和MaskNet结构的细节。最后，将介绍作为损失函数的对数损失函数（log loss）。

## 3.1 嵌入层

CTR任务的输入数据通常包括稀疏和密集特征，其中稀疏特征大多是分类类型。这些特征被编码为one-hot向量，这通常会导致对于大词汇量来说特征空间维度过高。解决这个问题的常见方法是引入嵌入层。通常，稀疏输入可以表述为：

$$
x = [x_1, x_2, ..., x_f] \quad (1)
$$

其中：

- f表示fields的数量
- $ x_i \in \mathbb{R}^n $ 表示一个具有n个特征的categorical field的one-hot向量；对于一个numerical field，它是只有一个值的向量

我们可以通过以下方式为one-hot向量$x_i$获得特征嵌入$e_i$：

$$
e_i = W_e x_i \quad (2)
$$

其中：

- $W_e \in \mathbb{R}^{k \times n} $ 是n个特征的嵌入矩阵，k是字段嵌入的维度。

数值特征 $x_j$ 也可以通过以下方式转换到相同的低维空间：

$$
e_j = V_j x_j \quad (3)
$$


其中:

- $ V_j \in \mathbb{R}^k $是对应字段嵌入，大小为k。

通过上述方法，嵌入层应用于原始特征输入，将其压缩到低维、dense的实值向量。嵌入层的结果是宽连接向量：

$$
V_{emb} = \text{concat}(e_1, e_2, ..., e_i, ..., e_f) \quad (4)
$$

其中：

- f 表示fields数量
- $ e_i \in \mathbb{R}^k $表示一个字段的嵌入。

尽管输入实例的特征长度可能不同，但它们的嵌入长度相同，为$ f \times k $，其中k是字段嵌入的维度。

我们使用实例引导掩码将乘法操作引入DNN排序系统，这里的所谓"实例"在本文的后续部分指的是当前输入实例的特征嵌入层。

## 3.2 实例引导掩码

我们通过实例引导掩码从输入实例中收集全局信息，以**动态突出特征嵌入（feature embedding）和前馈层（feed-forward layer）中的有信息的元素**。对于特征嵌入，掩码强调具有更多信息的关键元素，以有效表示这一特征。对于隐藏层中的神经元，掩码通过考虑输入实例中的上下文信息，帮助重要的特征交互脱颖而出。除了这一优势外，实例引导掩码还将乘法操作引入DNN排序系统，以更高效地捕获复杂的特征交叉。

如图1所示，实例引导掩码中使用了两个具有恒等函数的全连接（FC）层。请注意，实例引导掩码的输入始终来自输入实例，也就是说，来自特征嵌入层。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/4060a4852c53efc92529a2e94482240999560637d07f28efe0a0a20fa67f018c54de88e74a8bec1ed756aab6b20dc61d?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 Instance-Guided Mask的网络结构

第一层FC层称为“聚合层”，与第二层FC层相比，它是一个相对更宽的层，以便更好地收集输入实例中的全局上下文信息。聚合层有参数 $W_{d1}$，这里d表示第d个掩码。对于特征嵌入和不同的MLP层，我们采用具有其参数的不同实例引导掩码，以从输入实例中学习捕获每层的各种信息。

第二层FC层称为“投影层”，它将维度降低到与特征嵌入层$ V_{emb}$ 或隐藏层$V_{hid}$相同的大小，参数为$W_{d2}$。形式上，

$$
V_{mask} = W_{d2}(\text{ReLU}(W_{d1} V_{emb} + \beta_{d1})) + \beta_{d2} \quad (5)
$$

其中：

- $ V_{emb} \in \mathbb{R}^{m=f \times k} $：指的是输入实例的嵌入层，f表示fields的数量，k是对应fields的嵌入维度。
- $ W_{d1} \in \mathbb{R}^{t \times m}$ 和 $ W_{d2} \in \mathbb{R}^{z \times t}$ 是实例引导掩码的参数， t和 z分别表示聚合层和投影层的神经元数量，
- $ \beta_{d1} \in \mathbb{R}^{t \times m} $ 和 $ \beta_{d2} \in \mathbb{R}^{z \times t} $是两个FC层的学习偏置。

请注意，**聚合层通常比投影层宽，因为投影层的大小需要与特征嵌入层或MLP层的大小相等**。因此，我们定义了大小 $ r = t/z $ 作为缩减比率，这是一个超参数，用于控制两层神经元数量的比率。

逐元素乘积在此工作中被用来将实例引导掩码聚合的全局上下文信息整合到特征嵌入或隐藏层，如下所示：

$$
V_{mask_{emb}} = V_{mask} \odot V_{emb} \\
V_{mask_{hid}} = V_{mask} \odot V_{hid} \quad (6)
$$

其中：

- $ V_{emb} $ 表示嵌入层
- $ V_{hid} $ 表示DNN模型中的前馈层
- $ \odot $ 表示两个向量之间的逐元素乘积，

如下所示：

$$
V_i \odot V_j = [V_{i1} \cdot V_{j1}, V_{i2} \cdot V_{j2}, ..., V_{iu} \cdot V_{ju}] \quad (7)
$$

这里:

- u 是向量 $V_i$ 和 $ V_j$的大小。

实例引导掩码可以被看作是一种**特殊类型的位级注意力或门控机制**，它使用输入实例中包含的全局上下文信息来指导训练期间的参数优化。$ V_{mask}$ 中的较大值意味着模型动态识别特征嵌入或隐藏层中的一个重要元素。它被用来增强向量 $ V_{emb}$ 或 $ V_{hid} $ 中的元素。相反，$ V_{mask} $中的较小值将通过减少对应向量 $ V_{emb} $或 $ V_{hid}$中的值来抑制信息较少的元素甚至噪声。

采用实例引导掩码的两个主要优点是：

- 首先，掩码和隐藏层或特征嵌入层之间的逐元素乘积**以统一的方式将乘法操作引入DNN排序系统**，更有效地捕捉复杂特征交互。
- 其次，这种由输入实例引导的细粒度位级注意力可以同时减弱特征嵌入和MLP层中的噪声影响，同时突出DNN排序系统中的有信息量的信号。

## 3.3 掩码块

为了解决前馈层在深度神经网络（DNN）模型中捕捉复杂特征交互的效率问题，我们在这项工作中提出了一个名为掩码块（MaskBlock）的基本构建模块，用于DNN排序系统，如图2和图3所示。所提出的掩码块由三个关键组件组成：层归一化模块、实例引导掩码和前馈隐藏层。层归一化可以简化网络的优化。实例引导掩码为标准DNN模型的前馈层引入了乘法交互，并使前馈隐藏层聚合掩码信息，以更好地捕捉重要的特征交互。通过这种方式，我们将标准DNN模型中广泛使用的前馈层转变为一种加法和乘法特征交互的混合体。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/8cffa9e742531366eb247cf6adb378f61d3aa2cd121f059d2a88d9f0c1e086d81ed0fa46a9c01d1b93413571ea509293?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2

首先，我们简要回顾一下层归一化（LayerNorm）的公式。

**层归一化**：

通常，归一化的目的是确保信号在通过网络传播时具有零均值和单位方差，以减少“协变量偏移”[10]。例如，层归一化（Layer Norm或LN）[1]被提出以简化循环神经网络的优化。具体来说，设 $ \mathbf{x} = (x_1, x_2, ..., x_H) $ 表示大小为 H 的输入向量到归一化层。层归一化将输入 $ \mathbf{x} $ 重新中心化和重新缩放，公式如下：

$$
h = g \odot \mathcal{N}(x) + b, \quad \mathcal{N}(x) = \frac{x - \mu}{\delta}, \\
\mu = \frac{1}{H} \sum_{i=1}^{H} x_i, \quad \delta = \sqrt{\frac{1}{H} \sum_{i=1}^{H} (x_i - \mu)^2} 
$$

其中：

- h是层归一化层的输出。
- $\odot$是逐元素乘法操作。
- $\mu$ 和 $\delta$ 分别是输入的均值和标准差。
- 偏置b和增益g是具有相同维度H的参数。

作为掩码块（MaskBlock）的关键组件之一，层归一化可以应用于特征嵌入和前馈层。对于特征嵌入层，我们将每个特征的嵌入视为一个层，以如下方式计算LN的均值、标准差、偏置和增益：

$$
\text{LN}_{\text{EMB}}(V_{\text{emb}}) = \text{concat} \left( \text{LN}(e_1), \text{LN}(e_2), ..., \text{LN}(e_i), ..., \text{LN}(e_f) \right) 
$$
...(9)

对于DNN模型中的前馈层，LN的统计数据是在相应隐藏层中包含的神经元之间估计的，如下所示：

$$
\text{LN}_{\text{HID}}(V_{\text{hidden}}) = \text{ReLU}(\text{LN}(W_i X)) 
$$

其中：

- $X \in \mathbb{R}^t$ 指的是前馈层的输入
- $W_i \in \mathbb{R}^{m \times t}$ 是层的参数，
- t和m分别表示输入层的大小和前馈层的神经元数量。

请注意，我们在多层感知器（MLP）上有两处可以放置归一化操作：一处是在非线性操作之前，另一处是在非线性操作之后。我们发现，在非线性之前进行归一化的性能始终优于在非线性之后进行归一化的性能。因此，在我们的论文中，MLP部分使用的所有归一化都放在非线性操作之前，如公式（4）所示。

**特征嵌入上的掩码块**：

我们通过结合三个关键元素：层归一化、实例引导掩码和随后的前馈层，提出了掩码块。掩码块可以堆叠形成更深的网络。根据每个掩码块的不同输入，我们有两种掩码块：特征嵌入上的掩码块和掩码块上的掩码块。我们将首先介绍本小节中图2所示的特征嵌入上的掩码块。

特征嵌入 $ V_{\text{emb}} $ 是特征嵌入上掩码块的唯一输入。在对嵌入 $ V_{\text{emb}} $ 进行层归一化操作后，掩码块利用实例引导掩码通过逐元素乘法突出显示 $ V_{\text{emb}} $ 中的信息元素，形式上表示为：

$$
V_{\text{maskedEmb}} = V_{\text{mask}} \odot \text{LN}_{\text{EMB}}(V_{\text{emb}})
$$

其中：

- $\odot$ 表示实例引导掩码和归一化向量 $ \text{LN}_{\text{EMB}}(V_{\text{emb}}) $ 之间的逐元素乘法，
- $ V_{\text{maskedEmb}} $表示掩码特征嵌入。

请注意，实例引导掩码 $ V_{\text{mask}} $ 的输入也是特征嵌入 $ V_{\text{emb}} $。

我们引入了一个前馈隐藏层以及随后的层归一化操作到掩码块中，通过归一化的非线性变换更好地聚合掩码信息。掩码块的输出可以按以下方式计算：

$$
V_{\text{out}} = \text{LN}_{\text{HID}}(W_i V_{\text{maskedEmb}}) = \text{ReLU}(\text{LN}(W_i (V_{\text{mask}} \odot \text{LN}_{\text{EMB}}(V_{\text{emb}})))))
$$

其中：

- $ W_i \in \mathbb{R}^{q \times n} $ 是第 i 个掩码块中前馈层的参数，
- n表示 $ V_{\text{maskedEmb}} $ 的大小，
- q表示前馈层的神经元数量。

实例引导掩码将逐元素乘法引入特征嵌入中，作为一种细粒度的注意力机制，而特征嵌入和隐藏层上的归一化都简化了网络优化。掩码块中的这些关键组件帮助前馈层更有效地捕获复杂的特征交叉。

**掩码块上的掩码块**：

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/58bffb86e74ddfd574c995f596e2ebc62def8b8b916342a03e0057b392b251adc09f91127c1592f3b7c7fbcb8a273595?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3 

在这一部分，我们将介绍如图3所示的掩码块上的掩码块。这种掩码块有两种不同的输入：

- 特征嵌入 $ V_{\text{emb}} $
- 前一个掩码块的输出 $ V_{\text{out}}^{(p)} $

这种掩码块的实例引导掩码的输入始终是特征嵌入 $V_{\text{emb}}$。掩码块利用实例引导掩码通过逐元素乘法突出前一个掩码块输出 $ V_{\text{out}}^{(p)} $中的重要特征交互，形式上表示为：

$$
V_{\text{maskedHid}} = V_{\text{mask}} \odot V_{\text{out}}^{(p)}
$$

其中：

- $ \odot $ 表示实例引导掩码 $ V_{\text{mask}} $ 和前一个掩码块的输出 $ V_{\text{out}}^{(p)} $ 之间的逐元素乘法
- $ V_{\text{maskedHid}} $ 表示掩码隐藏层。

为了更好地捕获重要的特征交互，掩码块中又引入了另一个前馈隐藏层以及随后的层归一化。通过这种方式，我们将标准DNN模型中广泛使用的前馈层转变为一种加法和乘法特征交互的混合体，以避免那些加法特征交叉模型的无效性。掩码块的输出可以按以下方式计算：

$$
V_{\text{out}} = \text{LN}_{\text{HID}}(W_i V_{\text{maskedHid}}) = \text{ReLU}(\text{LN}(W_i (V_{\text{mask}} \odot V_{\text{out}}^{(p)}))))
$$
...(14)

## 3.4 掩码网络（MaskNet）

基于掩码块（MaskBlock），根据不同的配置，可以设计出各种新的排序模型。由掩码块组成的排序模型在这项工作中被称为掩码网络（MaskNet）。我们还提出了两种使用掩码块作为基本构建模块的掩码网络模型。

**序列掩码网络（Serial MaskNet）**：

我们可以将一个掩码块堆叠在另一个掩码块之后来构建排序系统，如图4左侧模型所示。第一个块是特征嵌入上的掩码块，所有其他块都是掩码块上的掩码块，形成更深的网络。预测层放置在最终掩码块的输出向量上。我们在论文中将这种序列配置下的掩码网络称为序列掩码网络（SerMaskNet）。每个掩码块中实例引导掩码的所有输入都来自特征嵌入层 $ V_{\text{emb}} $，这使得序列掩码网络模型看起来像是一个在每个时间步共享输入的RNN模型。

**并行掩码网络（Parallel MaskNet）**：

我们提出另一种掩码网络，通过在共享的特征嵌入层上并行放置几个掩码块，如图4右侧模型所示。在这种配置下，每个块的输入仅是共享的特征嵌入 $ V_{\text{emb}} $。我们可以将这种排序模型视为像MMoE[15]那样由多个专家混合而成。每个掩码块关注特定类型的重要特征或特征交互。我们通过连接每个掩码块的输出来收集每个专家的信息，如下所示：

$$
V_{\text{merge}} = \text{concatenate}(V_{\text{out},1}, V_{\text{out},2}, ..., V_{\text{out},i}, ..., V_{\text{out},u})
$$
...(14)

其中：

- $ V_{\text{out},i} \in \mathbb{R}^q $ 是第i个掩码块的输出
- q表示掩码块中前馈层的神经元数量
- u是掩码块的数量。

为了进一步合并每个专家捕获的特征交互，多个前馈层堆叠在连接信息 $ V_{\text{merge}} $上。设 $ H_0 = V_{\text{merge}} $ 表示连接层的输出，然后 $H_0$ 被送入深度神经网络，前馈过程为：

$$
H_l = \text{ReLU}(W_l H_{l-1} + \beta_l)
$$
...(16)

其中：

- l是深度，
- ReLU是激活函数。
- $ W_l$，$ \beta_l $，$ H_l $ 分别是第l层的模型权重、偏置和输出。预测层放置在多个前馈网络的最后一层。

在本文的后续部分，我们称这个版本的掩码网络为“并行掩码网络”（ParaMaskNet）。

## 3.5 预测层

总结来说，我们给出了我们提出的模型输出的总体公式如下：

$$
\hat{y} = \delta(w_0 + \sum_{i=1}^{n} w_i x_i)
$$
...(17)

其中：

- $ \hat{y} \in (0, 1) $ 是预测的点击率（CTR）值，
- $ \delta $ 是Sigmoid函数，
- $ n $ 是最后一个掩码块的输出大小（序列掩码网络SerMaskNet）或前馈层（并行掩码网络ParaMaskNet），
- $ x_i $ 是前馈层的位值，
- $ w_i $ 是每个位值学习到的权重。

对于二元分类，损失函数是日志损失：

$$
L = -\frac{1}{N} \sum_{i=1}^{N} y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)
$$
...(18)

其中：

- $ N $ 是训练实例的总数，
- $y_i$ 是第i个实例的真实标签，
- $\hat{y}_i $是预测的CTR。

优化过程是最小化以下目标函数：

$$
\mathcal{E} = L + \lambda \| \Theta \| 
$$

其中：

- $ \lambda $ 表示正则化项，
- $ \Theta $ 表示参数集，包括特征嵌入矩阵中的参数、实例引导掩码矩阵中的参数、掩码块中的前馈层参数，以及预测部分的参数。

- 1.[https://arxiv.org/pdf/2102.07619](https://arxiv.org/pdf/2102.07619)