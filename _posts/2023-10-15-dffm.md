---
layout: post
title: DFFM多场景建模介绍
description: 
modified: 2023-10-12
tags: 
---

华为在《DFFM: Domain Facilitated Feature Modeling for CTR Prediction》中提出了一种DFFM的多场景建模方法。

# 4.方法

## 4.1 整体结构

域辅助特征建模(DFFM：Domain Facilitated Feature Modeling)的总体框架可分为两个主要模块：

- 域辅助特征交叉(DFFI)：Domain Facilitated Feature Interaction
- 域辅助用户行为(DFUB)：Domain Facilitated User Behavior

我们提出的DFFM概述如图2所示。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/8d60f3f2867c4b35c6c1d85c7a3737e2f8b66fa79cd31c964f10eab91b0b78361e6fd1482500b34a576076368aac26d4?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2

- DFFI模块：负责提供有价值的domain信息来支持特征交叉建模，
- DFUB模块：利用domain和目标知识来更好地建模用户行为。

这两个模块的详细设计将在下一小节中说明。

## 4.2 DFFI

特征交叉建模在CTR预测中是至关重要的，例如PNN、AutoInt和DCN。然而，所有这些模型都忽略了考虑领域（domain）信息的重要性。在不同领域中的特征交叉应该有不同的建模方式，例如，在某些领域中，一些特征交叉非常重要，而在其他领域中则毫无意义。因此，我们提出了域辅助特征交互(DFFI)模块，可以对任何特征交叉模型进行操作，引入领域（domain）信息来辅助交叉建模。

以PNN [18]中的内积为例，我们的域增强内积（domainenhanced inner product）可以表示为：

$$
F_{domain}(e_i, e_j) = <e_i, e_j>_{domain}, \\
=<D(e_i), D(e_j)>
$$

...(2)

其中：

- $e_i$和$e_j$是第𝑖个和第𝑗个字段的embedding；
- ⟨·，·⟩是内积；
- D(·)是用于融合domain信息的domain network，它是一个micro-MLP，可以表示为：

$$
h^{(0)} = h_{input}, \\
h^{(k)} = \sigma(W^{(k-1)} h^{(k-1)} + b^{(k-1)}), k \in [1, \cdots, L], \\
D(h_{input}) = h^{(L)}
$$

...(3)(4)(5)

其中：

- $h_{𝑖𝑛𝑝𝑢𝑡}$是输入向量
- 𝜎是激活函数
- 𝐿是领域网络（domain network）的深度

特别地，权重和偏置参数$W^{(k)}$和$b^{(k)}$是通过重塑(reshape)和分割(split)来将domain embedding进行concatenate来生成的。形式上，权重和偏置参数可以表示为：

$$
W^{(k)}, b^{(k)} = Reshape(Split(E^d))
$$

...(6)

其中：

- $E_𝑑$是所有domain embedding的拼接。该过程的可视化示例如图2所示。需要注意的是，PNN仅对2阶特征交互进行建模。对于具有高阶特征交互的模型，如DCN和AutoInt，我们应该将domain network应用于每阶的特征交互的输入。

有了该 domain-enhanced inner product，接着我们可以生成 domain-enhance feature interaction layer：

$$
h_{domain} = Concat(F_{domain}(e_1, e_2), \cdots, F_{domain}(e_n, e_{n-1})) \\
h_{DFFI} = MLP(Concat(h_{domain}, E))
$$

...(7)(8)

其中：

- $h_{𝐷𝐹𝐹𝐼}$是DFFI模块的输出表示；
- 𝑛是字段的数量；
- E是所有字段的拼接嵌入。

通过动态加权网络，特征交互的建模考虑了领域知识。同样的方法不仅适用于内积，还适用于其他特征交互模型，例如AutoInt中的自注意机制或DCN中的交叉网络，这将在第5.4.2节中得到验证。

## 4.3 DFUB

用户行为建模在CTR预测中起着重要作用[12]。当前的方法，如DIN和CAN，侧重于建模目标物品（target items）和用户历史序列之间的co-action关系。尽管这些方法在特定领域取得了成功，但我们强调考虑建模用户行为的领域相关信息（domain related information）的重要性。例如，**用户在售前（pre-sales）和售后领域（post-sales）之间的购买兴趣迅速变化，这表明他们的行为历史对点击行为的共同作用模式不同**。为了明确考虑这种现象，我们提出了域辅助用户行为建模（Domain Facilitated User Behavior modeling），简称DFUB。

DFUB采用**修改版的多头自注意机制（modified multi-head self-attention）来处理用户历史序列**。

假设：

- 用户行为历史的embedding可以表示为：$E^h=\lbrace e_1^h，e_2^h，\cdots，e_n^h}$，其中𝑛是行为历史长度。
- 目标物品（target item）表示为：$E^t=\lbrace e^t \rbrace$
- 领域特征（domain features）可以表示为：$E^𝑑=\lbrace 𝒆^𝑑 \rbrace$

对于第𝑖个head，标准的self-attention模块处理可以表示为公式9。

$$
head_i = SelfAttn_{\theta(E^t, E^d)}^{(i)}(E^h) = Softmax(\frac{Q_iK_i^T}{\sqrt{d_i}}) V_i
$$

...(9)

在公式9中：

- $𝑑_𝑖$是第𝑖个头的输出维度
- $Θ(E_𝑡，E_𝑑)$表示使用$E_𝑡$，$E_𝑑$来获取self-attention模块的参数。
- $Q_𝑖, K_i, V_i$分别表示：指query矩阵、key矩阵和value矩阵。这三个矩阵都是从行为序列embedding $E_𝒉$计算得出的，如公式10所示。

$$
Q_i = E^h W_Q^{(i)}, K_i = E^h W_K^{(i)}, V_i = E^h W_V^{(i)}
$$

...（10）

其中，$W_𝑄^{(𝑖)}，W_𝐾^{(𝑖)}，W_𝑉^{(𝑖)}是转换矩阵(transformation matrix)。到目前为止，所呈现的计算过程都是关于序列行为items的内部交叉。该目标是与target item和历史行为items间充分交叉，并利用domain information来指导这种交叉。我们通过根据公式11将目标嵌入E𝑡和领域嵌入E𝑑直接整合到自注意参数中来实现这一目标。

$$
W_𝑄^{(𝑖)}=W_𝑄^{(𝑖)}′E_𝑄^⊤, E_𝑄=Reshape(E^𝑡,E^𝑑) \\
W_𝐾^{(𝑖)}=W_𝐾^{(𝑖)}′E_K^⊤, E_K=Reshape(E^𝑡,E^𝑑) \\
W_𝑉^{(𝑖)}=W_𝑉^{(𝑖)}′E_V^T, E_V=Reshape(E^𝑡,E^𝑑)
$$

...(11)

如公式11所示，自注意力的参数可以分解为两部分。第一部分(W(𝑖)𝑄′，W(𝑖)𝐾′，W(𝑖)𝑉′)是自注意模块的固有参数。第二部分(E𝑄，E𝐾，E𝑉)是从目标项和与领域相关的特征嵌入中重塑的。通过这种方式，目标项和领域特征的信息可以逐个元素地参与到交互过程中，并得到彻底的处理。多头拼接组成了一个综合的多头自注意层，可以拼接成DFUB模块的最终输出，如公式12所示，其中W𝑜𝑢𝑡是输出转换矩阵：

$$
h_{𝐷𝐹𝑈𝐵}=Concat(head_1, \cdots, head_𝑛)W^{𝑜𝑢𝑡}
$$

...(12)

## 4.4 Prediction Layer和Optimization

然后，我们可以将DFFI和DFUB模块的输出表示拼接在一起，以获得：

$$
h_{𝑜𝑢𝑡𝑝𝑢𝑡}=Concat(h_{D𝐷𝐹𝐼},h_{𝐷𝐹𝑈𝐵})
$$

...（13）

使用带有Sigmoid函数的线性层来获得最终预测：

$$
\hat{y} = Sigmoid(W_o h_{output} + b_o)
$$

...(14)

其中，$W_𝑜$和$b_𝑜$是权重和偏置参数。我们采用广泛使用的交叉熵损失作为目标函数。

# 

- 1.[https://dl.acm.org/doi/pdf/10.1145/3583780.3615469](https://dl.acm.org/doi/pdf/10.1145/3583780.3615469)