---
layout: post
title: SENet介绍
description: 
modified: 2021-09-07
tags: 
---

Momenta公司在《Squeeze-and-Excitation Networks》提出了一种SENet的结构。我们来看下paper介绍：

# 1.介绍

CNNs已经被证明在解决许多视觉任务中是有用的模型。在CNN网络中的每个conv layer上，所使用的filters的集合可以用来表示：沿着input channels—的**相邻空间连接模式（neighbourhood spatial connectivity patterns）**——会在局部感受野（local receptive fields）内将**空间信息**和**channel-wise信息**融合（fusing）在一起。通过使用非线性激活函数和下采样操作符，与一系列conv layers进行交叉，CNNs可以生成图片表示（image representations），同时能捕获层级模式并获得global theoretical receptive fields。计算机视觉研究的一个中心主题是：关于许多强大的representations探索，只会对于一个给定任务，通过捕获一张图片的最显著属性来提升效果。对于视觉任务广泛使用的模型族来说，新的NN结构设计的发展，是一个前沿领域。最近研究表明，由CNNs生成的representations可以通过将学习机制（learning mechanisms）集合到网络中的方式来得到增强，可以捕获features间的空间相关性。这样的研究有：Inception家族，它会包含multi-scale过程到网络模块中，来达到效果提升。最近一些研究工作则会更好地建模空间依赖，并将spatial attention包含到网络结构中。

在本paper中，**我们研究了【network设计-—channels间的关系】的不同方面**。我们会引入一个新的结构单元，术语称为“Squeeze-andExcitation (SE) block”，它会通过显式建模convolutional  features间的相互依赖，生成的representations会提升quality。为了这个结果，我们提出了一个机制，它允许该网络执行特征再校准（feature recalibration），尽管它可以学习使用全局信息来选择性地增强有信息量的features，并消除更少信息量的features。

SE building block的结构如图1所示。对于任意给定的transformation $$F_{tr}$$，会将input X映射到feature maps U上，其中：$$U \in R^{H \times W \times C}$$，例如：一个convolution，我们**可以构建一个相应的SE block来执行feature recalibration**。

- **squeeze operation**：首先features U会通过一个squeeze operation进行传递，它会通过将**沿着它们的空间维度（H X W）的feature maps进行聚合来生成一个channel descriptor**。该descriptor的函数会生成一个关于channel-wise feature responses的全局分布，他允许来自该网络的global receptive field的信息通过所有它的layers来被使用。
- **excitation operation**：接着跟一个excitation operation来进行聚合，它会采用一个**简单的self-gating机制的形式**，会将该embedding作为input，并生成一个关于per-channel modulation weights的集合。该weights会被应用到feature maps U来生成SE block的output，它可以被直接feed到该network的后续layers上。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/e4eb56d55f32801efb3e2dd8e715950609810d821759775ac57def51a70f7e8128bd17a75af54cdd727e11db47b7b5aa?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 一个Squeeze-and-Excitation block

通过简单地将一个关于SE blocks的collection进行stacking，可以构造一个SE network（SENet）。另外，在网络结构的深度范围内，这些SE blocks可以被用来作为一个对于original block的简易替换(drop-in replacement)。而该building block的模板是通用的，它会在不同的depths上进行执行，而非贯穿该network。在更早的layers中，它会以一个分类不可知（class-agnostic）的方式来激发有信息量的features，增强共享的low-level representations。在更后的layers中，SE blocks会变得更专业，来以一个高度class-secific的方式响应不同的inputs。因此，通过SE blocks执行feature recalibration的好处是，可以通过network进行累积。

新的CNN结构的设计和开发是一个非常难的工程任务，通常需要许多新超参数以及layer配置的选择。相反的，SE block的结构非常简单，可以直接用在state-of-the-art的结构中，通过将组件替换成SE部分即可，其中：效果可以被有效增强。SE blocks的计算非常轻量，只会在模型复杂性和计算开销上引入一个轻微的增加。

为了提供证据，我们开发了许多SENets，并在ImageNet dataset上进行评估。我们的方法在ILSVRC 2017分类比赛中得到第一。在top-5 error上达到了2.251%的提升。

# 2.相关工作

# 3. SQUEEZE-AND-EXCITATION BLOCKS

一个Squeeze-and-Excitation block是一个计算单元，它可以在一个transformation $$F_{tr}$$上构建，该转换会将一个input $$X \in R^{H' \times W' \times C'}$$映射成feature maps $$U \in R^{H \times W \times C}$$。接着，我们将$$F_{tr}$$看成是一个convolutional operator，并使用$$V = [v_1, v_2, \cdots, v_C]$$来表示关于filter kernals的learned set，其中：$$v_c$$指的是：第c层filter的参数。我们接着将outputs写为 $$U = [u_1, u_2, \cdots, u_C]$$，其中：

$$
u_c = v_c * X = \sum\limits_{s=1}^{C'} v_c^s * x^s
$$

...(1)

其中，$$*$$表示convolution，$$v_c = [v_c^1, v_c^2, \cdots, v_c^{C'}], X = [x^1, x^2, \cdots, x^{C'}], u_c \in R^{H \times W}$$。$$v_c^s$$是一个2D spatial kernel，表示$$v_c$$的一个单通道（single channel），扮演着X的相应channel。为了简化该概念，bias项会被忽略。由于该output会通过一个关于所有channels的总结来生成，channel dependencies会隐式地嵌入在$$v_c$$中，但会与由filters捕获的local spatial correlation牵连在一起。通过convolution建模的channel relationships，天然就是隐式（implicit）和局部的（local），除了在top-most layers上。我们期望，convolutional features的学习可以通过显式建模channel间的相互依赖性来进行增强，因此，该network可以增加对informative features的感知，它们可以通后续的转换所利用。因此，我们希望在两个steps中提供访问全局信息和对filter responses进行recalibrate：squeeze和excitation，在它们被feed到下一transformation前。SE block的结构如图1所示。

## 3.1 Squeeze: Global Information Embedding

为了解决利用channel dependencies的问题，我们首先考虑在output features中每个channel的信号。每个learned filters会与一个local receptive field进行操作，相应的，该transformation output U的每个unit不能利用在该区域外的contextual information。

为了消除该问题，我们提出将全局空间信息（global spatial information）进行压缩（squeeze）到一个channel descriptor中。通过使用global average pooling来生成channel-wise statistics的方式来达到。正式的，一个statistic $$z \in R^C$$通过对U在它的空间维度$$H \times W $$上进行收缩（shrinking）来生成，以使z的第c个element可以被计算：

$$
z_c = F_{sq}(u_c) = \frac{1}{H \times W} \sum\limits_{i=1}^H \sum\limits_{j=1}^W u_c(i, j)
$$

...(2)

讨论：transformation U的output可以被解释成一个关于local descriptors的集合，它的statistics对于整个image来说表达力强。利用这样的信息在之前的feature engineering工作中很盛行。我们选择最简单的聚合技术，global average pooling，注意：最复杂的策略在这也可以使用。

## 3.2 Excitation：自适应再校准（Adaptive Recalibration）

为了利用在squeeze操作中聚合的信息，我们会接着使用一个二级操作符（second operation），它的目标是完全捕获channel-wise dependencies。为了满足该目标，该函数必须满足两个准则：首先，它必须是灵活的（特别的，它必须能学习一个在channels间的非线性交叉）；第二，它必须学习一个非相互排斥（non-mutually-exclusive）关系，由于我们会确保：多个channels会被允许强调（非而强制一个one-hot activation）。为了满足这些准则，我们选择采用一个使用sigmoid activation的简单gating机制：

$$
s = F_{ex}(z, W) = \sigma(g(z, W)) = \sigma(W_2 \sigma(W_1 z))
$$

...(3)

其中，$$\sigma$$指的是ReLU function，$$W_1 \in R^{\frac{C}{r} \times C}$$和$$W_2 \in R^{C \times \frac{C}{r}}$$。为了限制模型复杂度，以及aid generalisation，我们会对gating机制进行参数化，通过在非线性周围构建一个具有两层FC layers的瓶颈来达到：例如，一个具有reduction ratio为r的降维层（dimensionality-reduction layer）（该参数选择可以在第6.1节中讨论）、一个ReLU、接着一个增维层（dimensionality-increasing layer），它会返回transformation output U的channel维度。该block的最终output可以通过使用activations s，并对U进行rescaling来获得：

$$
\bar{x}_c = F_{scale}(u_c, s_c) = s_c u_c
$$

...(4)

其中，$$\bar{X} = [\bar{x}_1, \bar{x}_2, \cdots, \bar{x}_C]$$以及$$F_{scale}(u_c, s_c)$$指的是在标量$$s_c$$和feature map $$u_c \in R^{H \times W}$$间的channel-wise乘法。

讨论：excitation operator会将input-specific descriptor z映射到一个关于channel weights的set上。就这一点而言，SE blocks内在会基于input上引入动态性（dynamics），它会被看成是在channels上的一个self-attention function，它的关系会受限于convolutional filters所响应的local receptive field。

## 3.3 实例化

SE block可以被集成到如VGGNet的标准结构中，在每个convolution的非线性之后通过插方式实现。再者，SE block的灵活性意味着，它可以被直接应用到在标准卷积之外的转换上。为了说明这一点，我们开发了SENets，通过将SE blocks包含到许多更复杂结构的实例中，来描述。

我们首先考虑，对于Inception networks上SE blocks的构建。这里，我们简单地将transformation $$F_{tr}$$看成是一整个Inception module（如图2所示），并且通过对于在结构上的每个module做出这种变更，我们可以获得一个SE-Inception network。SE blocks可以被直接与residual networks一起使用（图3描述了SE-ResNet module的schema）。这里，SE block transformation $$F_{tr}$$被看成是一个residual module的非正定分支（non-identity branch）。Squeeze和Excitation会在使用identity branch进行归纳前同时起作用。更进一步，使用ResNeXt、Inception-ResNet、MobileNet和ShuffleNet集成SE blocks的变种，可以通过遵循相应的schemes来构建。对于SENet结构的具体示例，如表1有给出：SE-ResNet-50、SE-ResNeXt-50的详细描述。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/d009141355e85da2cedf44a4d7873f517dd4bc64d9f06f67744b57e4ea0c0ce2435c57092bcfb702e4dc6c9586231187?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/04f39fc080522cf3cd0a10d29fdd1174f0080344a3b2ed82e01adff63b77940a23b1ca4a9818de225051069f87ddec38?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3

SE block的灵活性的一个结果是，有许多变种方式，可以很容易集成到其它结构中。因此，为了对所使用的集成SE blocks到网络结构中的策略评估敏感性，我们可以提供消融实验（ablation experiments）来探索关于block的不同设计。

# 4.模型和计算复杂度

对于提出的SE block设计，它必须提供一个在效果和模型复杂度间的较好tradeoff。为了说明与module相关的计算开销，我们会考虑在ResNet-50和SE-ResNet-50间的一个对比作为示例。ResNet-50在单个forward pass上对于一个224 x 224 pixel input image上需要~3.86 GFLOPs。每个SE block会在squeeze阶段利用一个global average pooling操作符，而在excitation阶段会使用两个小的FC layers，接着跟一个昂贵的channel-wise scaling操作。在聚合时，当设置reduction ratio r到16时，SE-ResNet-50需要~3.87 GFLOPs，对比起原始ResNet50，对应有一个0.26%的相对提升。除了增加轻微的额外开销外，SE-ResNet-50的accuracy要超过ResNet50，可以达到一个更深的ResNet-101网络的效果，而它需要~7.58 GFLOPs（表2）。

在实际项中，通过ResNet-50的单个pass forwards和backwards会花费190ms...

略


- 1.[https://arxiv.org/pdf/1709.01507.pdf](https://arxiv.org/pdf/1709.01507.pdf)