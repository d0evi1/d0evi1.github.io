---
layout: post
title: CNN分类
description: 
modified: 2016-08-09
tags: [cnn]
---

# 0.介绍

Yoon Kim在《Convolutional Neural Networks for Sentence Classification》介绍了使用CNN来做句子分类的任务。下面基于对该paper的理解，简单地做个介绍：

# 1.模型架构


<img src="http://pic.yupoo.com/wangdren23/GxHt7R1B/medish.jpg">

图1. 对于一个语句，使用双通道的模型架构

\$ x_i \in R^k \$ 为句子中第i个词的k维词向量。句子长度为n（不足补齐: pad），表示成：

$$
x_{1:n} = x_1 \oplus x_2 \oplus x_3 ... \oplus x_n
$$

... (1)

其中：

\$ \oplus \$为串联操作符(concatenation operator)。 \$ x_{i:i+j} \$ 表示 \$ x_i \$至\$ x_{i+j} \$的串联。

卷积（convolution）操作符涉及到一个过滤器（filter）： \$ w \in R^{h \times k} \$，它可以应用于一个含h个词的窗口，来生成一个新的特征。例如，可以由一个词窗口\$ x_{i:i+h-1}\$来生成一个**特征\$c_i\$**：

$$
c_i = f(w \cdot x_{i:i+h-1} + b)
$$

...(2)

这里 \$ b \in R^{n-h+1} \$是一个bias项，f是一个非线性函数（例如：假设函数tangent）。将filter应用在每个可能的句子中的词窗口：\$ \{x_{1:h}, x_{2:h+1},...,x_{n-h+1:n}\} \$来生成一个**特征图（feature map）**。

$$
c = [c_1, c_2, ..., c_{n-h+1}]
$$

...(3)

其中\$ c \in R^{n-h+1}\$，我们接着在该feature map上应用一个max-over-time pooling操作，并采用最大值\$ \hat{c} = max \\{ c \\} \$作为该指定filter相应的特征。**该思路用来捕获最重要的特征——对于每个feature map取最大值得到**。该pooling scheme天然就可以处理不同的句子长度。

我们接着描述该过程，通过从一个filter上抽取一个feature。**该模型使用多个filters(具有不同的窗口size)来获得多个feature**。这些特征构成了倒数第二层（penultimate layer），并被传到一个fully connected softmax layer，它的输出为在label上的概率分布。

在另一个模型变种中，我们试验了具有两个词向量的通道（channels）——一个保持static throughout training，另一个通过backpropagation进行 fine-tuned。在多通道的架构上，如图1所示，每个filter被应用于多个channel。被添加的结果用来计算等式(2)式中的\$c_i\$。该模型和单个channel的架构相似。

## 2.1 Regularization

对于Regularization，我们在倒数处二层（penultimate layer）使用dropout，使用一个关于权重向量的l2-norm的约束（constraint）。通过进行随机dropout， **Dropout可以阻止隐单元的相互适应现象（co-adaptation）**——这样，在前向传播（forward-backpropagation）期间将比例为p的隐单元置为0. 也就是说，给定倒数第二层（penultimate layer）：\$ z = [\hat{c}_1, ..., \hat{c}_m] \$（注意：这里有m个filter），做为替换，不再使用：

$$
y = w \cdot z + b 
$$

...(4)

对于在前向传播（forward propagation）中的输出单元y，dropout使用：

$$
y = w \cdot (z \circ r) + b
$$

...(5)

其中\$ \circ \$是element-wise乘法操作，\$ r \in R^{m}\$是一个关于Bernoulli随机变量的'masking'向量，它具有概率p的部分为1。梯度通过后向传播，只通过unmasked的单元。在测试时，学到的weight向量通过p进行归一化，例如：\$ \hat{w} = pw \$，其中\$ \hat{w} \$被用来（没有dropout）对未见过的句子（unseen sentences）进行打分。我们又额外增加权重向量的l2-norms约束，通过对w进行rescaling，使得：$ {\|\|w \|\|}_{2}$，在经历一个梯度下降的step后，将永远$ {\|\|w \|\|}_2 > s $。

# 数据集

- MR: 电影评论(Movie Reviews)。分类检测正负语义。（Pang and Lee, 2005）
- SST-1:  Stanford Sentiment Treebank——MR的扩展，具有train/dev/test splits，提供了细粒度标签（very positive,
positive, neutral, negative, very negative）。 Socher et al. (2013)
- SST-2: 类似SST-1. 移除了neutral评论，增加了binary labels
- Subj：Subjectivity数据集，分类任务：将句子分类成：subjective or objective。(Pang and Lee, 2004).
- TREC: TREC question数据集——将一个question分类成6个问题类型（该问题是关于：person, location, numeric information, etc.） (Li and Roth, 2002)
- CR： 多种商品的顾客评价(Customer reviews)。预测positive/negative 评论。(Hu and Liu, 2004).
- MPQA：MPQA数据集的意见极性检测（Opinion polarity detection）。 (Wiebe et al., 2005).

## 3.1 超参数和训练

对于所有数据集，统一使用：

- ReLU
- filter window(h)为：3, 4, 5
- 每个window具有100个feature map
- dropout rate (p)为：0.5
- l2 constraint (s)为：3
- mini-batch size为：50

这些值的选择在 SST-2 dev set上通过grid search找到。

我们不执行任意的指定数据集的调整，而是在dev sets上做early-stopping。对于没有标签dev set的数据集，我们随机选对10%的训练数据作为dev set。训练过程通过在shuffled mini-batchs数据上，使用Adadelta update rule（Zeiler, 2012），以及SGD来完成。

## 3.2 Pre-trained词向量

从非监督神经语言模型中获取词向量进行初始化，这种方法很流行。我们使用word2vec对Google News的1000亿个词进行训练。这些向量具有300维，使用CBOW架构，不在pre-trained词向量中的词则随机初始化。

## 3.3 模型变种

- CNN-rand: 作为baseline模型，所有的词都是随机初始化，接着在训练中进行修改。
- CNN-static: 使用来自word2vec的pre-trained vector的model。所有的词（包括随机初始化的未登陆词）保持static，只有模型中的其它参数是通过学习得到。
- CNN-non-static: 与上面的方法相似，但对于每个任务，pre-trained vectors都会进行微调（fine-tuned）。
- CNN-multichannel: 模型具有两个词向量集合。每个向量集都看成是一个'channel'，每个filter都会作用于两个channel，但梯度的后向传播只通过其中一个channel进行。这里模型可以fine-tune一个向量集，让另一个保持static。两个channel都通过word2vec进行初始化

为了对上述变种vs.其它随机因子进行比较，我们消除了其它源的随机性——CV-fold任务，未登陆词向量的初始化，CNN参数的初始化——在每个数据集上对它们保持统一。

# 4.结果

<img src="http://pic.yupoo.com/wangdren23/Gy8hgP7T/medish.jpg">

表2: CNN模型vs.其它方法。其它方法详见paper解释.

结果如表2所示。baseline model是CNN-rand：全随机初始化词，表现并不理想。通过使用pre-trained vector，会获得效果的提升。使用CNN-static的效果很显著，比起其它更复杂的深度学习模型(使用pooling或parse tree等)，结果也有得一拼。这些结果说明了pre-trained vector很好、很通用（‘universal’）的feature extractors，并且可以跨数据集使用。对pre-trained vector进行微调（finetuning），可以为每个任务获得更进一步的提升（CNN-non-static）。

## 4.1 Multichannel vs. Single Channel Model

我们原先以为multichannel架构会阻止overfitting的发生（通过确保学到的vector与原先的值偏离太远），会比single channel效果更好，尤其是在小数据集的情况下。然而，结果参半，需要更进一步对fine-tuning过程进行正则化（regularizing）。例如，对于no-static部分使用一个额外的channel，你可以保持一个single channel，并可以额外的维度，它们允许在训练过程中进行修改。

<img src="http://pic.yupoo.com/wangdren23/Gy8z50lO/medium.jpg">

表3: top 4个邻近词——基于consine相似度——static channel(左列)中的向量，在SST-2数据集上，在训练后的multichannel模型中的non-static channel(右侧)中的finetuned vector。

## 4.2 static vs. Non-static表示

正如single channel non-static model的情况，multichannel模型能够对 non-static channel进行微调（fine-tune），来使要处理任务更具指定性。例如：good在word2vec中与bad最相似，推测起来是因为它们在句子结构(syntactically)上（大至）是相等的。但对于在SST-2数据集上进行微调的non-static channel中的词向量，不再有表3中的情况。相似的，在表示语义上，good与nice更接近（比起good与great），这的确可以反映在学到的向量中。

对于不在pre-trained vector中的token（随机初始化），进行fine-tuning可以使这些token学到更有意义的表示（representation）：该网络可以学到：感叹号（exclamation marks）与感情表达有关，逗号与连接词有关。

## 4.3 进一步观察

- Kalchbrenner et al. (2014)，使用一个 CNN得到更糟的结果，本质上与single model的架构一致。例如，它们的Max-TDNN(Time
Delay Neural Network)使用随机初始化的词，在SST-1上获得了37.4%，而我们的模型则为45.0%。我们将这种差异归因于：我们的CNN具有更大的容量（多个filter widths和feature maps）。
- Dropout被证明是一种很好的regularizer, 它很容易使用一个更大的网络，只需dropout去进行regularize即可。Dropout可以增加2-4%的效果提升
- 当随机初始化的词不在word2vec中时，通过从U[-a,a]中抽样每一维，可以获得微小的提升，其中选中的a，可以使随机初始化的向量具有与pre-trained vector具有相似的variance。在初始化过程，使用更复杂的方法来反映(mirror)pre-trained vectors的分布，来获得提升是挺吸引人的一件事。
- 我们试验了另一个公共的词向量（由Collobert et al. (2011) on Wikipedia训练得到），发现word2vec可以获得更好的效果提升。这一点不是很清楚：是否是因为o Mikolov et al. (2013)的架构，还是因为google news 1000亿词的数据集的原因。

# 5.结论

本文描述了在word2vec上构建CNN的一些试验。只需很少的超参数的调参，一个简单的CNN具有一层的卷积层，就可以得到令人吃惊的效果。本文的结果也有效验证了在Deep NLP中pre-trained word vector相当重要。

# 参考

[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)