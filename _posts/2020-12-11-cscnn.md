---
layout: post
title: CSCNN介绍
description: 
modified: 2020-12-11
tags: 
---

JD在《Category-Specific CNN for Visual-aware CTR Prediction at
JD.com》提出了CSCNN:

# 1.介绍

JD领先的广告系统，会服务数百万广告主(advertisers)与数十亿顾客(customers)相连。每天，顾客会访问JD，点击ads并留下数十亿的交互日志。这些数据不仅会反馈给到学到的系统，但也会增强技术演进提升用户体验。

在常见的CPC广告系统中，广告会通过eCPM进行排序，商品的竞价通过advertisers给出，广告系统则会预测CTR。精准的CTR预测对商业效果和用户体验有用。因而，该topic在机器学习界和工业界被广泛关注。

大多数广告会使用图片进行展示，因为它们具有更多的视觉展示，并对于文本描述能传达更丰富的信息。**一个有意思的现象是，许多广告通过切换更吸引人的图片会获得更高的CTR**。对于CTR预测，这驱使了关于提取更丰富可视化特征的许多研究。这些算法会采用**现成的（off-the-shelf）的CNNs来抽取可视化特征**，并将它们与非可视化的特征（non-visual features：比如：category, user）进行混合来进行最终CTR预测。**有了额外的可视化特征，这些算法在离线实验上可以远胜过无可视化的模型，并可以泛化到冷门和长尾ads上**。在实际在线广告系统中使用CNN仍然是non-trival的。使用CNN进行offline end-to-end训练必须足够有效遵循随时间变化（time-varying）的在线分布，online serving需要广告系统的满足低时延要求。

另外，我们注意到，在电商中的可视化特征抽取与图片分类的setting有大不同。在分类任务中，categories会被看成是要预测的target。而在电商系统中，广告的categories会被明显地进行标记，它包含了丰富的可视化先验信息，可以帮助进行可视化建模。一些学术研究通过在CNN embeddings的top之上[7]构建category-specific投影矩阵进行集成，并将可视化features显式解耦成styles和categories。这些研究会共享一个公共的架构：visual和categorical knowledge的late fusion，然而，它对于CTR预测来说是sub-optimal。也就是说，image embedding模块很少会利用categorical knowledge。**如果不知道ad category，通过这些CNNs抽取的embedding会包含与该category不相关的不必要features，从而浪费CNN的有限表达能力。相反，如果该ad category被集成其中，CNN只需要关注category-specific patterns，它会减轻训练过程**。

为了克服工业挑战，我们会同时为有效的end-to-end CNN training和低时延在线服务构建优化的基础设施。基于该有效地基础设施，为了**充分利用电商中的labeled category**，我们为CTR预测任务特别提出Category-specific CNN (CSCNN)。我们的关键思想是，以一个early-fusion的方式将category知识插入到CNN中。受SE-net、以及CBAM的启发，它会使用一个light-weighted self-attention模块来建模convolutional features间的相互依赖，CSCNN会进一步吸收ad category知识，并执行一个category-specific feature recalibration，如图2所示。更明显地，我们会接着使用category-specific channel和spatial attention modules来强调重要的以及与category相关的features。这些丰富的可视化特征对于CTR预测问题来说有巨大的效果增益。

总之，我们有以下的贡献：

- 据我们所知，我们是在visual-aware CTR预测中首个对visual和non-visual features的late fusion的负面影响进行强调的。
- 我们提出了CSCNN，为CTR预测特别设计了一个新的visual embedding模块。关键思想是组织category-specific channel和spatial self-attention来强调重要并且与category相关的特征。
- 我们通过大量离线实验、以及AB test验证了CSCNN的有效性。我们验证了许多self-attention机制的效果，以及network backbones通过插入CSCNN来进行一致性提升。
- 我们构建了高度有效地基础设施在real online电商广告系统中来使用CNN。在一天内100亿规模的真实产品数据集上，引入有效加速方法来对CNN完成end-to-end training，并满足在线系统的低时延需求（在CPU上20ms）。CSCNN已经被部署在JD的搜索广告系统中。

# 2.相关工作

## 2.1 CTR预测

...

## 2.2 CNN中的attention机制

**attention机制是一个重要的feature selection方法，它可以帮助CNN来强调feature maps的重要部分，并抑制不重要的部分**。spatial attention会告诉你关注**where**，而channel-wise attention则告诉你focus在**what**上。

在文献中，许多工作尝试从feature map中学习attention weights，称为“self-attention”。SOTA的算法称为CBAM， SE。除了self attention外，attention weights可以有额外的信息为条件，例如自然语言。成功应用的领域包括：language、image captioning以及可视化问答。

我们的工作受attention机制的启发。而非vision & language，我们设计了新的架构来使用attention机制来解决一个重要但长期忽略的问题：对于vision和non-vision feature的sub-optimal late fusion。我们同时将self-attention和attention conditioned on external information（称为：ad category）两者的优点相结合。作为结果，我们的图片embedding能够强调important和category相关的features。

# 3. JD中的CTR预测

我们首先review了3.1节中的CTR预测的背景。接着，我们描述了CTR预测系统的架构。我们会进一步挖掘新的visual modeling模块的细节。最终，我们会引入必要的加速策略来进行在线部署。表1中总结了相关概念。

## 3.1 先决条件

在在线广告工业界，一个ad会在一些contexts下被展示给一个user，该情景被记成一次曝光（impression）。CTR预测的目标是：在发生一次impression（ad, user, contexts）时，预测一次positive feedback（例如：click）的概率。准确的CTR预测直接有益于用户体验和商业效果，这使得该任务对于整个广告工业来说很重要。

CTR预测通常被公式化成二分类问题。特别的，它会从一个training set $$D = \lbrace (x_1, y_1), \cdots, (x_{\mid D \mid}, y_{\mid D \mid} )\rbrace$$中学习一个预测函数f: $$R^d \rightarrow R$$，，其中$$x_i \in R^d$$是第i次impression的feature vector，$$y_i \in \lbrace 0,1  \rbrace$$是class label表示一个click是否发生。

目标函数被定义成负的log-likelihood:

$$
l(D) = - \frac{1}{|D|} \sum\limits_{i=1}^{|D|} y_i log(\hat{y}_i)  + (1-y_i)log(1-\hat{y}_i)
$$

...(1)

其中，$$\hat{y}_i$$是predicted CTR，通过sigmoid $$\sigma$$归一化到(0, 1)中：

$$
\hat{y}_i = \sigma(f(x_i))
$$

...(2)

## 3.2 CTR预测系统的架构

我们现在描述了我们的CTR预测系统的架构，如图1所示。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/0b019b9a8fb8982b11ec590e9eb4fc9eba77fe2825c2dabc93bfcb01af84e7350cfe5dcb5f449b149778ebe67e350ee9?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=1.jpg&amp;size=750" width="500">

图1. CTR预测系统的架构。左下角：CSCNN，它会将一个ad image与它的category一起嵌入到一个visual feature vector $$x_v \in R^{150}$$中。注意，CSCNN只在offline下运行。而在online serving系统中，为了满足低时延需求，我们会使用一个高效的lookup table进行替代。右下角：non-visual feature embedding，从(ad, user, contexts)到一个non-visual feature vector $$x_{nv} \in R^{380}$$。TOP：主要架构，一个修改版的DCN，它会采用visual feature $$x_v$$和non-visual feature $$x_nv$$作为inputs。

### 3.2.1 DCN

DCN网络可以得到可靠的效果，它可以学到有效的特征交叉。这里，我们将DCN修改成两个inputs：

- 一个non-visual feature vector $$x_{nv} \in R^{380}$$
- 一个visual feature vector $$x_v \in R^{150}$$

visual feature被包含在deep net中。在layer 1中，我们将non-visual feature转换成1024维，并将它们与visual feature进行concatenate起来：

$$
h_1 = [x_v, ReLU-MLP(x_{nv})] \in R^{150 + 1024}
$$

...(3)

接着跟两个deep layers：

$$
h_{l+1} = ReLU-MLP(h_l), l \in \lbrace 1, 2\rbrace, h_2 \in R^{512}, h_3 \in R^{256}
$$

...(4)

cross net用于处理non-visual feature：

$$
z_{l+1} = z_0 z_l^T w_l + b_l + z_l
$$

...(5)

其中，对于layer $$l \in \lbrace 0, 1, 2\rbrace$$，input $$z_0 = x_{nv}$$。

最终，我们会为predicted CTR组合outputs：

$$
\hat{y} = \sigma(ReLU-MLP[h_3, z_3])
$$

...(6)

### 3.2.2 Non-visual Feature Embedding

我们现在描述embedding layer会将一次impression（ad, user, contexts）的raw non-visual features转成vector $$x_{nv}$$.

我们假设：所有features以categorical的形式进来（例如：binning，预处理后）。通常，一个categorical feature会以one-hot / multi-hot vector $$ x_hot \in \lbrace 0,1 \rbrace^v $$的方式编码，其中v是该feature的vocab size。我们以如下方式展示两条样本：

	WeekDay=Web  ==>  [0,0,0,1,0,0,0]
	TitleWords=[Summer,Dress] ==> [..., 0,1,0, ..., 0,1,0...]

不幸的是，该one/multi-hot coding不能应用于工业界系统，因为高维稀疏性。我们在我们的系统上采用一个低维的embedding策略：

$$
x_{emb} = E_{x_{hot}}
$$

...(7)

其中：

- $$E \in R^{d_e \times v}$$是对于该specific feature的embedding字典
- $$d_e$$是embedding size

我们接着将$$x_{emb}$$的所有features进行concatenate来构建$$x_{nv}$$.

实际上，我们的系统会使用来自用户的95个non-visual features（历史点击/购买，location），ads（category, title, #reviews等）以及rich contexts（query words, visit time等），总共有70亿vocab。设置$$d_e = 4$$，总的维度是95 x 4 = 380. 我们将进一步引入features和其它statistics。

## 3.3 Category-Specific CNN

converntional预测系统大多数使用off-the-shelf CNN来嵌入ad图片。**我们将它称为“off-the-shelf”，是因为它原始是用来分类设计的，而不是CTR预测**。他们将image category看成是要预测的target，而非inputs。这实际上在电商平台上是一个巨大的浪费，因为：categories会被精准标记，并包含丰富的可视化先验知识，可以用于visual modeling。

我们针对CTR预测通过提出一种新的CNN（Category-Specific CNN）来解决该问题，它会嵌入一个ad图片m，并与ad category $$k \in K$$一起concat到visual feature $$x_v$$上。特别的，category prior knowledge被编码成category embeddings（与CTR模型联合训练），并使用一个conditional attention机制来包含CNN。

理论上，CSCNN可以被用来在任意网络中当做任意的convoluation layer。在我们的系统中，我们插入CSCNN到ResNet18.

### 3.3.1 单个convolutional layer上的框架

对于每个category k以及每个convolutional layer l，CSCNN会学习一个 tensor $$A_c^k \in R^{1 \times 1 \times C'}$$，它会为该layer编码category prior knowledge在channel-wise attention上的影响。我们会出于简洁性，忽略subscript l。框架如图2所示。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/42a58513ab4a312c619447ae6e7cc007c1fd9c4ea12369ef47925c7f89feede223fefc30741ba5eb1eb1209f434d4b54?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=2.jpg&amp;size=750">

图2 我们提出的Category-Specific CNN框架。注意CSCNN可以被添加到任意单个convolutional layer上，但出于简单演示，我们只展示了单个layer的细节。TOP：一个将category映射到category prior knowledge的map，它会影响channel-wise & spatial attentions。Bottom：F是当前convolutional layer上的output feature map。通过顺序使用channel-wise和spatial attention进行refined，新的feature map $$F''$$被当成下一layer的input使用

给定一个intermediate feature map $$F \in R^{H \times W \times C}$$，convolutional layer l的output，CSCNN会首先学习一个channel attention map $$M_c \in R^{1 \times 1 \times C}$$，它基于当前feature map和category为条件。接着，channel-wise attention会被乘上feature map来获得一个重新定义的feature map $$F' \in R^{H \times W \times C}$$, 

$$
F' = M_c (F, A_c^k) \odot F
$$

...(8)

其中，$$ \odot $$表示与$$M_c$$的element-wise product，它沿着spatial维度$$H \times W$$进行广播。

相似的，CSCNN也会学到另一个tensor $$A_s^k \in R^{H \times W \times 1}$$，它会为spatial attention $$M_S \IN R^{H \times W \times 1}$$对category prior knowledge进行编码。这两个attention模块被顺序用来获得一个3D的refined feature map $$F'' \in R^{H \times W \times C}$$：

$$
F'' = M_s(F', A_s^k) \odot F'
$$

...(9)

其中，spatial attention会在element-wise product之前沿着channel维度进行广播。一个实际的关注点是，在$$A_s^k$$中存在大量参数，尤其是在前几层。为了解决该问题，我们提出只学习一个更小的tensor $$A_s^{'k} \in R^{H' \times W' \times 1}$$，其中$$H' << H$$以及$$W' << W$$，接着通过线性插件（linear interpolation）将它进行resize到$$A_s^k$$。$$H'$$和$$W'$$的作用会在后面实验结果进行讨论。注意，$$A_s^k$$和$$A_c^k$$会随机初始化并在训练期间学习，除category id外不需要额外的category prior knowledge。

channel-wise和spatial attention两者都被重新定义后，$$F''$$会被fed给下一layer。注意，CSCNN会被添加到任意CNNs上，通过只将input的F替代成next layer的$$F''$$。


### 3.3.2 category-specific channel-wise Attention

channel-wise attention会告诉要关注"what"。除了之前的inter-channel关系外，我们也会利用category prior knowledge和features间的关系（图3）。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/c2cbc4246bbf2f259f5ca1a9f1cce59300fdaf7cfbd3b5032b5cfa84ee636eff7eefacb6975ee103ed384b4a39e3252c?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=3.jpg&amp;size=750">

图3

为了收集spatial信息，我们首先将F的spatial dimension通过max和average pooling进行挤压（squeeze）。采用两者的优点由CBAM的实验所支持验证。两个squeezed feature maps接着与category prior knowledge $$A_c^k$$进行concatenated一起，并通过一个共享的two layer MLP进行froward传递，将维度从$$1 \times 1 \times (C + C')$$减小到$$1 \times 1 \times C$$上。最终，我们通过element-wise summation进行合并。

$$
M_c(F, A_c^k) = \sigma(MLP[Avg P(F), A_c^k] + MLP[MaxP(F), A_c^k])
$$

...(10)

### 3.3.3 Category-specific Spatial Attention

我们的spatial attention module如图3(bottom)。Spaital attention会通过利用features的inter-spatial关系，告诉需要关注哪里。受CBAM的影响，我们首先通过average pooling和max pooling沿着channel的维度聚合feature map $$F'$$的channel-wise信息。为了包含category prior knowledge，这两者接着与$$A_s^k$$进行concatenate一起来形成一个$$H \times W \times 3$$维度的feature map。最终，该feature map通过一个$$7 \times 7$$的convolutional filter进行传递来获得attention weights。

$$
M_s(F', A_s^k) = \sigma(Conv_{7 \times 7}(Max P(F'), Avg P(F'), A_s^k))
$$

...(11)

### 3.3.4 复杂度分析

注意，CSCNN实际上是一个轻量级module。特别的，我们在表2中展示了Baseline、CBAM以及我们的算法在参数数目和giga floating-point operations(GFLOPs)上的对比。

我们设轩$$C \in \lbrace 64, 128, 256, 512 \rbrace, C'=20$$，瓶颈下降至4， #categories $$\mid K \mid =3310 $$（表7中的real production dataset）。在CBAM中的每个convolutional yer中的“shared FC”中，参数数目是$$2 * C * C / 4$$。对于CSCNN，FC中的参数数目和channel category embedding是$$C * C / 4 + (C + C')*C/ 4 + C' * \mid K \mid$$。在channel attention中，参数数目的增加对比CBAM是1个conv layer为67-69k。另外，$$W' = H' = 6$$，在spatial attention中的额外参数数目是$$W' * H' * \mid K \mid + 6 * 6 \approx 120k$$。因此，总参数的增加为(120k + 68k) * 16 layers = 3.0M。额外的参数引入是可接受的，对比CBAM，额外计算只有0.03%。

## 3.4 系统部署

我们在搜索广告系统部署了CSCNN。图4描述了我们在线模型系统的架构。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/93aafa68b723f3e5356ef034c0b86c0ab262f6fff1d83b3320ad737b91fb327e291af31848a26a9aaf461244751fab0d?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=4.jpg&amp;size=750">

图4 

### 3.4.1 offline training

CSCNN会与整个CTR prediction系统进行jointly train，最近32天收集的100亿规模的直实数据集。在我们之前的调查中，CNN是训练期间的关键计算瓶颈。采用ResNet18 network，它的input pic size为224 x 224，单机4个P40 GPUs只能每天训练1.77亿图片。这意味着在分布式训练中加速CSCNN，我们需要226 P40 GPUs来计算1天内的100亿曝光，这很昂贵。为了加速，我们采用[2]中的sampling strategy。具有相同的ad的大约25个曝光（impressions）会收集成一个batch。一张图片的image embedding只需要管理一次，并在该batch中传播给多次曝光。有了28张P40 GPUs后，训练可以在1天内完成。

### 3.4.2 Offline inferring

images和categories会被feed到一个well trained CSCNN中来infer那些visual features。Fearures会传到一个lookup table中，接着在predictor memory中加载来替换CSCNN。在维度减小和频控后，一个20GB的lookup table可以覆盖超过下一天曝光的90%。

### 3.4.3 Online serving

一旦收到请求，visual features会直接从lookup table根据ad id进行发现。predictor会返回一个estimated CTR。在流量高峰中，每秒有超过3亿的items的吞吐量，我们的CPU online serving系统的tp99 latency会在20ms以下。

# 4.实验结果

略

# 参考


- 1.[https://arxiv.org/pdf/2006.10337.pdf](https://arxiv.org/pdf/2006.10337.pdf)