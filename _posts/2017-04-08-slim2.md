---
layout: post
title: local i2i介绍
description: 
modified: 2017-04-08
tags: 
---

《Local Item-Item Models for Top-N Recommendation》一文提到了slim的实现。

# 1.介绍

top-N推荐系统无处不在。它们会提供一个用户可能感兴趣的关于N个items的ranked list。

。。。

# 2.概念


# 3.相关工作

## 3.1 top-N推荐方法

在top-N推荐领域有许多工作。这里我们提出了一新的SOTA的方法。Deshpande[8]开发了一种最近邻item-based方法，它展示了item-based模型会比user-based模型生成更好的top-N推荐。Cremonesi【7】开发了pureSVD方法，它使用一个trucated SVD矩阵分解R来生成top-N的推荐。该工作表明：**将missing entries看成0会比矩阵补全方法生成更好的结果**。这也是l2r方法的观点。

### 3.1.1 topN推荐的Sparse LInaear Method（SLIM）

Ning引入了SLIM，它是首个使用使用statistical learning来计算item-item关系的方法，并表明了对top-N推荐的最好方法之一。**SLIM会估计一个sparse $$m \times m$$的聚合系数矩阵S**。用户u在一个未评分item（urated item） i上的推荐分，可以通过对所有用户过往有评分item（rated items）进行一个sparse aggregation来计算：

$$
\hat{r}_{ui} = r_u^T s_i
$$

其中：

- $$r_u^T$$是对应于user u的R的row-vector
- $$s_i$$是matrix S的第i个column vector，通过求解以下的optimization问题来进行估计得到：

$$
\underset{s_i}{minimize} (\frac{1}{2} \| r_i - R s_i \|_2^2 + \frac{\beta}{2} \| s_i \|_2^2 + \lambda \|s_i\|_1) \\
subject\ to \ s_i >=0, and \ s_{ii}=0
$$

..(2)

常量 $$\beta$$和$$\lambda$$是正则参数。使用非负constraint，以便vector估计包含正系数（positive coefficients）。其中$$s_{ii}=0$$的constraint确认了：当计算一个item的weights时，item本身不会被用到，因为它会导致trivial solutions。

## 3.2 推荐的local models

估计多个local models的思想在O'connor[6]中被提出，它通过对rating matrix进行item-wise聚类、并为每个使用最近邻CF生成的cluster估计一个独立的local model来进行rating prediction。

Xu、【19】开发了一个方法会将users和items进行co-clusters，并在每个cluster（通过不同的CF方法，包括item-based最近邻方法）上估计一个独立的local model。一个user-item pair的predicted rating是来自于对该用户具有最大weight的subgroup的prediction。

Lee[14,15]提出了一个方法，它依赖于：rating matrix是本地low-rank。首先，neighborhoods被标记成为在user-item pairs周围的anchor points，它基于一个衡量users和items的pairs间的距离函数来生成，对于每个neighborhood会估计一个local low-rank model。该估计会以一种迭代的方式进行，其中：首先latent factors表示anchor points会被估计，接着基于与这些anchor points与 observed entries的相似度，latent factors会被重新估计，直到收敛。prediction的计算被看成是一个local models的convex组合，它会通过相对应的local anchor point的相似度对user-item pair进行加权。

GLSLIM则有些不同：

i) 在上述提到的工作中，只有local models会被考虑到；而GLSLIM也会计算一个global model，它对于每个user具有一个个性化的factor来决定在global和local信息间的相互影响（interplay）。
ii) GLSLIM会更新users的assignment到subsets上，可以更好地估计local models。
iii) Lee[14,15]使用user和item latent factors，而GLSLIM则关注于item-item models
iiv) 在[6]中，作者使用item clusters，在[19]中作者使用co-clusters，在[14,15]中他们使用user-item anchor points。而GLSLIM则使用user subsets。

# 4.提出的方法

## 4.1 动机1

一个全局的item-item model可能捕捉用户集合的偏好不够充分，特别是当它们是具有多样、并且有时与偏好相左的用户子集。一个示例是：当local item-item models（iitem-item models会捕获在user subsets内的相似度）有效时，会胜过捕获全局相似度的item-item model（如图1所示）。它描绘了两个不同datasets的training matrix R，它们饮食了两个不同的user subsets。Item i是我们尝试去计算predictions的target item。在示例中，predictions通过使用一个item-item cosine similarity-based方法来计算。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/470a203548380d4a8e512efc24ba3a176286182eecab083a46f33a26ba166713ff54c96d224e3959282213e150d71766?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=1.png&amp;size=750">

图1

在左边的dataset中，图1a存在一些items，它们只被一个subset的users评过分，另外也有一些items它们被两个subset的用户集评过分。当为user-subset A估计时，（对比为user-subset B、对比整个matrix），Item c和i具有不同的相似度。特别的，它们的相似度对于subset B的用户来说为0（因为item i没有被该subset的用户评过分），但它对于subset A的用户来说是非零的（我们可以进一步假设：在该示例中的泛化性没有损失，它是很高的）。接着，i和c间的相似度会是在global case中计算的均值。因此，为该dataset的user subsets估计该local item-item similarities可以帮助捕获user-subsets A和B的多样性偏好，如果我们只通过全局方式计算它们会缺失这些。

然而，当使用item j来为item i做预测时，全局估计和为subset A做出local估计的相似度会相同，因为，他们只会被subset A的users评过分。对于该dataset的相同holds见图1b，



# 参考

- 1.[https://www-users.cs.umn.edu/~chri2951/recsy368-christakopoulouA.pdf](https://www-users.cs.umn.edu/~chri2951/recsy368-christakopoulouA.pdf)
