---
layout: post
title: 序列建模DIF介绍
description: 
modified: 2023-07-17
tags: 
---

香港科技大学团队在《Decoupled Side Information Fusion for Sequential Recommendation》提出了一种使用side information的序列建模新方案。

# 摘要

序列推荐（SR）中的边信息融合（Side Information Fusion）旨在有效利用各种边信息（side information）来提高下一个item的预测效果。大多数最先进的方法都建立在自注意力（self-attention）网络之上，并专注于探索各种解决方案，以**在注意力层之前将item embedding和side-info embedding进行整合**。然而，我们的分析表明，**各种类型embedding的早期整合由于秩瓶颈（rank bottleneck）限制了注意力矩阵的表现力，并限制了梯度的灵活性**。此外，它涉及不同异构信息资源之间的混合相关性（mixed correlations），这给注意力计算带来了额外的干扰。受此启发，我们提出了一种用于序列推荐的解耦边信息融合（DIF-SR），它将边信息从输入层移动到注意力层，并解耦了各种边信息和item表示的注意力计算。我们从理论和实证上展示了所提出的解决方案**允许更高秩的注意力矩阵和灵活的梯度，以增强边信息融合的建模能力**。此外，还提出了**辅助属性预测器**，以进一步激活边信息与item表示学习之间的有益交互。在四个真实世界数据集上的广泛实验表明，我们提出的解决方案稳定地超越了最先进的SR模型。进一步的研究表明，我们提出的解决方案可以轻松地集成到当前基于注意力的SR模型中，并显著提升性能。我们的源代码可在 https://github.com/AIM-SE/DIF-SR 上获取。

# 1.介绍

序列推荐（SR）的目标是：根据用户的历史行为来模拟他们动态的偏好，并推荐下一个item。随着在线场景中广泛而实际的应用，SR已经成为一个越来越吸引人的研究话题。提出了多种基于深度学习的解决方案[8, 17, 31]，并且基于自注意力[27]的方法[11, 25, 28]成为具有竞争力的主流解决方案。在基于自注意力方法的近期改进方法中，**一个重要的分支是与边信息融合[16, 33, 34, 37]相关的**。与仅使用item ID作为item属性的先前解决方案不同，边信息（如其他项目属性和评分）被考虑在内。**直观地说，高度相关的信息可以有益于推荐**。然而，如何有效地将边信息融合到推荐过程中仍然是一个具有挑战性的开放问题。

许多研究工作致力于在推荐的不同阶段融合side information。具体来说，早期的尝试FDSA [34]结合了两个独立的自注意力块分支，用于item和特征，并在最后阶段进行融合。S3-Rec [37]在预训练阶段使用自监督属性预测任务。然而，FDSA中项目和边信息表示的独立学习以及S3-Rec中的预训练策略，很难允许边信息(side info)直接与item自注意力进行交互。

最近，一些研究设计了将边信息嵌入（side-info embedding）整合到注意力层（attention layer）之前的item表示中的解决方案，以实现边信息感知的注意力。ICAI-SR [33]在注意力层之前使用属性到item的聚合层来整合边信息（side-info）到item表示中，并为训练使用独立的属性序列模型。NOVA [16]提出将纯item ID表示和整合边信息（side-info）的表示同时输入到注意力层，后者仅用于计算注意力的key和query，保持value的非侵入性。

尽管取得了显著的改进，当前基于早期整合的解决方案[16, 33]仍然存在几个缺点。

- 首先，我们观察到，在注意力层之前整合embedding会遭受注意力矩阵的秩瓶颈，导致注意力得分表示能力较差。这是因为**先前解决方案的注意力矩阵的秩本质上受到多头Q-K（multi-head query-key）的下投影（down-projection）size $d_h$的限制，这通常比矩阵可以达到的要小**。我们在第4.2.4节中进一步从理论上解释了这种现象。
- 其次，在复合embedding空间上执行attention可能会导致随机干扰，其中，**来自各种信息资源的混合embedding不可避免地会关注无关信息**。输入层中位置编码的类似缺点已被讨论[5, 12]。
- 第三，由于整合嵌入（integrated embedding）在整个注意力块（attention block）中仍然不可分割，早期整合迫使模型开发复杂而沉重的整合解决方案和训练方案，以实现各种边信息的灵活梯度。**使用简单的融合解决方案（例如广泛使用的加法融合），所有嵌入在训练中共享相同的梯度，这限制了模型学习边信息编码相对于item嵌入的相对重要性**。

为了克服这些限制，我们提出了一种用于序列推荐的**解耦边信息融合（DIF-SR）**。受到**解耦位置嵌入[2, 5]**成功经验的启发，我们提出要彻底探索和分析**解耦嵌入（decoupled embedding）**在序列推荐中边信息融合的效果。具体来说，**我们不是早期整合，而是将融合过程从输入层移动到注意力层**。我们通过为注意力层中的每个属性和item分别生成key和query来解耦各种边信息（side-info）以及项目嵌入（item embedding）。**然后，我们使用融合函数融合所有的注意力矩阵**。这种简单而有效的策略直接使我们的解决方案突破了秩瓶颈，从而增强了注意力机制的建模能力。图1显示了当前基于早期整合解决方案和我们的解决方案在相同embedding大小d和head投影（head projection）大小$$d_h$$下的秩比较。**我们的解决方案避免了由于异构嵌入的混合相关性引起的不必要的注意力随机性**。同时，它还实现了灵活的梯度，以适应不同场景中各种边信息的学习。我们进一步提出在多任务训练方案中使用轻量级的辅助属性预测器（AAP），以更好地激活边信息，对学习最终表示产生有益的影响。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/e6c0f3b8aa6ce74e0f152c047bc25a4376c6ae79046d51c785dfdd4631e06386b6212987eb65d8c29fe7d272d8b071cb?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 注意力矩阵的秩：比较基于早期集成嵌入的解决方案（即SASRecF和NOVA）与我们提出的DIF-SR在注意力分数矩阵的平均秩方面的差异。嵌入的早期集成导致注意力矩阵的秩降低，限制了表现力。

实验结果表明，我们提出的方法在序列推荐的四个广泛使用的数据库上超越了现有的基础SR方法[8, 11, 25, 26]和具有竞争力的边信息集成SR方法[16, 33, 37]，包括亚马逊美容、体育、玩具和Yelp。此外，我们提出的解决方案可以轻松地集成到基于自注意力的基础SR模型中。对两个代表性模型[11, 25]的进一步研究表明，当基础SR模型集成了我们的模块时，取得了显著的改进。对注意力矩阵的可视化还提供了对解耦注意力计算和注意力矩阵融合合理性的解释。

我们的贡献可以总结如下：

- 我们提出了DIF-SR框架，它能够有效地利用各种边信息进行序列推荐任务，具有更高的注意力表示能力和灵活性，以学习边信息的相对重要性。
- 我们提出了新颖的DIF注意力机制和基于AAP的训练方案，这些可以轻松地集成到基于注意力的推荐系统中并提升性能。
- 我们从理论和实证上分析了所提出解决方案的有效性。我们在多个真实世界数据集上实现了最先进的性能。全面的消融研究和深入分析展示了我们方法的鲁棒性和可解释性。


# 2.相关工作

## 2.1 序列推荐（Sequential Recommendation）

序列推荐（SR）模型的目标是：从用户的历史序列交互数据中捕捉用户的偏好，并进行下一个item的预测。早期的SR研究[6, 10, 23, 38]通常基于马尔可夫链假设和矩阵分解方法，这些方法难以处理复杂的序列模式。随后，受到深度学习技术在序列数据上成功应用的启发，许多研究者提出使用神经网络，如卷积神经网络（CNNs）[26, 32]、循环神经网络（RNNs）[8, 19, 21, 22, 30, 36]、图神经网络（GNNs）[1]和变换器[11, 25, 28]来模拟用户-item交互。具体来说，基于自注意力的方法，如SASRec[11]和BERT4Rec[25]，因其能够捕捉item之间的长期依赖关系而被认为具有强大的潜力。最近，提出了许多针对基于自注意力解决方案的改进，考虑了个性化[28]、项目相似性[15]、一致性[7]、多种兴趣[5]、信息传播[29]、伪先前项目增强（pseudo-prior items augmentation）[18]、模式（motifs）[3]等。然而，大多数当前的SR方法通常假设只有item ID可用，并没有考虑项目属性等边信息，忽略了这些高度相关的信息可以提供额外的监督信号。与基础的SR解决方案不同，我们的工作与边信息感知的SR紧密相关，旨在设计有效的融合方法，更好地利用各种边信息。

## 2.2 序列推荐的side info融合

边信息感知（side-info aware）的SR已经成为推荐系统领域公认的重要研究方向。近年来，在基于注意力的SR模型中，边信息融合（side-info fusion）也得到了广泛的探索。

- FDSA [34]：采用不同的自注意力块来编码item和边信息，它们的表示直到最后阶段才融合。
- S3-Rec [37]：采用预训练来包含边信息。具体来说，设计了两个预训练任务来利用这些有意义的监督信号。

这些方法验证了边信息可以帮助下一个项目的预测。**然而，它们并没有有效且直接地使用边信息来帮助item表示和预测的注意力聚合**。

一些最近的工作尝试在注意力层（attention layer）之前将边信息嵌入到item嵌入中，以便最终表示的注意力学习过程可以考虑边信息（side info）。

- 早期的工作如p-RNN[9]：通常使用简单的串联直接将边信息注入到item表示中。
- ICAI-SR [33]：使用item-属性聚合模型根据构建的异构图计算item和属性embedding，然后将这些融合的embedding输入到序列模型中以预测下一个item。并行实体序列模型用于训练。
- NOVA[16]：提出在不损害item嵌入空间一致性的情况下融合边信息，其中集成的embedding仅用于Key和Query，value来自纯item ID嵌入。

尽管有所改进，我们认为这些**基于集成embedding和异构信息资源之间的耦合注意力**计算的方法存在包括有限的表示能力、不灵活的梯度和复合嵌入空间等缺点。

# 3.问题公式

在本节中，我们明确了边信息集成序列推荐的研究问题。

- 设I、U：分别表示item和user的集合。
- 对于用户$u \in U$，用户的历史交互可以表示为： $𝑆_𝑢= [ 𝑣_1, 𝑣_2, \cdots, 𝑣_𝑛]$，其中：$𝑣_𝑖$表示按时间顺序排列的序列中的第i次交互。
- 边信息：可以是用户的属性、item的属性、和行为的属性，这些为预测提供了额外的信息。根据先前工作[16]的定义，边信息包括与项目相关的信息（例如，品牌、类别）和与行为相关的信息（例如，位置、评分）。

假设我们有p种类型的边信息。那么，对于边信息集成的序列推荐，每次交互可以表示为：

$$
v_i=(I_i , f_i(1), \cdots, f_i(p))
$$

其中: 

- $f_i(j)$表示序列中第i次交互的第j种类型的边信息
- $I_i$: 表示第i次交互的item ID

给定这个序列$S_u$，我们的目标是预测用户u最有可能与之交互的item $Item_{pred} \in I$： 

$$
Item_{pred}=I^{(\widehat{k})}
$$

其中：

- $\widehat{k}=argmax_{k} ​P(v_{n+1}=(I^{(k)}, ⋅) \mid S_u)$


# 4.方法

在本节中，我们将介绍我们的DIF-SR，以有效且灵活地融合边信息以帮助下一个项目预测。DIF-SR的整体架构如图2所示，由三个主要模块组成：嵌入模块（第4.1节）、解耦边信息融合模块（第4.2节）和带有AAP的预测模块（第4.3节）。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/3023132d1e940393d8ae7881c372eac7baed82c70b9ff45fc68523d458bb662f2e67601b0f0c6ac9a82f4c8f24bd3ff5?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 提出的框架总览

# 4.1 嵌入模块

在嵌入模块中，输入序列 $S_u=[v_1, v_2, \cdots, v_n]$ 被输入到item embedding层和各种属性embedding层，以获取item嵌入 $E_{ID}$ 和边信息嵌入 $E^{f_1}, \cdots, E^{f_p}$：

$$
E^{ID} = \epsilon_{id}([I_1, I_2, \cdots, I_n]) \\
E^{f_1} = \epsilon_{f_1}([f_1^{(1)}, f_2^{(1)}, \cdots, f_n^{(1)}]) \\
\cdots \\
E^{f_p} = \epsilon_{f_p}([𝑓_1^(𝑝), 𝑓_2^{(p)}, \cdots, 𝑓_𝑛^{(p)}])
$$

其中：

- $\epsilon$表示相应的embedding layer，它可以将该item和不同item属性编码成vectors。
- $M_{𝑖𝑑} \in R^{\mid I \mid×𝑑}, 𝑀_{𝑓_1} \in R^{ \mid 𝑓_1 \mid × 𝑑_{𝑓_1}} , \cdots, 𝑀_{𝑓_𝑝} \in R^{\mid 𝑓_𝑝 \mid × 𝑑_{𝑓_𝑝}}$: look-up embedding矩阵
- $\mid \cdot \mid$ 表示相应的不同items和多种side info的总数
- d和$d_{f_1}, \cdots, d_{f_p}$：分别表示item embedding维度、side info的维度

请注意，由所提出的DIF模块中的操作支持，不同类型的属性可以灵活地使用嵌入维度。在第5.4.3节中进一步验证了，**我们可以为属性应用比item更小的维度，从而在不损害性能的情况下大大提高网络的效率**。然后，嵌入模块得到输出嵌入:

$$
E_{ID​} \in R^{n×d}，E_{f_1} \in R^{n×d_{f_1}}​​, \cdots, E_{f_p} \in R^{n×d_{f_p}}​
$$​

## 4.2 解耦边信息融合模块

我们首先在第4.2.1节中指定了模块的整体层结构。为了更好地说明我们提出的DIF注意力，我们在第4.2.2节中讨论了先前解决方案[11, 16]的自注意力学习过程。之后，我们在第4.2.3节中全面介绍了所提出的DIF注意力。最后，在第4.2.4节中，我们对DIF在增强模型表现力方面的理论分析进行了阐述，包括注意力矩阵的秩和梯度的灵活性。

## 4.2.1 层结构

如图2所示，解耦边信息融合模块包含多个堆叠的序列组合DIF注意力层和前馈层块。该块结构与SASRec[11]相同，只是我们将原始的多头自注意力替换为多头DIF注意力机制。每个DIF块涉及两种类型的输入，即当前item表示和辅助边信息嵌入，然后输出更新后的item表示。请注意，辅助边信息嵌入不会逐层更新，以节省计算量并避免过拟合。设：

- $R_i^{(ID)} \in R^{n×d}$：表示第i块的输入item表示。

该过程可以表示为： 

$$
R_{i+1}^{(ID)}= LN(FFN(DIF(R_i^{(ID)}, E^{f_1}, \cdots, E^{f_p}))) \\
R_1^{(ID)}=E_{ID} 
$$

...(2)(3)

其中：

- FFN代表全连接前馈网络，LN表示层归一化。

### 4.2.2 先前注意力解决方案

图3展示了将边信息融合到项目表示更新过程中的先前解决方案的比较。在这里，我们重点关注自注意力计算，这是几种解决方案的主要区别。 

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/42e9776553772d71c177770487a93cd5b9dbb88f0e89d62e2cfbe6e57bdc30cf2b77c86c33af6e4b95ce980b63e72e06?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3 各种解决方案的项目表示学习过程的比较。(a) SASRecF：SASRecF将边信息融合到item表示中，并使用融合后的项目表示来计算K、Q和V。(b) NOVA-SR：NOVA-SR使用融合的项目表示来计算K和Q，同时保持V的非侵入性。(c) DIF-SR：与早期融合以获得融合的item表示不同，所提出的DIF-SR解耦了各种边信息的注意力计算过程，以生成融合的注意力矩阵，从而提高表示能力，避免混合相关性，并实现训练梯度的灵活性。

SASRecF：如图3(a)所示，该解决方案直接将边信息嵌入到项目表示中，并在集成嵌入上执行普通的自注意力，这是从原始的SASRec [11]扩展而来的。 

设输入长度为n，hidden size为d，多头Q-K下投影size为$d_h$，我们可以设：

- $R \in R^{n×d}$表示集成嵌入
- $W_Q^i, W_K^i, W_V^i \in R^{d×d_h}$ ​：$i \in [h]$表示h个头的查询、键和值投影矩阵（$d_h=d/h$），

然后注意力分数的计算可以形式化为：

$$
SAS\_att^i ​ =(RW\_Q^i)(RW\_K^i)^T
$$

...(4)

然后每个头的输出可以表示为： 

$$
SAS_head^{i}=σ(\frac{SAS\_att^{i}}{\sqrt{d}}(RW_V^i) 
$$

...(5)

其中：

- σ 表示Softmax函数。 尽管这个解决方案允许边信息直接影响项目表示的学习过程，但观察到这种方法有一个缺点，即侵犯了项目表示[16]。

NOVA：为了解决上述问题，[16]中的工作提出了一种非侵入式的边信息融合方法。如图3(b)所示，NOVA从集成嵌入$R \in R^{n×d}$计算Q和K，而从纯item ID嵌入$R (ID) \in R^{n×d}$计算V。同样， 𝑊 𝑖 𝑄 , 𝑊 𝑖 𝐾 , 𝑊 𝑖 𝑉 ∈ 𝑅 𝑑 × 𝑑 ℎ , 𝑖 ∈ [ ℎ ] W i Q ​ ,W i K ​ ,W i V ​ ∈R d×d h ​ ,i∈[h] 表示查询、键和值的投影矩阵： NOVA_att 𝑖 = ( 𝑅 𝑊 𝑖 𝑄 ) ( 𝑅 𝑊 𝑖 𝐾 ) 𝑇 , ( 6 ) NOVA_att i ​ =(RW i Q ​ )(RW i K ​ ) T ,(6) 然后每个头的输出可以表示为： NOVA_head 𝑖 = 𝜎 ( NOVA_att 𝑖 / 𝑑 ℎ ) ( 𝑅 ( 𝐼 𝐷 ) 𝑊 𝑖 𝑉 ) , ( 7 ) NOVA_head i ​ =σ(NOVA_att i ​ / d h ​ ​ )(R (ID) W i V ​ ),(7) 在NOVA方法中，通过将查询和键从集成嵌入中计算，而将值从纯项目ID嵌入中计算，它试图保持项目表示的一致性，同时允许边信息在注意力计算中发挥作用。这种方法减少了边信息对项目表示的直接影响，但仍然允许它通过注意力机制影响最终的项目表示。


### 4.2.3 DIF注意力

我们认为，尽管NOVA解决了Value的侵入问题，但使用集成嵌入（integrated embedding）来计算Key和Value仍然存在复合注意力空间（compound attention space）的问题，以及在注意力矩阵的秩和训练梯度灵活性方面表现力的降低。支持这种观点的理论分析在第4.2.4节中展示。

因此，与之前将属性embedding注入item表示以形成混合表示（mixed representation）的研究不同，我们提出利用解耦边信息融合（decoupled side information fusion）解决方案。如图3(c)所示，在所提出的解决方案中，所有属性自身进行自注意力以生成解耦的注意力矩阵，然后将其融合到最终的注意力矩阵中。解耦的注意力计算通过打破由头投影大小$d_h$限定的注意力矩阵的秩瓶颈，提高了模型的表现力。它还避免了梯度的不灵活性和不同属性与项目间的不确定交叉关系，以实现合理且稳定的自注意力。 

给定：

- 输入长度n
- item hidden size为d
- 多头Query-Key下的投影尺寸$d_h$，

对于item表示$R^{(ID)} \in R^{n×d}$，我们有：

- $W_Q^i, W_K^i, W_V^i ​\in R^{d×d_h}, i \in [h]$ 分别表示对应于h（$d_h=d/h$）个heads的query、key、value的投影矩阵

然后item表示的注意力分数计算如下：

$$
att_{(ID)}^i=(R^{(ID)} W_Q^i)(R^{(ID)} W_K^i)^T
$$

...(8)

与之前的工作不同，我们还为每个属性生成多头注意力矩阵，有属性嵌入：$E^{f_1} \in R^{n×d_{f_1}}, \cdots, E^{f_p} \in R^{n×d_{f_p}}$。注意我们有: $𝑑_{𝑓_𝑗}≤𝑑, 𝑗 \in [𝑝]$ 以避免过度参数化并减少计算开销。 

然后我们有对应的 $W_Q^{(f_j)i}, W_K^{(f_j)i}, W_V^{(f_j)i} ​ \in R^{d×d_{h_j}}, i \in [h], j \in [p]$，表示h个头的query、key和value投影矩阵（$d_{hj}=d_{fj}/h$）： 

$$
att_(f_1)^i =(E^{f_1} ​ W_Q^{(f_1)i}) (E^{f_1} W_K^{(f_1)i})^T,\\
\cdots \\
att_(f_p)^i =(E^{f_p} ​ W_Q^{(f_p)i}) (E^{f_p} W_K^{(f_p)i})^T,\\
$$

...(9)

然后我们的DIF注意力通过融合函数F 融合所有的注意力矩阵，该函数在先前的工作[16]中进行了探索，包括加法、连接和门控机制，并得到每个头的输出： 

$$
DIF\_att^i ​ =F(att i (ID) ​ ,att i (f 1 ​ ) ​ ,...,att i (f p ​ ) ​ ), \\
DIF\_head^i ​=\sigma(\frac{DIF\_att^i}{\sqrt{d}})(R^{(ID)} W_V^i) 
$$

...(10)

最后，所有注意力头(attention heads)的输出被拼接起来（concatenated）并输入到前馈层（feed-forward layer）。

# 

略

# 

- 1.[https://arxiv.org/pdf/2204.11046](https://arxiv.org/pdf/2204.11046)