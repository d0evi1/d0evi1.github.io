---
layout: post
title: Learning from Logged Implicit Exploration Data介绍
description: 
modified: 2016-08-13
tags: 
---

我们来看下2010提出的《Learning from Logged Implicit Exploration Data》，LinUCB的姐妹篇，具体方法如下：

# 摘要

对于在"contextual bandit"或"partially labeld" setting中（只有一个选中的action的值会被学到）使用非随机曝光数据，我们提供了一个合理和一致的基础。在许多settings中的主要挑战是曝光策略（exploration policy）并非显式可知（它会记录“离线(offline)”数据）。之前提出的解决方案是需要在学习期间对actions进行控制，记录下随机曝光、或者以一种可重复的方式遗忘式的选择actions。这里提出的该技术会解除这些限制，在给定从历史数据中(没有随机发生或者被记录)的特征时，允许学习一个policy选择actions。

# 介绍

考虑广告展示问题，一个搜索引擎公司会选择一个ad来展示给它的目标用户。当用户点击展示的ad时，会产生来自广告主付费的收益(Revenue)。该问题的经济特性，产生了许多著名的公司。

在讨论提出的方法前，我们将该问题公式化，接着解释为什么许多常见方法会失败。

**对于contextual exploration的warm-start问题**：

假设：

- X是一个任意输入空间
- $$A=\lbrace 1, \cdots, k \rbrace$$是一个actions集合。

contextual bandit problem的一个实例，可以通过在tuples $$(x, \vec{r})$$上的一个分布D来指定，其中$$x \in X$$是一个输入，$$\vec{r} \in [0, 1]^k$$是一个关于rewards的向量。事件（Events）会以一个round-by-round的方式发生，其中每个round t有：

- 1.抽取 $$(x, \vec{r}) \sim D$$，并公布x
- 2.算法会选择一个action $$a \in A$$，一个关于x和历史信息的函数
- 3.真实世界中，公布action a的reward为$$r_a$$，但对于$$a'\neq a$$不会是$$r_{a'}$$

理解以下这点很重要：这不是一个标准的监督学习问题，因为其它actions $$a' \neq a$$的reward并没有透露。

在该setting中标准的目标是：在多轮交互上最大化rewards $$r_a$$的求和。为了这样做，使用之前记录的events来在第一轮交互上形成一个好的policy是很重要的。因而，这就是一个"warm start"问题。正式的，给定一个dataset，形如：$$S=(x, a, r_a)^*$$，它通过一个不受控的logging policy交互生成，我们希望构建一个policy h来最大化（近似或逼近）：

$$
V^h := E_{x,r}
$$

有许多方法会失败。

我们的方法分为三个step:

- 1.对于每个event $$(x,a,r_a)$$，使用回归(regression)来估计logging policy会选择action a的概率(probability) $$\hat{\pi}(a \mid x)$$。这里，“probability”是随时间移动的——我们可以想像：在不同时间点，采用一个均匀随机的方式从policies集合（可能是deterministic）中抽取。
- 2.对于每个event $$(x,a,r_a)$$，根据$$(x,a,r_a, 1/max \lbrace \hat{\pi}(a \mid x), \tau \rbrace)$$，创建一个人造的controlled contextual bandit event，其中$$\tau > 0$$是一些参数。数量$$1 / max \lbrace \hat{\pi}(a \mid x), \tau \rbrace$$，是一个importance weight，它指定了当前event是有多重要。参数$$\tau$$对于数值稳定性很重要。
- 3.应用一个ooffline contextual bandit算法到人造的contextual bandit events集合上。在我们第二个实验集合（4.2节）中，argmax regressor的一个变种是：使用两个临界修改：(a) 我们将argmax的范围限定在具有正概率的那些actions上 (b) 我们importance weight events，以便训练过程会强调对于每个action平等的好估计。需要强调的是，在本paper中的理论分析可以应用于对于在contextual bandit events学习上的任何算法——我们选择该方法是因为它很容易在已存在方法上进行简单修改。

上述方法与前面提到的Propensity Score方法相似。相对它来说，我们使用一个不同的概率定义，当logging policy完全确定时该概率不必是0或1.

考虑该方法时，会提出三个关键问题。

- 1.当给定特征x时，logging policy会确定式选中一个action (ad) a，$$\hat{\pi}(a \mid x)$$意味着什么？基本observation是，一个policy，在day 1会确定选中action a，接着在day 2会确定选中action b，这会被看成是在action a和b间使用概率0.5(当events数目在每天都相同时，并且events间是IID的）进行随机化。因而，在给定在logged events的时间间隔上的特征x，$$\hat{\pi}(a \mid x)$$是一个关于action a被展示的期望频率的估计。第3节中，我们会展示该方法在期望上是合理的，它提供了关于new policy的值的一种无偏估计。
- 2.在$$\hat{\pi}(a \mid x)$$上的必然errors是如何影响该过程的？结果表明它们有影响，取决于$$\tau$$。对于非常小的$$\tau$$值，$$\hat{\pi}(a \mid x)$$的估计必须极其精准来生成好的效果；而对于更大的值，会需要更小的accuracy。第3.1节会证明健壮性
- 3.参数$$\tau$$是如何影响最终结果的？当在估计过程中创建一个bias时，结果表明该bias的形式是很轻微的并且相对合理的——基于条件x具有低频展示的actions具有一个under-estimated value。这与期望的对于没有频北京的actions的限制是一致的。

# 2.公式设定和假设

假设$$\pi_1, \cdots, \pi_2$$是T个policies，其中，对于每个t，$$\pi_t$$是一个函数，它会将X的input映射到在A上的一个(possibly deterministic)分布。该学习算法会给定一个关于T个样本的dataset，每个为：$$(x,a,r_a) \in X \times A \times [0,1]$$，其中(x,r)从D中抽取，action $$a \sim \pi_t(x)$$根据第t个policy被选中。我们将该随机过程通过$$(x,a,r_a) \sim (D, \pi_t(\cdot \mid x))$$来表示。相似的，与T个policies的交互会产生一个关于T条样本的序列S，我们表示为：$$S \sim (D, \pi_i(\cdot \mid x))_{i=1}^T$$。该learner不会给出关于$$\pi_t$$的先验知识。

**offline policy estimator**：给定一个数据集：

$$S = \lbrace (x_t, a_t, r_{t,a_t}) \rbrace_{t=1}^T $$

...(1)

其中：$$\forall t, x_t \in X, a_t \in A, r_{t,a_t} \in [0, 1]$$，我们会形成一个predictor $$\hat{\pi}: X \times A \rightarrow [0,1]$$，接着对于一个policy h的值，使用它与一个阀值$$\tau \in [0,1]$$来形成一个offline estimator。

正式的，给定一个new policy h: $$X \rightarrow A$$以及dataset S，将estimator 定义为：

$$
\hat{V}_{\hat{\pi}}^h(S) = \frac{1}{|S|} \sum\limits_{(x,a,r) \in S} \frac{r_a I (h(x)=a)}{max \lbrace \hat{\pi}(a \mid x), \tau \rbrace}
$$

...(2)

其中，$$I(\cdot)$$是指示函数（indicator function）。当没有二义时，使用符号$$\hat{V}_{\hat{\pi}}^h$$。$$\tau$$的目标是sum中单独项的上界，与之前的robust importance sampling方法相似。

# 3.理论成果

我们接着展示了我们的算法和主要理论成果。主要思想有两个：

- 1.我们具有一个policy estimation阶段，其中我们估计未知的logging policy;
- 2.我们具有一个policy optimization阶段，其中我们会使用我们估计的logging policy。

我们的主要结果，提供了一个泛化边界——解决estimation和optimization error对于total error贡献的问题。

logging policy $$\pi_t$$可以是确定的(deterministic)，这意味着：依靠在logging policy中的随机化的常用方法是不可用的。我们会在下面展示：当现实是IID条件、以及policy会随actions变化时，这是ok的。我们会有效使用算法中的随机化来替代现实中的随机。

一个基本的要求是，estimator对于一个随机policy(stochastic policy)来说是在期望上是等价的，定义如下：

$$
\pi(a | x) = E_{t \sim UNIF(1,\cdots,T)} [\pi_t (a | x)]
$$

...(3)

其中，$$UNIF(\cdots)$$表示均匀分布。stochastic policy $$\pi$$会在T policies $$\pi_t$$上随机均匀选择一个action。我们的第一个结果是，当现实中根据$$\pi$$或者policies序列$$\pi_t$$进行选择actions时，estimator的期望值是相同的。尽管该结果和它的证明是简单的，它是本paper后面部分的基础。注意，policies $$\pi_t$$可以是任意的，但我们会假设，它们不依赖于用于evaluation的数据。该假设只对证明来说是必要的，可以在实际中放宽些，如4.1节。

**定理3.1**: 对于任意contextual bandit问题D，在T轮上做出相同的抽取，对于任意可能的stochastic policies $$\pi_t(a \mid x)$$的序列，以及对于任意的预测 $$\hat{\pi}$$，有：

$$
E_{S \sim (\pi_i(\cdot|x))_{i=1}^T} \hat{V}_{\hat{\pi}}^h(S) = E_{(x,\vec{r}) \sim D, a \sim \pi(\cdot |x)} \frac{r_a I (h(x)=a)}{max \lbrace}
$$
 
当T policies被用于更简单、更标准的setting（其中：会使用单个确定的stochastic policy）时，该定理与我们的estimator的期望值相关。

## 3.1 Policy Estimation

在本节中，我们展示了对于$$\tau$$和$$\hat{\pi}$$的一个合适选择，在评估new policies h时，我们的estimator足够精准。我们会进而使用前面章节的简化版，它展示了我们可以将数据看成是由一个确定的stochastic policy $$\pi$$生成，比如：对于所有t来说，$$\pi_t = \pi$$。

对于一个给定$$\pi$$的estimate $$\hat{\pi}$$，定义了一个函数的"regret"：$$reg:X \rightarrow [0,1]$$：

$$
reg(x)=max_{a \in A} [ \pi(a \mid x) - \hat{\pi}(a \mid x))^2]
$$

...(2)

我们不会使用$$l_1$$和$$l_\infty$$ loss，因为它们比$$l_2$$ loss更难最小化。我们接下来的结果是：new estimator是一致的。在接下来的理论中，$$I(\cdot)$$表示了indicator function，$$\pi(a \mid x)$$是logging policy在输入x上选择action a的概率，而$$\hat{V}_{\hat_{\pi}}^h$$是由等式(2)基于参数$$\tau$$的定义。

**引理3.1**：假设$$\hat{\pi}$$是从X到在actions A分布上的任何函数。假设：$$h: X \rightarrow A$$是任意的deterministic policy。假设：$$V^h(x) = E_{r \sim D(\cdot \mid x)}[r_{h(x)}]$$表示在input x上执行policy h的期望值。我们有：

$$
E_x [I(\pi(h(x) | x) \geq \tau) \cdot (V^h(x) - \frac{\sqrt{reg(x)}}{\tau}] \geq E[\hat{V}_{\hat{\pi}}^h] \geq E_x [ I(\pi(h(x) \mid x) \geq \tau) \cdot \frac{\sqrt{reg(x)}}{\tau}
$$

在上式中，期望$$E[\hat{V}_{\hat{\pi}}^h]$$会在关于T个tuples (x,a,r)的所有序列上计算，其中$$(x,r) \sim D$$以及$$a \sim \pi(\cdot \mid x)$$

该引理限制了在我们的estimate $$V^h(x)$$上的bias。有两个bias来源——一个来自于在估计$$\pi(a \mid x)$$的$$\hat{\pi}(a \mid x)$$引入的error，另一个来自于阀值$$\tau$$。对于第一个来源，我们根据squared loss分析了结果，
 
# 参考

- 1.[https://arxiv.org/pdf/1812.02353.pdf](https://arxiv.org/pdf/1812.02353.pdf)
- 2.[Minmin Chen video](https://www.youtube.com/watch?v=HEqQ2_1XRTs)
- 3.[Minmin Chen ppt](https://coladrill.github.io/2019/03/30/Youtube案例研究/)
- 4.[CFNRNN](https://openreview.net/pdf?id=S1dIzvclg)