---
layout: post
title: Learning from Logged Implicit Exploration Data介绍
description: 
modified: 2016-08-13
tags: 
---

我们来看下2010提出的《Learning from Logged Implicit Exploration Data》，LinUCB的姐妹篇，具体方法如下：

# 摘要

对于在"contextual bandit"或"partially labeld" setting中（只有一个选中的action的值会被学到）使用非随机曝光数据，我们提供了一个合理和一致的基础。在许多settings中的主要挑战是：**曝光策略（exploration policy）并非显式可知（它会记录“离线(offline)”数据）**。之前提出的解决方案是需要在学习期间对actions进行控制，记录下随机曝光、或者以一种可重复的方式遗忘式的选择actions。这里提出的该技术会解除这些限制，学习一个policy，使得：在给定来自历史数据的features（随机化没有出现或者被记录）下选择actions。

# 介绍

考虑广告展示问题，一个搜索引擎公司会选择一个ad来展示给它的目标用户。当用户点击展示的ad时，会产生来自广告主付费的收益(Revenue)。该问题的经济特性，产生了许多著名的公司。

在讨论提出的方法前，我们将该问题公式化，接着解释为什么许多常见方法会失败。

**对于contextual exploration的warm-start问题**：

假设：

- X是一个任意输入空间
- $$A=\lbrace 1, \cdots, k \rbrace$$是一个actions集合。

contextual bandit problem的一个实例，可以通过在tuples $$(x, \vec{r})$$上的一个分布D来指定，其中$$x \in X$$是一个输入，**$$\vec{r} \in [0, 1]^k$$是一个关于rewards的向量**。事件（Events）会以一个round-by-round的方式发生，其中每个round t有：

- 1.抽取 $$(x, \vec{r}) \sim D$$，并公布x
- 2.算法会选择一个action $$a \in A$$，可以是一个关于x和历史信息的函数
- 3.公布action a的reward为$$r_a$$，但不会公布$$a'\neq a$$是$$r_{a'}$$

理解以下这点很重要：这不是一个标准的监督学习问题，因为其它actions $$a' \neq a$$的reward并没有透露（**注：有点绕，即隐式的**）。

在该setting中标准的目标是：在多轮交互上最大化rewards $$r_a$$的求和。**为了这样做，使用之前记录的events来在第一轮交互上形成一个好的policy是很重要的。因而，这就是一个"warm start"问题**。

正式的，给定一个dataset，形如：$$S=(x, a, r_a)^*$$，它通过一个不受控的logging policy交互生成，我们希望构建一个policy h来最大化（近似或逼近）下面的期望：

$$
V^h := E_{(x,r) \sim D} [r_{h(x)}] 
$$

有许多方法尝试解决该问题的会失败：

- 1.监督学习：略
- 2.Bandit方法：略
- 3.Contextual Bandits：略
- 4.Exploration Scavenging：略
- 5. 倾向指数（Propensity Scores）, naively：当进行一个调查时，提问一个关于收入的问题，接着在不同收入级别上的回答者的比例，可以与基于参与该调查所选中的某些人的收入估计一个条件概率进行对比。给定该估计概率，结果可以被importance-weighted来估计在整个人口(population)上的平均调查结果。这里，该方法是有问题的，因为policy会做出决策，当记录数据是deterministic而非probabilistic。换句话说，对logging policy选择一个ad的概率的精准预测，意味着总是预测0或1, 这对于我们的目标是没用处的。尽管propensity scores的直接使用并不work，我们采用的该方法可以被认为是，对于一个propensity score的更好使用，下面会讨论。Lambert[4]提供了一个在互联网ad环境中关于propensity scoring的一个良好解释。

而我们的方法分为三个step:

- 1.**对于每个event $$(x,a,r_a)$$，使用回归(regression)来估计logging policy会选择action a的概率(probability) 为$$\hat{\pi}(a \mid x)$$**。这里，“probability”是随时间变化的——我们可以想像：在不同时间点，采用一个均匀随机的方式从policies集合（可能是deterministic）中抽取。
- 2.对于每个event $$(x,a,r_a)$$，根据$$(x,a,r_a, 1/max \lbrace \hat{\pi}(a \mid x), \tau \rbrace)$$，创建一个人造的controlled contextual bandit event，其中$$\tau > 0$$是一些参数。**$$1 / max \lbrace \hat{\pi}(a \mid x), \tau \rbrace$$这个量是一个importance weight，它指定了当前event是有多重要**。需要明确的是，参数$$\tau$$对于数值稳定性很重要。
- 3.应用一个offline contextual bandit算法到人造的contextual bandit events集合上。在我们第二个实验结果的集合（4.2节）中，采用的argmax regressor的变种使用了**两个关键修改之处：(a) 我们将argmax的范围限定在具有正概率的那些actions上 (b) 我们对events进行importance weight，以便训练过程会强调对于每个action平等的好估计**。需要强调的是，在本paper中的理论分析可以应用于对于在contextual bandit events学习上的任何算法——我们选择该方法是因为它很容易在已存在方法上进行简单修改。

上述方法与前面提到的Propensity Score方法相似。相对它来说，我们使用一个不同的概率定义，当logging policy完全确定时该概率不必是0或1.

当考虑该方法时，会有三个关键问题：

- 1.**$$\hat{\pi}(a \mid x)$$意味着什么**，假如logging policy在当给定特征x时，会确定式(deterministically)选中一个action (ad) a？基本observation是，一个policy如果在day 1会确定式选中action a、接着在day 2会确定选中action b，这可以被看成是在action a和b间使用概率0.5(当events数目在每天都相同时，并且events间是IID的）进行随机化。因而，在给定在logged events的时间间隔上的特征x，**$$\hat{\pi}(a \mid x)$$是一个关于action a被展示的期望频率的估计**。第3节中，我们会展示该方法在期望上是合理的，它提供了关于new policy的值的一种无偏估计。
- 2.**在$$\hat{\pi}(a \mid x)$$上的客观误差（inevitable errors）是如何影响该过程的？**结果表明它们有影响，取决于$$\tau$$。对于非常小的$$\tau$$值，$$\hat{\pi}(a \mid x)$$的估计必须极其精准来生成好的效果；而对于更大的值，会需要更小的accuracy。第3.1节会证明健壮性
- 3.**参数$$\tau$$是如何影响最终结果的？**当在估计过程中创建一个bias时，结果表明该bias的形式是很轻微的并且相对合理的——基于条件x具有低频展示的actions具有一个under-estimated value。这与期望的对于没有频北京的actions的限制是一致的。

# 2.问题设定和假设

假设：

- $$\pi_1, \cdots, \pi_2$$是T个policies，

对于每个t，$$\pi_t$$是一个函数，它会将X的input映射到在A上的一个可能的deterministic分布。该学习算法会给定一个关于T个样本的dataset，每个为：$$(x,a,r_a) \in X \times A \times [0,1]$$（注：0,1为reward），其中：

- $$(x,r)$$按第1节所述的方式从D中抽取
- action $$a \sim \pi_t(x)$$根据第t个policy被选中。

我们将该随机过程通过$$(x,a,r_a) \sim (D, \pi_t(\cdot \mid x))$$来表示。相似的，与T个policies的交互会产生一个关于T条样本的序列S，我们表示为：$$S \sim (D, \pi_i(\cdot \mid x))_{i=1}^T$$。该learner不会给出关于$$\pi_t$$的先验知识。

**offline policy estimator**：

给定一个数据集：

$$S = \lbrace (x_t, a_t, r_{t,a_t}) \rbrace_{t=1}^T $$

...(1)

其中：$$\forall t, x_t \in X, a_t \in A, r_{t,a_t} \in [0, 1]$$

**我们会形成一个predictor $$\hat{\pi}: X \times A \rightarrow [0,1]$$**，接着对于一个policy h的值，使用它与一个阀值$$\tau \in [0,1]$$来形成一个offline estimator。

正式的，给定一个new policy h: $$X \rightarrow A$$以及dataset S，将estimator 定义为：

$$
\hat{V}_{\hat{\pi}}^h(S) = \frac{1}{|S|} \sum\limits_{(x,a,r) \in S} \frac{r_a I (h(x)=a)}{max \lbrace \hat{\pi}(a \mid x), \tau \rbrace}
$$

...(2)

其中，$$I(\cdot)$$是指示函数（indicator function）。当没有二义时，使用符号$$\hat{V}_{\hat{\pi}}^h$$。$$\tau$$的目标是sum中单独项的上界，与之前的robust importance sampling方法相似。

# 3.理论成果

我们接着展示了我们的算法和主要理论成果。主要思想有两个：

- 1.我们具有一个policy estimation阶段，其中我们估计未知的logging policy;
- 2.我们具有一个policy optimization阶段，其中我们会使用我们估计的logging policy。

我们的主要结果，提供了一个泛化边界——解决estimation和optimization error对于total error贡献的问题。

logging policy $$\pi_t$$可能是确定的(deterministic)，这意味着：基于logging policy中的随机化的常用方法是不可用的。我们会在下面展示：当现实情况是IID条件、并且policy会随actions的不同而变化时，这是ok的。**我们会有效使用算法中的随机化标准方法来替代现实中的随机**。

**一个基本断言（假设）是：在期望上，estimator和一个随机policy(stochastic policy)是等价的**，定义如下：

$$
\pi(a | x) = E_{t \sim UNIF(1,\cdots,T)} [\pi_t (a | x)]
$$

...(3)

其中，$$UNIF(\cdots)$$表示均匀分布。

**stochastic policy $$\pi$$会在T policies $$\pi_t$$上随机均匀选择一个action**。我们的第一个结果是，当现实中根据$$\pi$$或者policies序列$$\pi_t$$进行选择actions时，我们的estimator的期望值与它是相同的。尽管该结果和它的证明是简单的，**它是本paper后面部分的基础**。注意，policies $$\pi_t$$可以是任意的，但我们会假设，它们不依赖于用于evaluation的数据。该假设只对证明来说是必要的，可以在实际中放宽些，如4.1节。

**定理3.1**: 对于在T轮上做出相同抽取的任意contextual bandit问题D，对于任意可能的stochastic policies $$\pi_t(a \mid x)$$的序列($$pi$$由上面求得)，以及对于任意的预测 $$\hat{\pi}$$，有：

$$
E_{S \sim (D,\pi_i(\cdot|x))_{i=1}^T} \hat{V}_{\hat{\pi}}^h(S) = E_{(x,\vec{r}) \sim D, a \sim \pi(\cdot |x)} \frac{r_a I (h(x)=a)}{max \lbrace \hat{\pi}(a|x), \tau \rbrace} 
$$

...(4)

S就是上面指的dataset。
 
当在更简单、更标准的setting中（使用单一固定的(fixed) stochastic policy）使用T个policies时时，该定理与我们的estimator的期望值相关。

## 3.1 Policy Estimation

在本节中，我们展示了如果$$\tau$$和$$\hat{\pi}$$的值选得合适，我们的estimator在评估new policies h时会足够精准。我们会进而使用前面章节的简化版，它展示了：**我们可以将数据看成是由一个确定的stochastic policy $$\pi$$生成，比如：对于所有t来说，$$\pi_t = \pi$$**。

对于$$\pi$$一个给定的estimate $$\hat{\pi}$$，我们将"regret"定义为一个函数：$$reg:X \rightarrow [0,1]$$：

$$
reg(x)= \underset {max}{a \in A} [(\pi(a \mid x) - \hat{\pi}(a \mid x))^2]
$$

...(2)

我们不会使用$$l_1$$和$$l_\infty$$ loss，因为它们比$$l_2$$ loss更难最小化。我们接下来的结果是：new estimator是一致的(consistent)。在接下来的理论声明中：

- $$I(\cdot)$$表示了indicator function
- $$\pi(a \mid x)$$是logging policy在输入x上选择action a的概率
- $$\hat{V}_{\hat{\pi}}^h$$是由等式(2)基于参数$$\tau$$的定义

**引理3.1**：假设$$\hat{\pi}$$是从X到在actions A分布上的任何函数。假设：$$h: X \rightarrow A$$是任意的deterministic policy。假设：$$V^h(x) = E_{r \sim D(\cdot \mid x)}[r_{h(x)}]$$表示在input x上执行policy h的期望值。我们有：

$$

E_x [ I(\pi(h(x) | x) \geq \tau) \cdot (V^h(x) - \frac{\sqrt{reg(x)}}{\tau}) ] \leq E[\hat{V}_{\hat{\pi}}^h]  \leq V^h + E_x [ I(\pi(h(x) \mid x) \geq \tau) \cdot \frac{\sqrt{reg(x)}}{\tau} ]
$$

注：在上式中，期望$$E[\hat{V}_{\hat{\pi}}^h]$$会在关于T个tuples (x,a,r)的所有序列上计算，其中$$(x,r) \sim D$$以及$$a \sim \pi(\cdot \mid x)$$

该引理限制了在我们的estimate $$V^h(x)$$上的bias。有两个bias来源——一个来自于在估计$$\pi(a \mid x)$$的$$\hat{\pi}(a \mid x)$$引入的error，另一个来自于阀值$$\tau$$。对于第一个来源，我们根据squared loss分析了结果，因为限定在squared loss估计上的regret的抽样复杂度是可行的。

。。。

## 3.2 Policy Optimization

之前章节证明，我们可以通过观察一个stochastic policy $$\pi$$来有效评估一个policy h，只要由h选中的actions在$$\pi$$下有足够的支持，特别的：对于所有inputs x来说，有$$\pi(h(x) \mid x) \geq \tau$$。然而，我们通常感兴趣的是：在观察到logged data后，从一个policies集合H中选择最好的policy h。再者，如第2节所述，logged data从T个固定的、possibly deterministic的policies $$\pi_1, \cdots, \pi_T$$来生成，而非单个stochastic policy。如第3节所述，我们通过等式3定义了stochastic policy $$\pi$$：

$$
\pi(a | x) = E_{t \sim UNIF(1,\cdots,T)}[\pi_t(a | x)]
$$

第3.1节的结果可以应用到policy optimization问题上。然而，注意，该数据被假设成：通过执行一个关于T个policies $$\pi_i, \cdots, \pi_T$$的序列上抽取得到，而非从$$\pi$$上的T个抽取。

接下来，我们展示了使用在H（在$$\pi$$下充分支持）中最好的hypothesis可能效果很好。（即使数据不是从$$\pi$$中生成的）

**定理3.2**：假设$$\hat{\pi}$$是任意从X到actions A分布的函数。假设H是任意deterministic policies的集合。定义：$$\tilde{H}=\lbrace h \in H \mid \pi(h(x) \mid x) > \tau, \forall x \in X \rbrace$$以及$$\tilde{h} = argmax_{h \in \tilde{H}} \lbrace V^h \rbrace$$。

假设$$\hat{h} = argmax_{h \in H} \lbrace \hat{V}_{\hat{\pi}}^h \rbrace$$是hypothesis：最大化在等式(2)中定义的empirical value estimator。接着，概率至少为$$1-\delta$$：

$$
V^{\hat{h}} \geq V^{\tilde{h}} - \frac{2}{\tau} (\sqrt{E_x[reg(x)]} + \sqrt{\frac{ln(2|H|/\delta}{2T}})
$$

...(6)

其中，reg(x)是定义好的，对应于$$\pi$$，在等式(5)中

定理3.2的证明依赖于我们estimator的下界属性（引理3.1的左侧不等式）。换句话说，如果H包含了一个非常好的policy，它在$$\pi$$下具有较小的支撑，我们不能通过我们的estimator来检测它。换句话说，我们的estimation是安全的，我们从不会弹性过估计在H中任意policy的value。“underestimate, but don’t overestimate"的特性，对于应用optimization技术来说很重要，这意味着我们可以使用一个不受限的学习算法来生成一个warm start policy。

# 4.评估

我们在两个来自Yahoo!的真实数据集中评估了我们的方法。第一个数据集包含了均匀随机的曝光数据，该数据集中可以获取一个关于任意policy的无偏估计。该数据集接着用于验证我们的offline evaluator(2)的accuracy。第二个数据集接着会演示如何从非随机离线数据中去做policy optimization。

## 4.1 实验I

...

## 4.2 实验II

在第二个实验中，我们会在warm-start问题上研究我们的方法。该数据集由Yahoo!提供，包含了在2008年一个月周期的数据。该数据由events日志(x,a,y)组成，其中，每个event表示了一个user访问了一个特定web页面x（它来自于一个大的web页集合X）。商业系统会从一个大的广告集合A中选择单个广告a放置到最前置、最重要的位置。它也会选择额外的广告来展示，但在我们的测试中会忽略。输出y是一个indicator，表示用户是否点击了该ad。

在该dataset中的ads数近似为880000个。训练数据集包含了3500w的events。test数据集包含了1900w的event，它们在训练集中的events之后发生。去重的web页数目为3400w。

我们基于当前页，训练一个policy h来选择一个广告，使得能最大化点击概率。学了学习，每个ad和page在内部都被表示成一个稀疏高维的feature vector。在page中或ad中出现的词(words)所对应于的features，可以通过词频进行加权。每个ad平均包含了30个ad features，每个page包含了大约50个page features。f的特征形式在它的输入(x,a)的特征上是线性的。（注：所使用的feature vector通常是page vector和ad vector的笛卡尔积）

被优化的指定policy，具有一个argmax形式：$$h(x)=argmax_{a \in C(x)} \lbrace f(x,a) \rbrace$$，在关于f(x,a)如何被训练的问题上与之前的方法不同。这里：$$f: X \times A \rightarrow [0,1]$$是一个regression函数，它被训练用来估计点击概率，并且$$C(x)=\lbrace a \in A \mid \hat{\pi}(a \mid x) > 0 \rbrace$$是一个可行ads的集合。

训练样本的形式为：(x,a,y)，其中y=1表示ad a在被展示到page x时被点击，否则为0。regressor f被选择用来逼近最小化weighted squared loss：$$\frac{(y-f(x,a))^2}{max \lbrace \hat{\pi}(a_t \mid x_t), \tau \rbrace}$$。使用SGD来最小化squared loss。

在评估期间，我们会计算在test data $$(x_t, a_t, y_t)$$上的estimator：

$$
\hat{V}_{\hat{\pi}}^h = \frac{1}{T} \sum\limits_{t=1}^T \frac{y_t I(h(x_t)=a_t)}{max \lbrace \hat{\pi}(a_t | x_t) , \tau \rbrace}
$$

...(7)

如简介所述，该estimator是有偏的，因为使用了参数$$\tau > 0$$。如第3节分析所示，该bias通常会欠估计（underestimate）policy h的true value。

我们解释了使用不同的阀值$$\tau$$以及其它参数。结果如表2所示。

Interval列的计算使用Chernoff bound相对熵的形式，$$\delta=0.05$$持有假设：变量是IID的，（即：在我们的case中，用于estimator计算的样本）。注意，该计算有些复杂，因为变量的范围是$$[0, 1/\tau]$$，而非常见的$$[0, 1]$$。这可以通过乘以$$\tau$$来进行缩放，应用到bound上，接着将结果乘以$$1/\tau$$进行缩放。

"random" policy指的是，从feasible ads集合中随机选择：$$Random(x)=a \sim UNIF(C(X))$$，其中$$UNIF(\cdot)$$表示均匀分布。

"Naive" policy对应于。
 
# 参考

- 1.[http://papers.nips.cc/paper/3977-learning-from-logged-implicit-exploration-data](http://papers.nips.cc/paper/3977-learning-from-logged-implicit-exploration-data)