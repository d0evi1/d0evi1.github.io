---
layout: post
title: SATrans介绍
description: 
modified: 2024-12-2
tags: 
---

weixin在《Scenario-Adaptive Feature Interaction for Click-Through Rate Prediction》提出了一种在序列建模中考虑场景信息的方法：SATrans。

# 一、摘要

传统的点击率（CTR）预测模型通常在**单一场景下**进行训练和部署。然而，大规模的商业平台通常包含多个推荐场景，其流量特征可能非常不同。最近的研究证明，学习一个统一的模型来服务于多个场景可以有效地提高整体性能。然而，大多数现有方法都各自存在各种限制，例如：**区分度建模不足、随着场景增加效率低下、以及缺乏可解释性**。更重要的是，据我们所知，现有的多场景建模方法在建模场景差异时没有考虑显式的特征交互（explicit feature interaction），这限制了网络的表现力，从而影响效果。在本文中，我们提出了一个名为SATrans的新型场景自适应特征交互框架（Scenario-Adaptive Feature Interaction framework），将场景差异（scenario discrepancy）建模成**特征相关性（feature correlations）模式**的差异。具体而言，SATrans建立在Transformer架构上，以学习高阶特征交互，并在**自注意力建模中涉及场景信息，以捕捉场景之间的分布变化**。我们提供了各种实现我们的框架来提高性能，并在公共和工业数据集上进行实验，结果表明SATrans:

- 1）显著优于现有的最先进方法进行预测
- 2）参数效率高，随着场景增加而空间复杂度略微增加
- 3）在实例级别和场景级别都具有良好的可解释性

我们已经将该模型部署在微信公众号平台上，在三个主要场景中平均在线CTR增加了2.84％。

# 一、介绍

近年来，多场景点击率（MS-CTR：Multi-Scenario Click-Through Rate）预测[8, 19, 20, 28, 29]已成为在线推荐领域广泛研究的热点，它主要关注于预测在多个场景中的用户-物品对的CTR。在像腾讯和阿里巴巴这样的大型商业公司中，通常存在许多业务场景（例如主页信息流、横幅信息流）[30]。此外，从服务平台收集的日志数据可以根据一些**代表性特征（例如性别、国家）**分成多个子集。这些子集具有不同的CTR分布，可以被视为场景[29]。不同的场景间可以共享**共性**（例如重叠的用户或物品、一般性偏好），可以使所有场景的预测受益。**同时，用户行为和曝光分布在不同场景下可能会有很大的差异[32]**。因此，在估计CTR时建模场景之间的共性和差异非常重要。此外，特征交叉(feature interaction)学习在CTR预测任务中起着至关重要的作用。有效地模拟特征的高阶组合可以提高网络的表达能力，从而有助于提高预测性能[4, 10, 21]。

通常有三种典型的MS-CTR预测方法：

- (1)利用传统的CTR预测模型[4, 5, 10, 13, 24, 27]和启发式训练策略，例如：为**每个场景训练单独的模型、或使用所有场景实例训练共享模型，然后进行微调**。这类方法可以自然地继承传统CTR预测模型的所有优点（例如显式特征交互），但它们在知识转移和场景建模方面的能力有限。
- (2)**基于多任务学习（MTL）构建统一框架，将每个场景视为一个任务**[2, 8, 20]。这种策略需要为每个场景建立单独的网络模块（例如门控网络、专家或输出塔），随着场景的增加，会消耗过多的参数。更糟糕的是，MTL模型通常将骨干网络或专家网络视为广义深度神经网络（DNN）[11, 17, 22]，以位逻辑和隐式方式学习高阶特征交互，受到离散特征交互的梯度不敏感问题的困扰，无法适应POLY函数[14]或简单的点积[16]。尽管可以用因子分解机（FM）[15]或DCN [24]等显式交互模型替换DNN，但特征交互和场景建模的过程是分离的，这限制了模型的可解释性，并可能导致次优的性能。
- (3)**利用辅助编码器（auxiliary encoder）使用场景相关特征作为输入，生成场景自适应单元（SAU），以影响网络[28-30]**。这些方法比MTL方法更灵活、参数更有效，可以处理大量场景和多个场景特征字段。然而，这一类现有方法并没有直接、明确地考虑场景特性对特征交互的影响，因此跨场景的特征相关性和组合的差异仍不清楚。

从特征交互的角度来看，来自不同场景的样本可能具有不同的模式。以电子商务推荐为例，性别、位置和品牌可能是三个重要的特征，它们的组合可能会显著影响CTR得分。然而，**同一特征组合的重要性在不同场景中是不同的**。考虑二阶特征组合，<品牌，位置> 可能对食品推荐场景更有意义，因为用户的食品偏好受地理因素的影响很大，而 <品牌，性别> 在服装推荐中可能更相关，因为这个场景中有特定的性别区分。据我们所知，现有的MS-CTR方法都不能明确地捕捉到这种特征交互的差异，这限制了网络的表达能力，并导致模型的可解释性不足。

为了解决这些限制，本文提出了一种名为Scenario-Adaptive Transformer（SATrans）的MS-CTR预测的显式特征交叉模型，**将场景信息纳入特征的相关建模中**，以学习每个场景的独特和自适应的高阶特征交互。具体而言，我们利用Transformer [23]作为骨干架构，对输入特征进行高阶交叉和组合建模，该方法已被AutoInt [21]和InterHAt [9]证明是有效的。Transformer中的多头自注意机制允许每个特征字段与所有其他特征交叉，并自动识别相关特征以形成有意义的高阶特征。为了将场景特性纳入特征交叉中，我们：

- 首先设计了一个场景编码器，将场景相关特征转换为固定长度的场景embedding。
- 然后利用场景自适应交叉层来测量相关性，使用特征对的embedding和场景embedding作为输入，其中注意力分数通过**一个精心设计的场景自适应相关函数**计算。

提出的场景自适应自注意机制赋予SATrans许多优点：

- （1）共性建模：每个交叉层中的共享特征转换矩阵和嵌入层自然地捕捉到共同知识；
- （2）差异建模：自适应注意力分数编码了场景之间的分布偏移；
- （3）高可扩展性：网络参数的规模几乎不依赖于场景的数量，使SATrans能够高效地处理数千甚至数百万个场景；
- （4）良好的可解释性：注意力分数可以衡量特征之间的相关性，提供实例级和场景级的可解释性。

总之，在本文中，我们做出了以下贡献：

- 我们是第一个从特征交互的角度对MS-CTR预测问题进行建模，并提出了一种新颖的SATrans，它在输入特征上明确地进行场景自适应高阶交叉
- 我们分别为SATrans设计了三种场景编码器和场景自适应交互模块的实现，相比于基本的自注意力机制，显著提高了特征交互的质量
- 我们在公共和工业数据集上进行了广泛的实验。在多场景CTR预测任务上的实验结果表明，我们提出的方法不仅在预测方面显著优于现有的最先进方法，而且具有良好的可扩展性和模型可解释性
- 考虑到MS-CTR预测中开源代码的稀缺性，我们发布了我们模型的实现以及比较基准3，以促进未来的研究

# 三、问题公式化

点击率（CTR）预测数据集可以表示为：

$$
D = \lbrace (𝑥_𝑗，𝑦_𝑗) \rbrace_{j=1}^{|D|}
$$

其中：

- $𝑥_𝑗$和$𝑦_𝑗 \in \lbrace 0,1 \rbrace$：表示第j个样本的特征集和点击label

在现实世界的推荐中，通常存在多个业务场景，这意味着数据集D可以分为多个特定场景的子集 $D^s$（例如：$D = U_s D^s$），其中场景𝑠的子集$D^s= \lbrace（𝑥_𝑖^s，𝑥_i^a，𝑦_𝑖）\rbrace_{𝑖=1}^{\mid D^s \mid}$根据$𝑥_i^s$获得。这里将整个特征集$𝑥_𝑖$分为：

- 场景相关（scenario-specific）的特征集：$𝑥_𝑖^s$
- 场景无关（scenario-agnosti）的特征集：$𝑥_𝑖^a$

$𝑥_𝑖^s$中的场景相关特征可以是：**业务ID或展示位置ID**等上下文特征，也可以扩展为**用户配置文件特征（例如，性别，年龄组）或item特征（例如，类别，品牌）**，这可能会导致不同的行为或曝光分布。将每个场景子集$D^𝑠$拆分为：训练集$D_{train}^s$和测试集 $D_{test}^s$，我们有：$D_{train} = U_s D_{train}^s$和$D_{test} = U_s D_{test}^s$。MS-CTR预测的目标是：基于$D_{train}$构建一个统一的CTR模型，可以为$D_{test}$中的所有场景子集提供准确的CTR预测。

# 4.方法

## 4.1 架构总览

为了建模多个场景下特征交互的特殊性，对于MS-CTR预测问题，我们提出了SATrans。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/9683c3c01004cfdd712cc7c536d49be38c8fb4c66ec145eb8c7c0ad265411131ca4afc7612601871a2038a09c18cadbb?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 SATrans的总体框架。左侧是场景编码器，使用场景相关特征作为输入生成固定大小的嵌入。右侧是由多个SAI层组成的骨干网络。场景编码器和SAI层的实现细节分别在第4.2节和第4.3节中详细说明。

如图1所示，SATrans将基于自注意力的交互层堆叠作为backbone，并由两个场景相关组件（ scenario-specific components）：

- (1) 场景编码器（Scenario Encoder）：将特定场景特征转换为固定长度的embedding向量
- (2) 场景自适应交互层（Scenario-Adaptive Interaction: SAI layers）：通过场景自适应自注意机制进行高阶特征交叉。

给定输入特征集${𝑥_𝑖^s，𝑥_𝑖^a}$，我们首先将其转换为稀疏特征向量：

$$
x = [x^s; x^𝑎] = [x_1^s; \cdots; x_𝑀^s; x_1^a; \cdots; x_{𝑁-𝑀}^a]
$$ 

... (1) 

其中：

- 𝑀是场景相关特征（scenario-specific features）的数量
- 𝑁是所有特征的数量

之后，我们首先将场景相关特征$x^s$输入到场景编码器（scenario encoder）中以获取场景embedding s，然后使用embedding layer将所有特征x投影到相同的低维空间，并获得dense embedding $e = [e_1; \cdots; e_𝑁]$，接着进行多个场景自适应交互层（scenario-adaptive interacting layers），其中在场景embedding的指导下，通过自注意机制将高阶特征组合在一起。通过堆叠𝑙个交叉层，可以建模多达（𝑙+1）阶的场景自适应特征交叉。最终交叉层的输出被连接，然后经过线性层和sigmoid函数来估计CTR。SATrans的关键在于如何设计有效的场景编码器和场景自适应交互模块。在接下来的部分中，我们将介绍我们提出的方法的详细信息。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/0b17d9b211999b7913a52ffa37fe930bc3c4607b43082c7b1a7ef3ca7dea7a063db2146c1b07b0b791e3202469c3e0e3?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 三种类型的Scenario Encoder

## 4.2 Scenario Feature Encoder

给定场景相关特征$x^s=[x_1^s;\cdots;x_𝑀^s]$，我们使用一个场景自适应编码器（scenarioadaptive encoder）将场景特征编码为固定长度的场景embedding $s \in R^L$，以指导在每个SAI层中的特征交互，其中维度𝐿取决于SAI层的具体实现。为了提高场景embedding的质量，我们考虑三个信息来源：

- 1）场景专有信息，区分不同的场景；
- 2）共享知识，编码场景之间的共性；
- 3）结构位置，表示场景嵌入在自注意网络中涉及的位置（position）（例如，当前层的深度，查询或键嵌入）。

我们针对不同的信息来源提出了三种实现方式。

- **独立嵌入（IE: Independent Embedding）**：该方法首先将场景特征拼接一起稀疏向量$x^𝑠$转换为一个one-hot稀疏特征$x^𝑜$，然后使用嵌入矩阵将其投影到低维向量s中。这种做法**将所有场景特征字段的每种可能组合视为一个场景**，并使用独立embedding来表示每个场景，这意味着场景之间没有共享知识。更糟糕的是，当特征组合数增加时，嵌入矩阵可能会很大，这将导致参数效率低下和不灵活。

- **编码网络（EN: Encoding Network）**：为了更灵活地编码场景特征并涉及共享知识，我们考虑利用共享编码网络来转换场景特征。对于每个场景特征字段，首先使用嵌入矩阵$E_𝑖^s$将稀疏特征向量$x_𝑖^s$投影为低维向量$e_𝑖^s$。我们将每个字段的embedding向量连接起来，得到：$e^𝑠=[e_1^s; e_2^s; \cdots; e_𝑀^s]$，然后通过非线性激活层（例如ReLU [1]）将其feed到共享编码网络$𝑓_𝑒(·)$中，以获取最终的场景embedding s。在我们的实验中，我们发现一个简单的矩阵变换已经足够，即：$s=W_𝑠 ReLU(e^𝑠)$。

- **带有结构位置ID的编码网络（ENP）**：由于场景embedding在不同的交叉层和backbone自注意力网络中的不同位置（例如，查询或键）上操作，因此生成**位置感知的场景嵌入(position-aware scenario embeddings)**以提高网络的表达能力是合理的。为此，除了场景特征外，我们还将位置ID作为额外特征馈送到网络中，以为SAI层中的每个结构位置生成唯一的场景embedding。具体而言，我们有：

$$
s_{𝑙,ℎ} = W_𝑠 ReLU(Concat(e^𝑠, p_{𝑙,ℎ}))
$$ 

...(2) 

其中：

- $p_{𝑙,ℎ}$是position embedding
- 𝑙是层深度(layer depth) ID
- $ℎ \in \lbrace 𝑄,𝐾 \rbrace$

在EN和ENP方案中，每个场景（或场景特征）的单独网络参数只是低维度（在我们的实验中为32）的embedding向量，与每个场景具有独立门控网络、特定专家网络或输出塔的MTL方法相比，这是非常参数有效的，使得SATrans可用于大量场景。我们在实验中比较参数复杂度。在接下来的部分中，我们省略下标𝑙，并使用$s_Q$和$s_K$分别表示查询和键表示的场景嵌入，以简化表示。请注意，对于IE和EN策略，$s_Q=s_K$。

## 4.3 Scenario-Adaptive Interacting Layer

一旦我们在相同的低维空间中拥有特征embedding: $e=[e1;...;e𝑁]$，和每个交互层中每个位置的场景embedding: $s_{𝑖,𝑗}$，我们就开始建模场景自适应高阶组合特征。假设第𝑖个特征的输入表示为$h_𝑖$，并且在第一个交互层中$h_𝑖=e_𝑖$。

我们首先引入多头自注意机制来确定每个特征组合的重要性。以第𝑖个特征为例，首先，在特定的注意力头ℎ下，第𝑖个特征与第𝑗个特征（$𝑖,𝑗 \in \lbrace 1,...,𝑁 \rbrace$）之间的相关性定义为：

$$
𝛼_{𝑖,𝑗}^{(ℎ)}=\frac{exp(𝜙(ℎ)(h_𝑖,h_𝑗))}{\sum_𝑘^N exp(𝜙(ℎ)(h_𝑖,h_𝑘))}
$$ 

...(3)

$$
𝜙(ℎ)(h_𝑖,h_𝑗)=⟨W_Q^{(ℎ)} h_𝑖, W_K^{(ℎ)} h_𝑗⟩
$$

...(4) 

其中：

- $𝜙(ℎ)(·,·)$：是一个attention函数，它定义了在头ℎ下第𝑖个特征和第𝑗个特征之间的未归一化相关性。它可以是一个神经网络或者简单的内积，即⟨·,·⟩。 
- $W_Q^{(h)}, W_K^{(h)} \in R^{𝑑′\times 𝑑}$：是变换矩阵，将原始的嵌入空间$R^𝑑$投影到一个新的空间$R^{𝑑′}$。其中$𝑑′=𝑑/𝐻$，𝐻是注意力头的数量。

然后，通过系数$𝛼_{𝑖,𝑗}$聚合其他特征，第𝑖个特征在子空间ℎ中的表示被更新为：

$$
\hat{h}_𝑖^{(ℎ)} = \sum\limits_l^M 𝛼_{𝑖,𝑗}^{(ℎ)} (W_V^{(ℎ)} h_𝑙)
$$
...(5)

其中：

- $W_V^{(ℎ)} \in R^{𝑑′×𝑑}$
- $\hat{h}_𝑖^{(ℎ)}$：是在头ℎ下第𝑖个特征及其相关特征的组合。

等式（4）中的相关函数将所有场景的实例视为相同，忽略了不同场景之间的分布差异。为了建模场景之间的分布转移，**我们在特征之间的相关系数计算中引入了场景embedding**。我们首先将$s_Q、s_K$的场景embedding分成𝐻个部分，即：

$$
s_Q=[s_Q^{(1)}, \cdots, s_Q^{(𝐻)}]，\\
s_K=[s_K^{(1)}, \cdots, s_K^{(𝐻)}]
$$

其中：

- $s_K^{(h)}，s_Q^{(h)} \in R^{𝐿/𝐻}$

然后在head h下改进场景自适应注意力函数，如下所示：

$$
𝜙_{𝑠𝑎}^{(ℎ)}(h_𝑖,h_𝑗,s_Q^{(ℎ)},s_K^{(ℎ)})
$$
... (6) 

现在的问题是：如何设计场景自适应注意力函数$𝜙_{𝑠𝑎}^{(ℎ)}(·,·,·,·)$，它会显著影响交互质量。基于计算复杂度从低到高的顺序，我们考虑了三种方法，如图3所示。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/93db365a438a7827a37e0ffd7c8185869222f5c2a8b0bff77dee1ddb96159921160bd12e9d38d5464204c39b477c4ae1?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3

SA-Gate（按位）：一种直接使用按位转换来引入场景嵌入的策略是门控机制。具体而言，我们基于场景嵌入生成门控模块，以过滤特征嵌入： 

$$
𝜙_{𝑠𝑎}^{(ℎ)}(h_𝑖,h_𝑗,s_Q^{(ℎ)}, s_K^{(h)})=⟨\sigma(s_Q^{(h)}) \odot (W_Q^{(h)} h_𝑖), \sigma(s_K^{(h)}) \odot (W_K^{(h)} h_𝑗)⟩
$$
...(7) 

其中:

- $\sigma(𝑥)=1/(1+𝑒^(−𝑥))$：表示Sigmoid函数
- $\odot$：表示element-wise乘积

SA-Bilinear（双线性）：这种方法对特征嵌入进行双线性变换，由场景感知矩阵S参数化。注意力分数计算为：

$$
𝜙_{𝑠𝑎}^{(ℎ)}(h_𝑖,h_𝑗,s_Q^{(ℎ)},s_K^{(ℎ)})=(W_Q^{(ℎ)} h_𝑗)^⊤ S(W_K^{(ℎ)} h_𝑖)
$$
...(8) 

其中：
$$
S=Reshape(s_Q^{(ℎ)}) \in R^{𝑑×𝑑}
$$
...(9) 

在这种策略中，每层中的$s_Q^{(ℎ)}$和$s_K^{(ℎ)}$是相同的。

SA-MetaNet（非线性）：前两种策略采用按位和双线性变换来引入场景特征，其表达能力有限，可能无法建模场景信息与交互特征之间的复杂关系。为此，我们考虑通过MetaNet机制进行非线性变换，类似于动态权重单元[30]。

以$s_Q^{(ℎ)}$为例，首先将其分成𝑃个slots：$[s_{Q,1}^{(ℎ)};\cdots;s_{Q,𝑃}^{(ℎ)}]$，

生成一个𝑃层Meta Network $𝑓_{s_Q^{(ℎ)}}^m(·)$的投影参数：$ 𝑓_{s_Q^{(ℎ)}}^m=W_1 \sigma(W_2 \sigma(\cdots \sigma(W_𝑃 x)\cdots))$

其中：

- $W_𝑝=Reshape(s_{Q,p}^{(ℎ)})，W_𝑝 \in R^{𝑑_{𝑝−1}×𝑑_𝑝}$
- $𝑑_𝑝$：是第𝑝+1层的输入维度
- $\sigma$：是非线性激活函数（例如ReLU）

我们使用相同的过程构建$𝑓_{s_K^{(ℎ)}}^m(·)$来处理场景嵌入$s_K^{(ℎ)}$。生成的MetaNet用于在pair-wise交叉前对input特征embedding进行转换。直观地说，$s_Q$和$s_K$的不同slots以及激活函数，可以被视为从低层到高层的场景感知滤波器，对特征嵌入进行处理，赋予网络捕捉场景之间隐含差异的能力。现在，场景自适应注意力得分（scenario-adaptive attention score）计算如下：

$$
𝜙_𝑠𝑎^{(ℎ)}(h_𝑖,h_𝑗,s_Q^{(ℎ)},s_K^{(ℎ)})=⟨LN_Q^{(ℎ)}(𝑓_{s_Q^{(ℎ)}}^m (W_Q^{(ℎ)} h_𝑖)), LN_K^{(ℎ)} (𝑓_{s_K^{(ℎ)}}^m (W_K^{(ℎ)} h_𝑗))⟩
$$ 
...(10) 

其中：

- $LN_Q^{(ℎ)}(·)$和$LN_K^{(ℎ)}(·)$：是层归一化层，用于归一化嵌入分布，具有独立的层参数。

我们发现归一化层是必不可少的，因为经过多层非线性变换后，embedding的方差会显著放大，这会严重影响收敛。在实践中，我们将MetaNet与LN层一起移动到多头分区之前，这允许跨不同头部进行信息交互，并在经验上实现了更好的性能。因此，注意力头ℎ下的注意力得分表示为：

$$
𝜙_{𝑠𝑎}^{(h)}(h_𝑖,h_𝑗,s_Q,s_K)=⟨[LN_Q(𝑓_{s_Q}^{m}(W_Q h_𝑖))]^h, [LN_K(𝑓_{s_K}^m (W_K h_𝑗))]^ℎ⟩
$$ 
...(11) 

其中：

- $W_Q$和$W_K \in R^{𝑑×𝑑}$：是变换矩阵
- $[\cdot]^h$：表示分区操作和选择第ℎ个子空间

根据公式5，我们会更新在attention head h下的第𝑖个特征的representation为$\hat{h}_𝑖^h$，然后将不同子空间的特征聚合如下： 

$$
\hat{h}_𝑖 = \hat{h}_i^1 \oplus \hat{h}_2^h \cdots \oplus \hat{h}_𝐻^h
$$

... (12) 

其中：

- $\oplus$是concatenation运算符。

接下来，我们使用投影矩阵$W_Agg$将学习到的特征进行转换，并添加标准的残差连接(residual connections)以保留以前学习到的组合特征(combinatorial
features)，包括原始的个体特征（即一阶特征），接着是一个层归一化层。形式上，第𝑖个特征的输出表示为： 

$$
h_𝑖^O=LN(W_A \hat{h}_𝑖 + h_𝑖)
$$
...(13) 

**通过这样一个interacting layer，每个特征表示会被更新到一个新的特征空间中，具有在场景信息的指导下来自其他字段的信息聚合**。我们可以堆叠多个这样的层来模拟任意阶的组合特征。我们将最后一层的输出embedding串联起来以获得$h^{Out}=h_1^{Out} \oplus h_2^{Out} ... \oplus h_𝑁^{Out}$，并使用带有Sigmoid函数𝜎的线性层来获得最终预测：

$$
pCTR=\sigma(W_O h^{Out} +b_O)
$$

...(14) 

其中：

- $W_O \in R^{1×𝑁_𝑑}$ 和 $b_O \in R$。

整个网络通过交叉熵损失进行优化。空间和时间复杂度的分析详见附录A。

略

[https://dl.acm.org/doi/pdf/10.1145/3580305.3599936](https://dl.acm.org/doi/pdf/10.1145/3580305.3599936)