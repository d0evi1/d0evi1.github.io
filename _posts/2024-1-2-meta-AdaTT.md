*---
layout: post
title: Meta AdaTT介绍
description: 
modified: 2024-1-2
tags: 
---

meta在《AdaTT: Adaptive Task-to-Task Fusion Network for Multitask Learning in Recommendations》提出了AdaTT的多任务建模方法。

# 摘要

多任务学习（MTL）旨在通过同时在多个任务上训练机器学习模型来提高它们的性能和效率。然而，MTL研究面临两个挑战：

- 1）有效地建模任务之间的关系以便实现知识共享，
- 2）共同学习任务特定（task-specific）知识和共享知识

本文提出了一种名为自适应任务融合网络（AdaTT：Adaptive Task-to-Task Fusion Network）的新模型，以解决这两个挑战。AdaTT是一个深度融合网络，具有多层的专有任务单元（task-specific unit）和可选共享融合单元。通过利用一个**残差机制（residual）**和一个**门控机制（gating）**来进行**任务间融合（task-to-task fusion）**，这些单元可以自适应地同时学习共享知识和专有任务知识。为了评估AdaTT的性能，我们使用各种任务组在公共基准和工业推荐数据集上进行实验。结果表明，AdaTT明显优于现有的最先进基线。此外，我们的端到端实验表明，与替代方案相比，该模型表现更好。

# 1.引言 

在线推荐系统旨在为用户生成个性化的高质量推荐。这些系统的有效性通常取决于它们准确学习用户偏好的能力，这通常需要同时优化多个目标。例如，一个短视频推荐系统应该考虑用户**观看视频（watch）**的可能性（likelihood）和他们**喜欢视频（like）**的可能性（likelihood）。多任务学习（MTL）是这些用例的典型解决方案。通过在单个框架内联合训练多个任务，MTL提供了几个好处：

- 首先，它增加了**计算效率**，这对于大规模在线推荐系统非常重要
- 此外，它通过**跨任务正则化（cross-task regularization）**和**知识共享（knowledge sharing）**，增强了模型表现

然而，MTL也面临着独特的挑战。其中一个主要挑战是**建模任务之间的关系**。由于每个任务可能与其他任务**具有不同程度的相关性**，仅仅建模所有任务的一般共性是不够的。这个问题的**复杂性随着任务数量的增加而增加**。有效的任务关系建模是实现任务自适应知识共享（task-adaptive knowledge sharing）的关键。例如，“分享视频（share）”任务共享的知识可以在类似于“喜欢视频（like）”的任务中得到很大的权重，同时也可以从具有丰富示例的其它任务中吸取不同方面的知识，例如“观看视频（watch）”。另一方面，它会最小化与高度不相关的任务的共享学习(shared learning)。

- 先前的工作[2、19]通常采用**静态共享表示(static shared representations)**。
- 其他工作，如cross-stitch network[24]（如图2（c）所示），**学习矩阵**来建模多个子网络之间的关系。然而，权重对于所有样本保持不变，子网络只是松散的特定任务。
- 最近的方法，如MMoE[22]（如图2（b）所示）和PLE[29]（如图2（e）所示），**使用专门的门控网络（gating networks）**来动态组合共享的子模块以实现灵活的共享，但是这些方法建模的**任务之间的关系是模糊和间接**的。 

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/39b41e9e3d0e4b08f800b15148d5b2753632a092f27387e345be503358892aa0d9f12ea6c0c1a544412f149b55d3b693?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 我们实验中使用的MTL模型。在多级MTL模型中，使用两个融合level来说明它们的设计。模块用不同的颜色表示：共享模块为蓝色，任务A特定模块为黄色，任务B特定模块为紫色

除了共享学习，**专有任务学习（task-specific learning）**也是多任务学习的一个重要组成部分。在两者之间取得适当的平衡对于解决任务冲突（task conflicts）和实现跨任务正则化（cross-task regularization）非常重要。

- 一方面，MTL可能会遇到，**负迁移（negative transfer）的问题：其中对一个任务的优化会对另一个任务的性能产生负面影响**，特别是当任务具有冲突的目标时。在这种情况下，MTL模型应该自适应地强调专有任务学习。
- 另一方面，专有任务学习过度和共享不足可能会导致**过拟合**，降低跨任务正则化的效益。每个任务的训练数据的数量和分布也会影响学习的重点：**具有更多数据的任务可以更多地依赖于它们的专有学习，而那些具有较少数据或高度倾斜数据的任务可以更多地集中于共享学习**。

考虑到样本之间的差异可以使两者之间的权衡更加动态。因此，自动学习平衡这两种类型的学习非常重要。许多**软参数共享模型（soft parameter sharing）**可以在不需要繁琐的手动调整[2]或学习所有样本的静态结构。然而，进一步的研究是 
需要理解：**如何建模在共享任务学习与专有任务学习间的交互**，以便提升效果。

为了共同应对这些挑战，我们提出了一种新颖的MTL模型，自适应任务到任务融合网络（AdaTT）。为了实现共享学习和可解释性，我们提出引入特定任务的专家、共享专家和门控模块，以明确地模拟任务对任务以及所有任务层面的交互。为了协同进行特定任务学习和共享学习，我们将它们区分并在**独立的融合模块**中进行建模，每个模块应用**不同的专家和融合策略**。然后**通过残差机制[12]结合融合结果**。此外，我们采用多级融合，每级针对不同的功能进行专门化处理，以提高学习性能。

为了评估AdaTT的性能，我们在一个真实世界的短视频推荐系统上进行了实验。我们调整实验组以检查其对不同任务关系的适应性。此外，我们还使用了一个公共基准测试进一步展示了其泛化能力。在所有这些实验中，AdaTT在不同数据集和任务组上始终表现优于基线模型。

为了评估AdaTT在大规模上的性能，我们对其超参数进行了研究，特别关注融合层次和专家数量。此外，我们设计了一项消融研究和可视化分析，以深入了解AdaTT的内部机制。消融研究验证了**残差设计的有效性**，通过分别建模的融合模块实现了**互补的任务特定和共享学习**。深度和浅层融合层次上专家权重的可视化提供了对不同且具有意义的在不同融合层级、任务以及任务组之间共享所习得的模式。

本文的贡献总结如下：

- 我们提出了一种新颖的MTL模型，自适应任务间融合网络（AdaTT），它同时实现了自适应任务间的知识共享和稳健的任务特定学习。
- 通过对真实世界基准数据和大规模视频推荐系统的彻底实验，我们评估了AdaTT与各种基线模型的有效性。
- 我们通过对其各个融合模块进行消融研究，并探究其融合单元在浅层和深层知识融合中的操作情况，展示了模型的可解释性。

# 2.相关工作

多任务学习在各个领域都有广泛的应用，包括计算机视觉[16, 19, 24, 34]、自然语言处理[5, 11]、语音识别[6]、机器人学[32]和推荐系统[10, 22, 29, 35]。许多研究都集中在开发创新的MTL架构上。这些模型可以分为两类：**硬参数共享**和**软参数共享**。硬参数共享涉及使用预定义的模型架构，在该架构中某些层在所有任务之间共享，而其他层则特定于单个任务。

**共享底层模型(Share Bottom Model)**[2]是硬参数方法中最广泛使用的模型之一。该模型利用共享的下层进行表示学习，并在其顶部添加特定于任务的层。**多线性关系网络**[20]通过在对特定任务层的参数施加张量正则化先验来改进这一结构。另一个例子是**UberNet**[16]，它使用图像金字塔方法联合解决不同层次的低级、中级和高级视觉任务。它使用特定于任务的层和共享层处理金字塔中的每个分辨率。硬参数共享模型通常具有紧凑的结构，但需要大量的人工努力来确定共享什么，并且缺乏适应性。此外，在不相关或冲突的任务之间过度共享可能导致负迁移，这可能对模型性能产生负面影响。

为了更好地应对这些挑战，已经提出了许多软参数共享的迁移学习（MTL）模型。

- **交叉拼接网络（Cross-stitch network）**[24]和**闸门网络（sluice network）**[26]使用可训练参数线性组合每一层的输出。然而，它们应用的线性组合是固定的，因此不能完全反映单个示例上的任务关系区分。其他工作提出使用注意力或门控模块，并根据输入动态组合或提取每个任务的知识。例如，
- **MTAN**[19]采用注意力模块产生逐元素掩码，从共享表示中提取特定于任务的知識。
- **MMoE**[22]引入了专家混合，并使用门控网络动态融合它们以适应每个任务
- **PLE**[29]被提出来进一步增强知识共享的灵活性。PLE明确引入了特定于任务的专家与共享专家相结合。此外，PLE提出使用门控模块进行渐进式分离路由，以选择性和动态地融合知识。

在这一系列中工作原理中，**PLE与我们的工作最为相关**。不同之处在于，我们的工作引入了两种**互补的融合模块**，分别用于模型特定任务学习和共享学习。此外，除了明确引入共享模块以学习所有任务间的共性外，我们还利用基于输入的直接任务对融合，以最大化知识共享的灵活性。

- **神经架构搜索（NAS）**[8, 17, 18, 25, 36]方法已被应用于多任务学习（MTL），以自动学习模型结构。
- **分支多任务网络**[30]通过基于亲和度分数对任务进行聚类，并将不相似的任务分配到不同的分支中来生成树状结构。[9]利用Gumbel-Softmax采样进行分支操作，而不是预先计算的亲和度分数，从而实现端到端的训练。
- **软层排序技术**[23]识别了传统固定顺序共享方法在MTL模型中的局限性，并提出学习特定于任务的比例参数，以实现每个任务共享层的灵活排序。
- **AdaShare** [28]学习一个特定于任务的策略，以选择执行哪些层来处理每个特定任务。
- **子网络路由（SNR）**[21]将共享层分割成子网络，并学习它们与潜在变量的连接。

NAS方法消除了大量手动工作，并提高了MTL模型中共享模式的灵活性。然而，由于对所有可能模型配置的穷举搜索在组合上是复杂的，这些方法通常依赖于简化假设，如分支[9, 30]、路由[21]、层排序[23]、层选择[28]等，以限制搜索空间。此外，生成的结构不会适应个别示例。

除了专注于多任务学习架构设计的工作外，另一条研究路线旨在改进多任务优化。基于不确定性的加权[15]根据任务的不确定性来学习每个任务的权重。GradNorm[3]控制不同任务的梯度大小以平衡它们的训练速率。GradDrop[4]概率性地选择一个符号并移除相反符号的梯度。梯度手术（PCGrad）[33]将冲突的任务梯度投影到彼此的正交平面上。RotoGrad[14]操纵任务梯度的大小和方向以缓解冲突。[27]将多任务学习视为一个多目标优化问题，目标是找到帕累托最优解。[31]引入了具有欠参数化小塔的自辅助损失，以平衡帕累托效率和跨任务泛化。虽然这些方法可以带来改进，但仅依赖它们而没有强大的模型架构可能会限制模型性能的上限。

# 3.模型结构

为了共同学习自适应共享表示并增强专有任务学习，我们提出了一个新模型：**自适应任务融合网络（AdaTT）**。AdaTT利用**门控和残差机制**来自适应地融合多个融合层中的专家（experts）。考虑一个具有两个预测任务的多任务学习场景。我们使用两个融合层在图1中说明了AdaTT的架构。AdaTT由**多层融合网络（multi-level fusion network）**和**任务塔（task towers）**组成。融合网络（fusion networks）由任务特定和可选共享融合单元构成，而任务塔建立在融合网络之上，并与最终融合层中的任务特定单元相连。我们的框架是通用的，支持灵活选择专家模块、任务塔网络、门控模块和可配置数量的专家和融合层。 在接下来的章节中，我们首先介绍AdaTT的一个特殊case：称为AdaTT-sp，它仅使用任务特定融合单元（如图1（a）所示）。然后，我们将描述通用的AdaTT设计，如图1（b）所示。 

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/c9add741332a5969c331be0c3bf8277a7b9fc686e9a87ac94f4d5038451669a07e8fbed7666bff6f2795a6e01f33921c?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 AdaTT-sp和具有2个fusion levels的通用AdaTT。任务A和B的特定和共享模块通过颜色区分：A为黄色，B为紫色，共享为蓝色。为了说明，我们为每个任务特定单元使用了2个专家。在通用AdaTT中，我们添加了一个共享融合单元，其中只有一个专家作为示例。请注意，通用AdaTT中的共享模块并不是必需的，因此使用虚线表示。当不存在共享模块时，通用AdaTT会回退到AdaTT-sp。

## 3.1 AdaTT-sp

AdaTT-sp的详细设计如下所示。给定输入𝑥用于𝑇个任务，任务𝑡（𝑡=1,2,...,𝑇）的预测被公式化为：

$$
y_t=h_t(𝑓_𝑡^L(𝑥))
$$

...(1) 
 
其中：
 
- L：是融合层数量
- $h_t$：表示任务𝑡的任务塔
- $𝑓_t^L$：表示在第𝐿个融合层产生任务𝑡的融合单元的函数
 
这里，$𝑓_𝑡^L(𝑥)$通过使用等式(2)和(3)，从底部到顶部应用融合层来计算：
 
$$
𝑓_1^0(𝑥)=𝑓_2^0(𝑥)=\cdots=𝑓_T^0(𝑥)=𝑥
$$

...(2) 
  
$$
𝑓_𝑡^l(𝑥)=𝐹𝑈_𝑡^l(𝑓_1^{(𝑙−1)}(𝑥), 𝑓_2^{𝑙−1}(𝑥), \cdots, 𝑓_𝑇^{l-1}(𝑥)), 𝑙=1 \cdots L
$$

...(3)

这里，FU表示融合单元。

### 3.1.1 融合单元(fusion unit)

下面我们详细介绍引入等式(3)中的$𝐹𝑈_𝑡^l$的构造。对于任务𝑡，在接收到前一个融合层（fusion level）的所有输出后，我们首先会使用函数$e_{𝑡,𝑖}^l$，和输入$𝑓_t^{l-1}(𝑥)$，来为该任务构造$𝑚_𝑡$个**本地专家(naive experts)**，表示为$𝐸_{𝑡,𝑖}^l$，即:

$$
𝐸_{𝑡,𝑖}^l=e_{𝑡,𝑖}^l(f_𝑡^{l-1}(𝑥))
$$

...(4)

其中：

- $i=1,2,\cdots,𝑚_t$
- $𝐸_{𝑡,𝑖}^l \in R^{1×𝑑^𝑙}$

在第𝑙层，**每个专家网络(expert network)会产生长度为$𝑑^𝑙$的向量**。为了简化表示，在第𝑙层，我们使用：

- $𝐸_𝑡^l$：表示属于任务𝑡的experts的所有垂直拼接（vertical concatenation）
- $𝐸^𝑙$：表示跨任务的所有experts的所有垂直拼接

具体而言，$𝐸_𝑡^l$ 和$𝐸^𝑙$表示为：

$$
𝐸_𝑡^l=[𝐸_{𝑡,1}^l, 𝐸_{𝑡,2}^l,\cdots,𝐸_{𝑡,𝑚_t}^l]
$$

...（5） 

$$
𝐸^𝑙=[𝐸_1^l,𝐸_2^l, \cdots, 𝐸_𝑇^l]
$$

...（6） 

其中：

- $𝐸_𝑡^l \in R^{𝑚_t \times 𝑑^𝑙}$
- $𝐸^𝑙 \in R^{(𝑚_1+𝑚_2+...+𝑚_𝑇)×𝑑^𝑙}$

在上述等式中：

- $[,]$：表示将向量或子矩阵垂直堆叠成较大矩阵的操作。 

**由于任务可能与其他任务具有不同的相关性**，$𝐹𝑈_𝑡^l$直接使用**门控模块$𝐴𝑙𝑙𝐸𝑥𝑝𝑒𝑟𝑡𝐺𝐹_𝑡^l$来结合所有任务的专家$𝐸^𝑙$来模拟任务间的知识融合**。此外，我们利用**轻量级线性组合$𝑁𝑎𝑡𝑖𝑣𝑒𝐸𝑥𝑝𝑒𝑟𝑡𝐿𝐹_𝑡^l$来融合任务𝑡的本地专家，即$𝐸_𝑡^l$**。概念上，门控模块模拟共享学习，本地专家的线性组合模拟专有任务学习。具体而言，任务𝑡在第𝑙层的特定单元的输出被公式化为： 

$$
𝑓_𝑡^l(𝑥) = AllExpertGF_𝑡^l(𝐸^𝑙, 𝐺_𝑡^l) + NativeExpertLF_t^l(𝐸_𝑡^l)
$$

...(7)

在公式7中，专家被融合如下： 

$$
𝑁𝑎𝑡𝑖𝑣𝑒𝐸𝑥𝑝𝑒𝑟𝑡𝐿𝐹_t^l(𝐸_𝑡^l)=(𝑣_𝑡^l)^T 𝐸^{𝑡^l} 
$$

...（8）

其中：

- 在𝐴𝑙𝑙𝐸𝑥𝑝𝑒𝑟𝑡𝐺𝐹中，$𝐸^𝑙$ 乘以由一个函数$𝑔_𝑡^l$生成的门控权重$𝐺_𝑡^l \in R^{(𝑚_1+𝑚_2+\cdots+𝑚_𝑇)\times 1}$
- 在𝑁𝑎𝑡𝑖𝑣𝑒𝐸𝑥𝑝𝑒𝑟𝑡𝐿𝐹中，相似的，$𝐸_𝑡^l$仅由一个可学习的向量$v_𝑡^l \in R^{𝑚_𝑡 \times 1}$组合在一起

当$𝑚_1=𝑚_2=\cdots=𝑚_𝑇=1$时，即所有融合单元仅有一个专家时，为了简化起见，$𝑁𝑎𝑡𝑖𝑣𝑒𝐸𝑥𝑝𝑒𝑟𝑡 𝐿𝐹_𝑡^l(𝐸_t^l)$回退到$𝐸_𝑡^l$，将一个单位权重分配给本地专家。有许多设计选项可用于$𝑔_𝑡^l$。常见的一种是使用由softmax激活的单层MLP：

$$
𝑔_𝑡^l(𝑓_𝑡^{𝑙−1}(𝑥))=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑊_𝑡^l 𝑓_𝑡^{𝑙−1}(𝑥)^T)
$$

...（10） 

这里：

- $𝑊_𝑡^l \in R^{(𝑚_1+𝑚_2+\cdots+𝑚_𝑇)} × 𝑑^{𝑙−1}$是一个学习的矩阵。 

### 3.1.2 简化

为了实现效率，考虑到公式8和公式9，我们实际上可以用零填充$(𝑣_𝑡^l)^T$以匹配$(𝐺_t^l)^T$的大小，加权并执行单个乘法来组合所有专家。因此，公式7可以简化为：

$$
𝑓_𝑡^l(𝑥)=(𝑝𝑎𝑑(𝑣_𝑡^{lT} )+𝐺_𝑡^{lT}) 𝐸^𝑙 
$$

... （11） 

正如我们所看到的，包含线性融合模块会导致计算量的最小增加。

## 3.2 常规版本的AdaTT

在其一般形式中，如图1(b)所示，AdaTT采用可选的**共享融合单元（shared fusion units）**。从概念上讲，专有任务模块pairs间的融合模拟了细粒度共享(fine-grained sharing)，而专有任务模块和共享模块间的融合则传递了适用于所有任务的广泛知识。这导致了高效灵活的任务间知识共享。**通用AdaTT的计算方式与AdaTT-sp类似，除了最后一个fusion level，共享融合单元不执行任何融合操作，只为专有任务融合单元产生专家输出进行处理**。 

总之，AdaTT明确地学习任务特定知识并自适应地与共享知识融合。融合是任务自适应的，因为：

- 1.门控模块学习与任务本地专家相关的残差
- 2.每个任务特定单元使用特定的门控模块融合专家，该门控模块以输入为条件（从第二个融合级别开始是唯一的）

通过允许每个任务直接而灵活地从其他任务中学习共享知识，AdaTT相比于仅依赖于共享专家作为媒介的PLE具有更大的灵活性。此外，AdaTT可以选择仅使用任务特定专家。与PLE不同，它在每个融合单元内的不同线性融合模块中单独融合本地专家，而不是在单个门控模块中处理所有选定的专家。这种设计增强了每个融合级别后任务特定学习的鲁棒性。尽管它很简单，但我们的实验表明，它胜过了PLE，后者将选择应用于不同的融合单元中的专家，并使用不同的路由路径来区分这些专家。

# 4.实验

略

[https://arxiv.org/pdf/2304.04959.pdf](https://arxiv.org/pdf/2304.04959.pdf)