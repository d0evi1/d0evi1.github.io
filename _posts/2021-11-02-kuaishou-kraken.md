---
layout: post
title: kuaishou的Kraken介绍
description: 
modified: 2021-11-02
tags: 
---

# 介绍

kuaishou在2020《Kraken: Memory-Efficient Continual Learning for
Large-Scale Real-Time Recommendations》提出了它们的实时推荐系统。

# 摘要

现代工业推荐系统常常使用深度学习（DL）模型，这些模型通过更多的数据和模型参数来实现更高的模型准确性。然而，当前的开源DL框架，如TensorFlow和PyTorch，在训练具有**数太字节参数的推荐模型**方面显示出相对较低的可扩展性。为了有效地从每天生成数百太字节（terabytes）训练数据的数据流中学习大规模推荐模型，我们引入了一个名为Kraken的持续学习系统。Kraken包含一个特殊的参数服务器实现，它能够动态适应持续变化的sparse特征集，以持续训练和服务推荐模型。Kraken提供了一个感知sparse性的训练系统，**该系统对dense和sparse参数使用不同的学习优化器**，以减少内存开销。使用真实世界数据集进行的广泛实验证实了Kraken的有效性和可扩展性。Kraken可以在相同的内存资源下提高推荐任务的准确性，或者在保持模型性能的同时将内存使用量减少到原来的三分之一。

# 一、引言

近年来，推荐系统已经成为许多流行移动应用的基石。它们为各种内容（包括新闻文章、短视频和广告）生成个性化排序，从而提高用户与这些应用的互动体验。正如流行商业分析师所报告的，推荐系统通过增加用户参与度，为许多大公司如亚马逊和Facebook带来了相当一部分收入[1]-[3]。

**时间敏感性**对于推荐系统实现合理性能至关重要。例如，用户在与移动应用互动时的兴趣通常是非常**非平稳的、季节性的，并且对趋势敏感。这被称为概念漂移[4]**。推荐系统的另一个重要问题是所谓的**冷启动问题[5]**，即在有限的时间内推断新用户的偏好或新item潜在的受众。解决这些问题的一个常见方法是：使用**实时持续学习（或在线学习）**[6]、[7]，这意味着不断用新数据训练推荐模型以保持模型的新鲜度。这种策略适用于许多经典机器学习模型，如逻辑回归[8]、[9]和矩阵分解[10]。然而，随着深度学习（DL）在推荐系统中的兴起，DL模型的在线学习在系统可扩展性和模型质量方面面临挑战。

与用于计算机视觉（CV）和自然语言处理（NLP）的经典机器学习模型或DL模型不同，推荐系统的DL模型使用大量sparse分类特征，这些特征表示为一维或多维的高维独热二进制向量。随着不同sparse特征的数量达到数百万甚至数十亿以提高准确性，模型大小变得多太字节，因此不适合单个GPU甚至单个服务器的内存。此外，与有限的内存资源相比，**每分钟都有大量新生成的内容和用户行为需要被表示到DL模型中**。庞大的模型和不断流动的数据流为训练系统创造了极高的内存压力。在通过大量数据训练巨型模型的压力下，更难以有效地服务实时更新的模型。

现有系统不足以克服这些挑战。一般的开源DL框架，如TensorFlow[11]和PyTorch[12]，对于CV和NLP领域批量训练复杂DL模型进行了高度优化。先前在生产环境中的研究[1]、[13]、[14]表明，这些**通用DL框架由于不善于处理大规模sparse特征**，因此在大型推荐模型方面扩展性不佳。此外，不足的端到端在线学习支持使它们无法在不断增长和持续更新的模型下工作。即使是一些开源框架的内部版本也被认为需要高维护成本才能启用在线学习[15]。

在本文中，我们介绍了Kraken，这是一个考虑到sparse嵌入的生产就绪系统，用于优化在线学习和服务大规模推荐模型。据我们所知，这是**第一份包含构建工业级推荐系统大规模持续学习系统在系统和算法方面的足够细节的论文**。Kraken的核心是一个感知sparse性的训练系统，它有一个特定的参数服务器实现，结合数据并行和模型并行来训练推荐模型。专门的参数服务器支持自动特征准入和过期机制，以高效管理在线学习期间sparse嵌入的生命周期，利用有限的内存资源以获得更好的模型性能。此外，Kraken的在线服务系统将sparse嵌入的存储和模型预测的计算解耦，这显著节省了网络和存储成本。我们在**TensorFlow 1.14**之上实现了Kraken的训练系统，因此Kraken与TensorFlow API兼容。我们通过使用真实世界数据集进行离线实验和在线A/B测试来检验Kraken。结果揭示了与原始TensorFlow系统相比，Kraken的有效性和可扩展性。

# 二、背景与动机

## A. 推荐系统中的深度学习

图1a展示了推荐系统的概览。对于给定的用户查询u，带有各种用户、item和上下文信息，推荐系统通常执行两步程序来从可能包含数十亿item的数据库中生成最相关items的$\lbrace x_i \rbrace$的排y序列表。

第一步称为**检索（retrieval）**，它通过使用简单的机器学习模型和轻量级人为定义的规则的组合来返回数百个item，以提高效率。

在缩小候选池(candidate pool)后，后续的**排序（ranking）**步骤根据复杂深度学习模型生成的排序分数对所有item进行排序。排序分数通常是$ P(y \mid u, x_i)$的估计，即用户u在查看item $x_i$后，后续用户行为标签y（例如，点击、点赞）的概率。

为了训练这些模型，真实的用户反馈数据以及用户和上下文信息被记录在日志中作为训练数据。为了获得排序分数的准确预测，现代推荐模型**使用大量sparse特征和复杂的DL模型**[16]-[18]。sparse类别型特征通常用于表示任何两个一般实体之间的交互。例如，用户偏好特征可以是用户曾经点击过的最后K个视频ID的列表。为了有效处理sparse特征，推荐模型经常使用一种称为sparse嵌入的技术将它们转换为低维dense表示。如图1b所示，sparse特征可以被视为sparseID的向量。每个sparse特征与一个嵌入表配对，并且在转换过程中，特征的每个sparseID用于查找嵌入表中的唯一列（称为嵌入向量）。然后，通过逐元素收集操作（称为池化操作）将所需的嵌入向量组合成一个dense向量。新形成的dense向量成为模型其余部分的输入，以进行最终预测。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/a1fe840c32f74ff9e5de14db8eb61ed97775b4de86b550ae8db9ee7e315919034d2c6929806a27bd2bc931469b2b9c5c?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 推荐系统概述。(a) 推荐系统中的两个步骤，检索和排名。(b) 推荐系统的典型深度神经网络（DNN）模型架构。sparse特征（一系列ID，例如UID（用户ID）和VID（视频ID））首先会被映射到不同嵌入表中的dense嵌入向量（模型的sparse部分）。然后，所需的嵌入向量被组合并成为模型其余部分的输入，以进行最终预测。(c) 显示了嵌入表中的哈希冲突。

这些sparse嵌入表通常被称为**推荐模型的sparse部分（sparse part）**。模型的其余部分，包括多层全连接深度神经网络（DNN），被称为**dense部分(dense part)**。在数据大小和访问模式方面，sparse和dense部分之间存在巨大差异。如表I所示，sparse部分的大小可能是dense部分的1000倍甚至更大。然而，**在每次训练或预测的小批量中，只有有限数量的嵌入向量在sparse部分被访问，而dense部分每个批次都被完全访问**。Kraken重新设计了sparse嵌入的存储，并采用了更好的哈希策略，允许它支持更大的嵌入表。

当前的开源DL框架，**如TensorFlow和PyTorch，使用dense可变矩阵（固定大小数组）来表示sparse嵌入**。对于这些框架的常见做法是，这些sparseID需要被哈希到一个预定义的、可计数的集合中，以控制每个嵌入表的大小，称为哈希技巧（见图1c）。例如，对于一个ID为j的视频，其嵌入存储在大小为M的数组的索引(hash(j) mod M)处。这种方法可能很棘手，因为一些sparseID比其他ID更频繁地被访问（例如，那些视频更受欢迎，被更多用户点击）。不幸的是，当热门ID被哈希到同一个哈希桶时，由于值重叠，会导致预测准确性降低。避免哈希冲突的一个简单方法是增加哈希表大小，这会浪费未使用哈希桶的内存。**Kraken重新设计了sparse嵌入的存储，并采用了更好的策略，允许它支持弹性扩展的嵌入表**。

## B. 推荐模型的并行范式

随着机器学习模型获得更多参数，由于其有限的计算和内存资源，单台机器不足以训练和服务大规模模型。许多先前的研究[11]、[19]-[21]提出了用于CV和NLP中使用的大规模神经网络的并行训练系统。典型的并行机制包括**数据并行和模型并行[20]**。数据并行将训练数据划分为多个数据集，每个worker一个，而模型并行将模型划分为可以并行训练的多个部分。

作为常见做法，推荐模型的并行训练结合了**模型并行和数据并行**，这是由于不同类型的参数特征（图2）。

- 对于模型的sparse部分：嵌入表是模型并行的，并通过**哈希在多个worker之间共享**，
- 而对于模型的dense部分：DNN是数据并行的，**每个worker有一个独特的副本**

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/80fc1a05e53de07370e1503d46be0e8ebe6dee0a89eabb26675543fc3bc3fa96db7fb9974eb4369f1befe4a08b91e3fc?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 推荐模型的典型并行范式结合了模型并行性和数据并行性

**sparse参数的大内存消耗基本上决定了我们至少需要多少worker来完成训练和服务**。Kraken提出了几项优化，以在最低的计算资源开销下提供内存高效的学习和服务。

## C. 大规模在线学习的必要性和挑战

先前的研究工作表明，在许多任务中增加深度学习模型的大小可以极大地提高它们的准确性和预测能力，而不会过拟合[22]、[23]。这一观察也适用于推荐系统，因为**更大的嵌入表可以捕捉更细粒度的用户行为和item属性**。图3a展示了基于DNN的推荐模型在三个工业数据集上的性能随着模型大小的增长而显著提高[13]、[24]。在大规模学习推荐模型方面，在线学习比批量训练有许多优势。

- 首先，工业推荐系统通常每天收集**高达数百太字节（terabytes）的训练数据**。在线学习通过流式实例使高效训练成为可能，每个训练实例只需要处理一次。
- 其次，**在线学习允许新模型更频繁地更新并部署**，比批量训练更重要，这对于解决第I节中提到的问题，如冷启动和概念漂移问题。

为了展示在线学习带来的好处，我们运行了一个在线A/B测试来比较Kraken的两种模式：

- 一种使用每五分钟更新一次模型的在线学习
- 另一种使用在测试期间不更新的静态模型

图3b绘制了A/B测试期间具有不同设置的两种模型的平均AUC，其中在线学习模型保持模型准确性稳定，但**静态模型的AUC在一小时后下降了4.7%**。它得出结论，在线学习在跟踪推荐系统中的用户兴趣方面是有效的。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/97d71dea2c88300988a466b15ea8c390d38aa8dafd06e871a123adf60c1134594383c72f3eb8e8ed038dc68f4f090a6d?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3 (a) 在三个工业数据集中，随着模型大小的增加，性能得到提升。(b) 线上学习模型与静态模型之间的性能比较。AUC值越高越好。

系统可扩展性受限于内存和长反馈循环（Long Feedback Loop）。采用在线训练推荐DL模型并将它们的sparse嵌入表扩展到多太字节的第一个挑战是：**在内存效率和模型准确性之间进行权衡**。如第II-A节所讨论的，sparse特征的sparseID不能在不考虑它们的重要性以及时间访问模式（例如，视频ID可能代表一个只应推广几天的趋势视频）的情况下均匀映射到哈希桶。随着在线训练的进行，sparse嵌入表动态增长，不同嵌入向量的数量增加得更快。这两种效应都导致嵌入表内哈希冲突的可能性高，模型性能下降。现有的开源训练和服务系统的另一个问题是：它们在部署大规模在线训练模型和支持实时数据反馈循环方面的效率低下[25]、[26]。在工业生产环境中，为了实现快速模型恢复和运行A/B测试以衡量它们的模型性能，**需要同一任务的多个模型版本共存**。因此，Kraken被重新设计，以超越以前的系统，支持具有超过数千亿参数的在线训练和服务推荐模型，同时保持稳定的模型准确性。

# 三、Kraken设计原则

为了构建一个内存高效的大规模推荐系统的在线学习系统，Kraken中的以下设计原则对于实现不仅系统可扩展性而且高模型质量至关重要：

- 1) 减少哈希冲突并在特征间共享内存空间。Kraken提出了一种**动态地接纳和驱逐sparse嵌入的技术**，以避免不必要的哈希冲突，并将所有sparse嵌入存储在全局共享的嵌入表中，以提高内存利用率。通过这种方式，Kraken不为每个sparse特征显式限制哈希桶的大小，而是**允许嵌入表在在线学习过程中弹性和自动地调整大小**。
- 2) 感知sparse性的训练框架。在拥有超过$10^{11}$的sparse特征的生产推荐系统中，**sparse参数占内存资源的99.99%以上**。Kraken引入了一个感知sparse性的训练框架来减少训练期间的内存使用。在这个框架下，我们还提出了一种新的优化器rAdaGrad，以进一步减少训练期间与sparse嵌入相关的内存使用。
- 3) 高效的持续部署和实时服务。在在线学习过程中，训练服务器中的模型始终从新生成的用户数据中学习。Kraken提出了一种高效的方法来部署不断更新的模型，而不会滞后。

为了最小化由哈希冲突导致的准确性损失，我们引入了无冲突但内存高效的嵌入结构——全局共享嵌入表（GSET），如图4所示。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/ffdf5ea5ac1b67ebe67888b06bad79dd8168049f0cf6e2490d2ac4d1a8e618862b64c2a70689cab40ac0cfba9db1f7d3?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=4.jpg&amp;size=750">

图4 Kraken的全局共享嵌入表（Global Shared Embedding Table，简称GSET）

通常增加嵌入表的容量用于减少哈希冲突。然而，很难预测表的大小，因此在部署前设置一个高（通常是恒定的）嵌入表大小，特别是对于在线学习系统来说，这是不高效的。相反，我们提出了GSET，以便：

- 1)通过全局映射器**将键值获取操作与特征嵌入过程解耦**，它为每个嵌入表提供了高逻辑容量，通过在它们之间共享内存来灵活地调整物理内存占用，
- 2)执行**自适应条目替换算法**，即特殊的特征接纳和驱逐策略，确保在长期执行期间内存占用低于预设阈值。

**GSET中的全局映射器：**

GSET中的全局映射器**将键值获取和嵌入解耦**。它以特征的名称和值（例如，‘UID’和‘001’作为特征用户ID）作为输入，并返回一个由预设映射方式生成的格式化键，该键被发送到后端的**内存中键值存储系统**，该系统负责嵌入管理，而不是传统结构中的普通数组。键的格式主要由两个表示特征名称和特征值的域组成，并且对于模型开发人员的特殊要求具有高度的可配置性。例如，可以分别设置特征值域的宽度，以控制逻辑容量并满足不同特征的倾斜需求。有了全局映射器，GSET允许每个sparse特征的弹性增长和收缩，共享它们之间的内存资源，而不是手工制作不同大小的嵌入表。

**特征接纳和驱逐：**

为了在长期执行期间控制内存使用，GSET在整个在线学习过程中对所有活动特征执行不同的**自适应条目替换算法**。自适应条目替换算法利用sparse特征的不同特征来决定如何接纳和驱逐sparse嵌入，包括它们的**频率、持续时间、特征重要性**等。

- 例如，许多sparseID在我们的生产数据集中只出现一次，这些不应该被添加到GSET中。
- 另一个例子是，一些与趋势视频相关的sparseID在这些视频从数据库中退役后就不再需要了。

基于这种领域知识（即特征工程），机器学习工程师为每类sparse特征定制条目替换算法，以最大化模型性能。

GSET中使用的自适应特征接纳政策是：**过滤掉频率低的sparseID**。由于跟踪永远不会有任何实际用途的稀有特征的统计数据成本很高，GSET支持如[9]中介绍的**概率基过滤器**。概率基过滤器以概率$ p $接纳不在GSET中的sparseID。sparseID需要被看到的次数遵循几何分布，期望值为$ \frac{1}{p} $。有了这些过滤器，低频ID将在训练过程之前被丢弃，这消除了多余的计算和内存使用。

GSET中使用的**传统内存缓存的条目替换算法（如LFU和LRU）**旨在最大化缓存命中率，只考虑每个缓存条目被引用的频率。相反，GSET使用在线学习过程中获得的额外信息来确定达到内存限制后条目驱逐的顺序，这称为**特征评分方法**。GSET为每个sparseID维护一个特征分数，该分数由包含sparseID的训练样本数量以及这些样本的最近程度决定。在间隔期间，GSET通过以下公式更新每个sparseID的特征分数：

$$
S^{t+1}_L = (1 - \beta)S^t_L + \beta (c^+ r + c^-)
$$

其中:

- $ \beta $：是时间衰减率，
- $ r $：是重要性权重，
- $ c_+, c_- $：分别是在此间隔中包含sparseID L的正例和负例的数量。

当$ \beta = 1 $且$ r = 1 $时，特征评分方法等同于LFU。**给正例和负例分配不同权重的原因是正例（例如，点击，点赞）相对较少，在模型预测中更有价值**。这与处理不平衡数据集时的下采样技术[9]有相似之处。

在我们的生产工作负载中，我们发现另外两种启发式方法很有用。

第一种称为**基于持续时间的政策**，它为每个已更新的sparseID设置一个过期时间戳。在使用特征评分方法驱逐sparseID之前，垃圾收集器将首先回收那些已过期的sparseID。这是因为许多sparse特征有明显的生命周期，它们在训练日志中的出现在短时间内迅速消失。机器学习工程师可以在过去的数据分析上运行离线分析，以估计它们的平均持续时间，并为每个sparse特征设置一个最优的持续时间值。例如，我们网站上的许多视频在发布两天后没有视频点击或观看。因此，与视频ID相关的一些特征可以在两天后安全地回收。

第二个优化称为**基于优先级的政策**，它为有限大小的sparse特征设置驱逐优先级类别。在我们的生产环境中，sparse特征通常被分类为两个优先级类别：高优先级和低优先级。当GSET达到内存限制时，只有低优先级的sparse特征被特征评分方法驱逐。sparse特征的优先级通常由机器学习工程师的领域知识和特征重要性算法决定。在开始在线训练之前，可以使用[27]-[29]中的特征选择现代方法在离线分析中估计特征重要性。然后，可以在它们的总大小不超过某些内存限制的约束下，将顶级特征列表分组到高优先级类别中。例如，在我们的生产工作负载中，关闭与用户相关的特征（例如，用户ID、城市级别）的驱逐有助于提高模型准确性。


## C. 高效的持续部署和实时服务

在本节中，我们主要介绍Kraken中为实时服务大规模推荐模型而构建的系统组件。

以前的服务系统设计[25]、[26]在一台预测机器内保持多个模型版本，无法支持需要跨节点共享的大规模推荐模型。

一种简单的方法是**同位部署（图5a）（Colocated Deployment）**，它直接在推理服务器中处理分片模型。具体来说，**每个推理服务器维护一个完整的dense部分和一个sparse部分分片**。在预测时，它从其他对等方获取所需的参数，并在本地进行预测。然而，这种直接的方式在面对不断更新的模型时引入了相对较高的财务成本。

- 一方面，每个推理服务器需要高容量的DRAM来存储一部分sparse参数。
- 另一方面，不断的模型更新影响推理服务器，浪费它们的计算资源和NIC带宽。

为了在提供生产环境所需的功能的同时增强可扩展性，我们重新设计了预测系统，将存储服务和推理服务分别在不同的服务器上处理，称为**非同位部署**。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/52581d35cfdb47259473ca12a78d89e48342bdc5ca274bd7079c4437b53d028965725a91ced4b63286c7cb2d57363931?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=5.jpg&amp;size=750">

图5 预测系统的两种架构。基线方案直接将所有参数分区到不同的推理服务器上，而Kraken则将sparse嵌入的存储与模型预测的计算解耦。

非同位部署（Non-Colocated Deployment）。如图5b所示，Kraken的预测系统构建为两个服务：预测参数服务器（简称PPS）和推理服务器。

- 与训练中使用的参数服务器架构类似，**预测参数服务器存储分片模型和嵌入表**。
- 为了进一步减少请求延迟并节省NIC带宽，**推理服务器缓存模型的某些部分，包括整个dense部分和频繁访问的嵌入向量**。

当接收到包含sparse特征ID列表的请求时，**推理服务器从预测参数服务器获取所需的sparse嵌入向量，然后执行模型推理**。使用非同位部署的主要好处是允许这两个服务使用不同的硬件资源分别进行扩展。预测参数服务器需要大内存和高网络带宽，而推理服务器主要受计算资源的限制。因此，**预测参数服务器可以使用高内存实例，推理服务器可以利用具有高计算能力的机器。**

通过非同位部署，我们有两个额外的机会进一步优化服务系统。一方面，为了增强局部性和负载均衡，Kraken支持按特征放置策略来根据它们的访问模式分布不同类型的参数。这个策略可以将一起访问的参数分组，并将它们放入同一个分片以获得更好的访问局部性。例如，一些用户端的二元sparse特征，如关注列表、收藏列表，通常以结合用户ID和其他项目ID的形式出现。因此，基于用户ID对这些sparse特征进行分片可以增强局部性，因为它们通常对同一用户一起访问。对于极受欢迎的参数，它们甚至可以在PPS的每个分片中复制，以减少热点并实现更好的负载均衡。另一方面，虽然我们的分布式在线训练系统实现了对机器学习模型的更频繁更新，但也希望以分钟级别的延迟部署新训练的模型进行在线服务。为了同时减少负载并实现实时模型更新，Kraken的训练子系统采用不同的更新策略执行增量模型更新。对于模型的sparse部分，不是每次都传输多太字节嵌入表的完整副本，而是每个嵌入向量的更新将触发一个包含新值的更新消息，然后该消息将被发送到所有下游预测服务器以进行更新。对于模型的dense部分，由于它们的参数比sparse参数的波动性小，整个dense参数的副本将每几秒钟批量更新一次。

## IV. 实现

Kraken使用C++11和Python实现。Kraken训练系统的初始版本实现了自己的工作引擎和参数服务器。然而，为了利用TensorFlow生态系统带来的好处，Kraken的新版本被构建为TensorFlow的插件，与TensorFlow的API完全兼容。该插件通过TensorFlow的C++底层API实现为定制操作符，这些操作符与Kraken的参数服务器交互，以执行sparse嵌入向量和dense变量的不同操作。为了预取和批量处理嵌入向量，我们还实现了嵌入缓存来存储在小批量中访问的嵌入向量。类似于Horovod[33]，**TensorFlow插件在Python API层面添加了钩子和变量代理，以调度工作器发送梯度和参数服务器发送模型参数的时间和通信模式**。

训练和服务于参数服务器的实现共享相同的代码库。**参数服务器的核心（core）是一个高性能的易读键值存储，可以执行梯度聚合算法以及自适应特征管理算法**。对于GSET，我们没有像[34]那样构建共享内存池，而是通过直接将所有参数分区到不同服务器来维护一个虚拟表，这是由于键值的简单语义。对于接纳算法，不需要维护任何状态，对于驱逐算法，所有策略都可以通过每个特征仅4字节（用于存储16位时间戳和16位特征分数）来支持。考虑到常见的128或256字节嵌入大小和由感知sparse性训练框架节省的空间，4字节的开销是微不足道的。

除了核心运行时，**推理服务器的实现针对生产环境中的推荐任务进行了高度优化，可以支持包括CPU、GPU和FPGA在内的异构计算设备**。Kraken的消息系统被构建为一个通用基础设施，不仅支持模型部署，还支持数据分发到需要可扩展解决方案的其他存储系统（例如，存储项目和用户特征的索引服务和用户档案服务）。

Kraken支持在线训练算法的异步和同步模式。在生产中，随着我们的模型扩展并需要更多的工作器，我们发现异步模式具有更高的训练速度，并且对机器故障更加健壮。因此，异步在线训练成为了训练我们的推荐模型的默认选项。

# V. 评估

我们首先评估我们提出的技术的好处，然后报告Kraken在生产中实际应用的性能。

## A. 实验设置

评估平台。我们在拥有64台服务器的集群上评估Kraken，除了从生产系统直接收集指标的生产性能评估外。集群中的所有服务器都配备了512GB DRAM和两个2.5GHz Intel(R) Xeon(R) Gold 6248 CPU，每个CPU有20个核心。服务器使用10 Gbps以太网连接。如果没有特别说明，每台服务器都配备了四个训练工作进程和一个参数服务器进程。

数据集。我们在公共数据集和生产数据集上衡量Kraken的性能，以便我们的实验可以轻松复现，同时展示实际工业场景中的性能。使用的公共数据集包括Criteo广告数据集、Avazu CTR数据集和MovieLens25M。Criteo广告数据集[35]在评估推荐模型时非常流行，并且很快将作为标准基准包含在MLPerf基准[36]中。Avazu CTR数据集[37]包含了一个领先广告平台的站点、应用和设备上的点击数据，共有11天。MovieLens-25M[38]通常作为一个稳定的基准数据集，包含2500万个电影评分，评分范围从1到5，以0.5为增量。在这里，我们将高于3的样本标记为正样本，其余为负样本，并将其训练为二元分类模型。两个生产数据集是从两个独立的现实世界推荐服务中收集的：Explore Feed和Follow Feed。这两项服务都向用户推荐与视频相关的内容。表III总结了不同数据集的特点。请注意，Explore Feed数据集需要5亿参数来构建一个合理的推荐模型，而Follow Feed数据集需要500亿参数（多100倍）。我们应用常见的指标，AUC和Group AUC[39]（GAUC），来评估模型的准确性。GAUC是所有用户AUC的加权平均值，以每个用户的样本数量为权重。因此，GAUC比AUC更能指示现实世界推荐系统中的模型性能。所有数据集都以在线学习的方式学习，即每个样本只训练一次。

实验比较了四种工业模型，DNN、Wide & Deep[16]、DeepFM[17]、DCN[40]在不同数据集上与Kraken和TensorFlow的结果。如果没有特别说明，默认的微基准模型是DeepFM。其他模型有类似的结论，因此我们省略它们以节省空间。更详细的设置，如模型的超参数，可以在我们的Artifact Description中找到，以便复现。

## B. 端到端系统性能

我们评估Kraken的好处，包括系统方面和模型方面的表现。我们分别在TensorFlow和Kraken中应用Adam和混合优化器。具体来说，在Kraken中，dense部分的优化器是Adam，而sparse部分是rAdaGrad。选择Adam的原因是它是最受欢迎的优化器，并且在许多研究论文[17]、[41]中因其出色的收敛速度和少调整而被使用。更多与优化器相关的评估，如AdaGrad或SGD，将在后面的章节中介绍。为了公平比较，TensorFlow的嵌入表大小被设置为使内存消耗接近Kraken（可以容纳60%的所有原始特征）。

模型准确性。我们比较Kraken和TensorFlow以评估模型的准确性。由于计算资源有限，我们不对Follow Feed进行评估，因为它需要64台服务器从头开始训练，并持续一个月以验证一个可能的配置，使用一个50B参数模型。对于TensorFlow，我们尝试了五种不同的嵌入表大小组合，并仅显示最佳结果。需要强调的是，在有限内存的情况下，调整TensorFlow的嵌入表大小以获得好的模型是费力且耗时的。

图6显示了Kraken与TensorFlow相比在不同模型上带来的模型准确性提升。对于评估的四个工作负载，Kraken在公共数据集上比TensorFlow高出0.46%到6.01%的AUC，在Explore Feed上比TensorFlow高出1.64%到2.01%的GAUC，这是一个巨大的提升（在生产中，超过0.5%的提升是显著的）。Kraken的提升主要来自GSET的无哈希冲突设计和感知sparse性训练框架的适应性。此外，它显著减少了时间和计算资源，允许每个嵌入表的弹性增长，消除了对嵌入表大小的广泛调整。图6显示GSET可以实现比最佳人工调整的嵌入表更好的模型性能。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/c178007e32f7eb9bb487012e1fd1a68c8f3aa190161b022a7ccbb7fd2f91aec4c8c14b06dadf2197ba23993067258022?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=6.jpg&amp;size=750">

图6

系统开销。然后我们评估GSET对底层TensorFlow施加的开销，这可能影响端到端的训练速度。图7a通过在Explore Feed上变化训练服务器的数量，比较了Kraken和原生TensorFlow的吞吐量（即每秒样本数）。如图所示，Kraken的吞吐量始终接近或优于TensorFlow，从而洞察到Kraken的非常小的额外开销。此外，随着工作器数量的增加，Kraken保持线性增长，而当工作器数量增长到大约75时，TensorFlow的增长率下降。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/5eaa671e9a985c7591e829b11412aefb989551f4c4c0903720a5459aa84208b14c10801469786dd3b0637d773fb1eb10?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=7.jpg&amp;size=750">

图7

可扩展性。图7b显示Kraken在Follow Feed数据集上线性扩展。不幸的是，TensorFlow由于内存不足，不支持如此大模型的训练，当需要将如此多的特征列适应到原生TensorFlow嵌入表中时。Kraken显示出对大规模模型的更好适应性。

## C. GSET评估

在本节中，我们评估GSET的设计。为了消除感知sparse性优化器的影响，我们在Kraken和TensorFlow中应用了相同的Adam优化器。如果没有特别说明，Kraken和TensorFlow使用相同的内存（足以存储60%的所有特征）。

GSET的内存效率。为了分析GSET在在线学习中的内存效率，我们在不同内存占用下（即，最多保存所有原始ID的几个比例）比较了使用Kraken和TensorFlow在Criteo数据集上不同模型的AUC。对于Kraken，特征接纳概率被设置为1，并启用了驱逐机制。如图所示，在不同的内存占用下，Kraken始终优于TensorFlow超过0.61%甚至高达3.72%。通常情况下，内存越少，GSET的提升越多。这是因为在原生TensorFlow中更激烈的哈希冲突使得模型更难学习代表输入特征的良好嵌入条目。GSET的设计减少了嵌入的哈希冲突，并在在线学习过程中学习了一组有效的特征。图8还说明了GSET的弹性增长设计可能是调整嵌入表大小的完美解决方案。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/ef42355eccdf9e2f991da7b0e41ca0b305e1becd9c3137c28070e973f137c5cb3aaf4724e82dd6acc15c6d4188efe58a?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=8.jpg&amp;size=750">

图8

特征接纳的效果。图9展示了在Explore Feed数据集中不同特征接纳概率P的效果。从图9a中，模型性能似乎不受接纳概率的影响。这是合理的，因为这些低频特征很少对整个模型做出贡献。Kraken简单地丢弃这些特征以避免频繁的过期。图9b显示了在最后一个训练小时中出现的不同频率级别的特征数量。有趣的是，不同接纳概率的低频特征数量几乎没有差异，这可能最初看起来违反直觉，但回想起来，尽管一些低频特征被过滤掉，一些相对高频的特征也更少地进入系统，从而成为新的低频特征。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/9372bd9dad38ac09eedae0b39f423713de5e3fc5f8f28896b9cf8fa764b4061b00d5cabc9a6eeae583c9726dbf31f197?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=9.jpg&amp;size=750">

图9

特征驱逐的效果。我们还进行了因素分析，以了解在保持内存使用的同时，每种特征驱逐策略对模型性能的贡献。在这个实验中，我们省略了Criteo Ad数据集，因为它的训练样本是特征匿名的，不包含时间戳信息，这将使特征驱逐盲目。

我们通过测量以下三种设置的模型性能差距来分解基线GSET仅具有LFU过期策略和结合所有三种驱逐策略的最佳性能：

- 特征评分策略（F）进一步考虑了正负样本的不同优先级。重要性权重r是从验证数据集中的[1, 3, 5]中选择的。特征分数每天衰减10%。
- 基于持续时间的策略（D）为每个特征类别设置不同的过期持续时间。我们预先采样10%的数据，分析具有相同ID的相邻样本之间的间隔时间分布，并取99.9百分位作为该特征类别的过期时间。
- 基于优先级的策略（P）禁止消除在Avazu和Explore Feed数据集中占用总内存不到10%的用户相关特征，基于我们的实践。然而，我们只在MovieLens数据集中禁用了占用较少内存并且访问更频繁的物品ID的驱逐。这是因为用户相关特征占据了高达50%，禁止驱逐它们将占用其他特征的内存，导致不满意的准确性。

图10表明，我们的特征评分策略在不同数据集上一致优于LFU策略，三种不同策略的累积可以结合它们的优势。可以得出结论，ML算法的领域知识和数据分布的感知可以使驱逐更智能，这就是为什么我们需要一个可配置的特征驱逐组件，以便ML工程师灵活定制。请注意，策略的超参数既与模型有关，也与数据集有关。我们在这里为了简单起见进行了经验设置，并且可以使用优化系统[42]进行更精细的调整。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/6316e7d75dd5941f6bde18d9bd2325277a7e7950bbb3f84e228f42629e76c6f1bf89b1bb67e65ecc163a761924e7c41d?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=10.jpg&amp;size=750">

图10

## D. 感知sparse性训练框架的效果

接下来，我们展示Kraken的感知sparse性训练框架能够像原始优化器一样正确地收敛模型，并且在更少的内存资源下提供更好的准确性。在这个实验中，我们只关注优化器，因此我们为GSET提供了足够的内存，并关闭了特征接纳和驱逐功能。

表IV显示了在三个公共数据集上不同原始优化器和感知sparse性优化器的模型性能和内存消耗。在相同的内存消耗下，我们提出的混合优化器总是能够取得更好的性能，除了在两个负样本上有一些相似的性能。我们提出的组合Dense(Adam)&Sparse(rAdaGrad)在Test AUC和内存消耗方面都优于其他所有基线优化器和混合优化器。

它能够在减少3倍内存使用的同时，提供与Adam（正常优化器中最好的）一样高模型性能。尽管组合Dense(Adam)&Sparse(SGD)的OSPs稍少，但由于缺乏适应性，最终模型性能更差。它未能在实时推荐场景的sparse数据中很好地学习。Kraken的rAdaGrad提供了最小的存储开销和学习的适应性。

感知sparse性训练框架在性能和内存效率上的提升与第三节B部分一致。结果表明，Kraken的算法不依赖于模型或数据集。此外，感知sparse性训练框架节省的内存资源可以用来替换更多的sparse参数，从而提升模型性能。

## E. 大规模在线预测评估

在本节中，我们构建了一个成本模型来评估Kraken预测系统的两种部署策略。我们考虑了两个集群组件的成本：预测参数服务器和推理服务器，并计算了云中每美元的预测吞吐量。基线是一个运行同位部署策略的预测系统。Kraken的非同位部署使其能够灵活地为预测参数服务器和推理服务器配置不同的服务器。这很重要，因为预测参数服务器是内存受限的，需要大内存，而推理服务器是计算受限的，不需要大内存。然而，对于基线的同位部署，所有推理服务器都需要是具有大内存的计算dense型服务器。
我们以一个16分片的Follow Feed模型为例进行进一步的成本建模。基于CPU和NIC带宽利用率的计算，一组16个预测参数服务器可以承载的推理服务器的最大数量估计为384。表V总结了使用两种不同部署策略的硬件成本。非同位部署在性价比上优于同位部署1.3倍（使用AWS价格数据[43]）或2.1倍（使用阿里云价格数据[44]）。从上述数据和分析中，我们得出结论，非同位部署在大规模推理集群中更有效，实现了更低的硬件成本。显然，通过将推理服务器从不断更新参数中解耦出来，Kraken实现了低成本和出色的推理性能。

## F. 生产评估

由于Kraken已经在生产中部署了两年，我们报告了在几个现实世界应用中使用Kraken的性能指标，以展示其成功。

1) 在线A/B测试结果：我们选择了三个由Kraken支持的代表性应用：视频分享、社交网络和游戏平台。表VI显示了在使用Kraken后它们的关键业务指标的增长，如下所述：

- 视频分享是一个相关视频推荐应用，在使用户观看共享视频后提供更多视频建议。其关键业务指标是每个视频的平均播放次数（平均视频播放次数）。Kraken实现了视频播放次数51%的增长，并显著提高了用户参与度。
- 社交网络是在我们的平台上向用户推荐潜在社交联系的服务。新社交联系的平均数量是评估此服务的关键指标。Kraken将核心指标提高了1.35%，使更多用户连接到其他用户。（1.35%是显著的，与旧系统中通常的0.1%改进相比。）
- 游戏平台是一个托管不同数字游戏的在线平台，Kraken用于在其Feed中生成个性化的游戏视频推荐。其关键指标是用户在阅读Feed上花费的总时间（Feed上总时间花费）。Kraken在关键指标上提高了24.41%，显示出提高用户粘性的有效性。

2) 日常监控结果：我们还通过监控Kraken服务的推荐模型的准确性以及其在整个一天的服务吞吐量和延迟来报告生产中的Follow Feed应用的性能。（Follow Feed应用与第V-B节中的离线评估中使用的应用相同。）

- 模型准确性：图11a显示了Kraken生成的平均预测点击率（CTR-P）和Follow Feed中项目的平均点击率真实值（CTR-GT）。高点击率通常意味着高用户参与度，更准确的CTR预测有助于项目推荐。如图所示，CTR-P曲线与CTR-GT曲线非常接近，表明Kraken的模型预测准确性高。
- 系统性能：图11b展示了Kraken的系统吞吐量（即推理请求的数量）以及平均延迟和尾部延迟（P99）随时间的变化。在这一天中有两个明显的高峰，12:00至14:00和20:00至23:00。在后一个时期（图11b中的阴影区域），称为高峰时段，吞吐量高达40k QPS（每秒查询次数），是平均吞吐量的两倍。与此同时，尽管吞吐量急剧上升，Kraken仍然很好地控制了平均延迟和尾部延迟（P99）。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/9e12f3caca40449e827a38073bc33e7b0471e8e0a69cebac0587a867dda86a78bde16b191c1bdcd012043660e25aa091?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=11.jpg&amp;size=750">

图11

# VI. 相关工作

尽管系统和架构社区已经投入了大量精力对用于计算机视觉或自然语言处理的深度学习进行性能分析和优化，但相对较少的关注集中在在线学习和实时推荐系统中大规模深度学习模型的实时服务上。最相关的工作是Facebook的DLRM[1]、[13]，它指出了在现代生产规模下训练推荐系统的DNN所面临的独特挑战。它提供了详细的性能分析，表明推荐DL模型需要更大的存储容量并产生不规则的内存访问。他们使用蝴蝶洗牌操作符在嵌入表上实现模型并行以缓解内存限制。XDL[14]和Parallax[45]明确区分DL模型中的dense部分和sparse部分，并尝试通过许多sparse变量改进训练模型。Parallax通过使用AllReduce架构处理dense变量和PS架构处理sparse变量，为NLP的同步训练采用混合架构。然而，上述系统大多关注批量训练模型，其嵌入表大小是固定的，没有动态增长和全局空间共享。这些系统错过了实时训练和成本有效地服务10-TB DL模型的机会。传统的深度学习框架如[11]、[12]、[46]、[47]被全球的机器学习科学家和工程师使用。然而，它们在面对推荐系统中大规模实时推荐挑战时都无法扩展。HugeCTR[48]是NVIDIA为推荐系统设计的高效率GPU框架。HugeCTR将整个嵌入表分布到多个GPU的高带宽内存（HBM）中，以加速CTR模型的训练。然而，HugeCTR受到HBM大小的限制，因为所有sparse参数都必须在HBM内维护。在这种情况下，Kraken提供的内存高效学习可以被完美应用，并在节省计算资源开销方面发挥关键作用。

参数服务器（PS）[21]作为数据并行分布式训练领域中的代表性架构之一，在Kraken中作为底层通信模型使用。多年来，有许多先前的工作在许多方面优化了并行训练，包括利用集群中的GPU[24]、[49]、网络[50]和调度[51]–[53]。这些工作提出的技术与我们的工作正交，可以用来进一步改进Kraken中的训练。随着持久内存（PM）如英特尔Optane DC持久内存的出现，将传统的基于PM的键值存储[54]适应到Kraken并将是一件有趣的事情，并在新硬件带来的高能力的基础上缓解内存短缺。

持续学习已被证明是跟上推荐系统中用户兴趣快速变化的有效解决方案[6]、[7]、[9]。Continuum[55]是一个通用的持续学习系统，通过封装不同的学习框架，使用批量模式在新数据集上重新训练模型。与Kraken相比，Continuum更新模型的频率较低，并且不适用于大规模推荐模型。

# VII. 结论

通过利用系统和训练算法的共同设计，Kraken提供了一个端到端的解决方案，用于实时训练和大规模推荐模型的服务。Kraken的训练系统实现了一个参数服务器，允许sparse嵌入表动态增长，并运行自动特征选择，在持续训练期间保持合理的内存大小。我们还提出了一个感知sparse性的框架，利用推荐模型的属性，在训练期间减少数据传输和内存占用。此外，Kraken的预测系统支持及时有效地部署大规模模型。Kraken已成功部署在生产中，用于广泛的推荐任务，并被证明在这些任务中迭代和服务大规模DL模型方面非常有效。



















# 参考

- 1.[https://storage.cs.tsinghua.edu.cn/papers/sc20-kraken.pdf/](https://storage.cs.tsinghua.edu.cn/papers/sc20-kraken.pdf/)