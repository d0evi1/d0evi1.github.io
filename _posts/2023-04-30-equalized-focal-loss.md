---
layout: post
title: Equalized Focal Loss(EFL)介绍
descrip_tion: 
modified: 2023-04-30
tags: 
---

Bo Li等在《Equalized Focal Loss for Dense Long-Tailed Object Detection》提出了EFL。


# 3. 方法论

## 3.1 重新审视Focal Loss

在单阶段检测器中，Focal Loss[28]是广泛使用的解决前景-背景不平衡问题的解决方案。它重新分配了easy样本和hard样本的损失贡献，极大地削弱了大多数背景样本的影响。二元分类的Focal Loss公式为：

$$
FL(p_t) = -\alpha_t (1 - p_t)^{\gamma} \log(p_t) \quad (1) 
$$

如[28]中所述，

- $ p_t \in [0, 1]$ ：表示对象候选的预测置信度分数，
- $ \alpha_t$： 是平衡正样本和负样本重要性的参数。
- 调节因子$ (1 - p_t)^{\gamma}$： 是Focal Loss的关键组成部分。

它通过预测分数$ p_t$ 和聚焦参数$ \gamma$ 减轻easy样本的损失，专注于hard样本的学习。如[23]中提到的，大量负样本容易被分类，而正样本通常较困难。因此，**正样本和负样本之间的不平衡可以大致看作是easy样本和hard样本之间的不平衡**。聚焦参数$ \gamma$ 决定了Focal Loss的效果。可以从公式(1)得出，**较大的$ \gamma$ 将大大减少大多数负样本的损失贡献，从而提高正样本的影响**。这一结论表明，正样本和负样本之间的不平衡程度越高，预期的$ \gamma$ 值就越大。

当涉及到多类别情况时，Focal Loss应用于由sigmoid函数转换的输出logits上的C个分类器。C是类别的数量，这意味着一个分类器负责一个特定类别，即一个二元分类任务。由于Focal Loss用相同的调节因子平等对待所有类别的学习，它未能处理长尾不平衡问题（见表2）。

## 3.2 均衡Focal Loss公式

在长尾数据集（例如LVIS）中，除了前景-背景不平衡问题外，单阶段检测器的分类器还受到前景类别之间的不平衡问题的影响。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/ea1c1d2a60bca9f61e5e82da45261882f18202306793ecbe1c9281c98d6a43b6eedc0dd46d6bff5b0d479318e613f936?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图2 在LVIS v1 [12] 训练集分割和COCO [29] trainval35k分割中，正样本与负样本数量的比率。为了更好地可视化，我们展示了比率的对数值，并将COCO的80个类别与LVIS的1203个类别对齐。我们采用ATSS [47]作为样本选择策略，以区分前景样本和背景样本。

如图2所示，如果我们从y轴观察，正样本与负样本比例的值远小于零，这主要揭示了前景和背景样本之间的不平衡。这里我们将比例的值称为**正负不平衡度**。从x轴观察，我们可以看到不同类别之间的不平衡度存在很大差异，这表明了前景类别之间的不平衡。显然，在平衡的数据分布（例如COCO）中，所有类别的不平衡度都是相似的。因此，使用相同的调节因子对所有类别在Focal Loss中是足够的。相比之下，在长尾数据情况下，这些不平衡度是不同的。**稀有类别比频繁类别遭受更严重的正负不平衡问题**。如表1所示，大多数单阶段检测器在稀有类别上的表现比在频繁类别上差。这表明，相同的调节因子不适合所有具有不同程度正负不平衡问题的情况。

聚焦因子。基于上述分析，我们提出了均衡Focal Loss（EFL），它采用与类别相关的聚焦因子来分别解决不同类别的正负不平衡问题。我们将第j个类别的损失公式化为：

$$
EFL(p_t) = -\alpha_t (1 - p_t)^{\gamma_j} \log(p_t) \quad (2)
$$

其中：

- $ \alpha_t $ 和 $ p_t $ 与Focal Loss中的相同。
- 参数 $ \gamma_j $ 是第j个类别的聚焦因子，它在Focal Loss中扮演与 $ \gamma $ 相似的角色。

如3.1节所述，不同的 $ \gamma $ 值对应不同程度的正负不平衡问题。

我们采用较大的 $ \gamma_j $ 来减轻稀有类别中的严重正负不平衡问题。对于轻微不平衡的频繁类别，较小的 $ \gamma_j $ 是合适的。聚焦因子 $ \gamma_j $ 被分解为两个部分，具体是一个类别不可知参数 $ \gamma_b $ 和一个类别特定参数 $ \gamma_{jv} $：

$$
\gamma_j = \gamma_b + \gamma_{jv} = \gamma_b + s \frac{1 - g_j}{1 + g_j} \quad (3)
$$

其中：
- $ \gamma_b $ 表示在平衡数据场景中的聚焦因子，它控制分类器的基本行为。
- 参数 $ \gamma_{jv} \geq 0 $ 是与第j个类别的不平衡度相关的可变参数。它决定了学习集中于正负不平衡问题的程度。

受EQLv2 [38]的启发，我们采用梯度引导机制来选择 $ \gamma_{jv} $。参数 $ g_j $ 表示第j个类别正样本相对于负样本的累积梯度比率。如[38]中提到的，较大的 $ g_j $ 值表示第j个类别（例如频繁）训练平衡，而较小的值表示该类别（例如稀有）训练不平衡。为了满足我们对 $ \gamma_j $ 的要求，我们将 $ g_j $ 的值限制在[0, 1]范围内，并采用 $ 1 - g_j $ 来反转其分布。超参数 $ s $ 是一个缩放因子，它决定了EFL中 $ \gamma_j $ 的上限。与Focal Loss相比，EFL独立处理每个类别的正负不平衡问题，从而提高了性能（见表3）。

加权因子。即使有了聚焦因子 $ \gamma_j $，仍然有两个障碍损害性能：（1）对于二元分类任务，较大的 $ \gamma $ 适用于更严重的正负不平衡问题。而在多类别情况下，如图3a所示，对于相同的 $ x_t^* $，$ \gamma $ 的值越大，损失越小。这导致我们想要增加对严重正负不平衡类别的学习集中度时，我们必须牺牲其在整体训练过程中的部分损失贡献。这种困境阻止了稀有类别取得优异的性能。（2）当 $ x_t $ 很小时，不同类别的样本的损失，具有不同的聚焦因子，将收敛到相似的值。实际上，我们希望稀有hard样本比频繁hard样本做出更多的损失贡献，因为它们稀缺，不能主导训练过程。
我们提出加权因子来缓解上述问题，通过重新平衡不同类别的损失贡献。与聚焦因子类似，我们为稀有类别分配较大的加权因子值以提高它们的损失贡献，同时保持频繁类别的加权因子接近1。具体来说，我们将第j个类别的加权因子设置为 $ \gamma_b + \gamma_{jv} / \gamma_b $ 以与聚焦因子保持一致。EFL的最终公式为：

$$
EFL(p_t) = -\sum_{j=1}^{C} \alpha_t \left( \frac{\gamma_b + \gamma_{jv}}{\gamma_b} \right) (1 - p_t)^{\gamma_b + \gamma_{jv}} \log(p_t) \quad (4) 
$$

如图3b所示，有了加权因子，EFL显著增加了稀有类别的损失贡献。

同时，它更多地关注稀有hard样本的学习，而不是频繁hard样本。

聚焦因子和加权因子构成了EFL中的类别相关调节因子。它使分类器能够根据其训练状态 $ p_t $ 和相应的类别状态 $ \gamma_j $ 动态调整样本的损失贡献。如4.3节所述，聚焦因子和加权因子在EFL中都扮演着重要角色。同时，在平衡的数据分布中，所有 $ \gamma_{jv} = 0 $ 的EFL等同于Focal Loss。这种吸引人的特性使得EFL能够很好地应用于不同的数据分布和数据采样器。

# 

- 1.[https://arxiv.org/pdf/2201.02593](https://arxiv.org/pdf/2201.02593)