---
layout: post
title: MV-DNN介绍
description: 
modified: 2017-04-05
tags: 
---

microsoft在2016年提出了MV-DNN结构：《A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems》。我们来看下该paper。

# 介绍

为了解决CF和CB的诸多限制，我们提出了利用user features和item features的推荐方法。为了构建user features，不同于许多user profile-based的方法，我们提出了从用记有浏览、搜索历史中抽取丰富特征来建模用户的兴趣。做出以下假设，用户的历史在线动作会影响用户的背景（background）和偏好（preference），因此提出了一种关于用户会对哪些items和topics感兴趣的更精确的看法。例如，一个用户做出的许多查询和网页访问都与婴儿有关，那么她很可能是一个新生儿的母亲。有了这些丰富的在线行为，推荐相关items可以更有效率和有效果。

在我们的工作中，我们提出了一种新的deep learning方法，它扩展了DSSM（Deep Structured Semantic Models），它将users和items映射到一个共享语义空间中，并推荐那些在映射空间中与该用户具有最大相似度的items。为了达到该目的，我们的模型会对users和items进行投影，每一者都通过一个丰富的特征集表示，通过非线性转换层映射到一个完全共享的隐语义空间中，其中user的mapping以及该用户喜欢的items的mappings间的相似度会最大化。这允放该模型学习感兴趣的mappings：比如，访问了fifa.com的用户可能会读取关于世界杯相关的新闻文章、或喜欢关于PC or XBox足球游戏。用户侧的丰富特征可以允许建模用户的行为，从而克服在content-based推荐中的限制。它也可以有效解决用户的冷启动问题，因为模型允许我们从queries和相关推荐items（比如音乐）上捕获用户兴趣，即使它们没有使用音乐服务的历史记录。我们的deep learning模型具有一个ranking-based objective函数，它会将正样本（用户喜欢的items）的排序比负样本的更高。该ranking-based objective对于推荐系统更好。

另外，我们扩展了原始的DSSM模型，它被称为是single-view DNN，因为它从来自单个域的user-features 和items学习模型。我们将新模型命名为“Multi-View DNN”。在文献中，multi-view learning是一个比较成熟的领域，它可以从相互不共享的公共特征空间的数据中进行学习。我们将MV-DNN看成了在multi-view learning setup中一种通用的Deep larning方法。特别的，在我们的数据集中有News、Apps和Movie/TV logs，我们不需要为这种不同域(domain)构建独立模型，它们可以将user features映射到相同域(domain)的item features上，我们会构建一个新的multi-view模型，它会为在隐空间中的user features发现一个单一映射，可以从所有域中使用items的特征进行联合优化。MV-DNN会利用多个跨域数据来学习一个更好的用户表示（user representation），也会利用全域的用户偏好数据使用以一种规则的方式解决数据稀疏性问题。

# 3.数据集描述

这部分会介绍数据集。我们描述了数据收集过程，以及每个数据集的特征表示，以及一些基础的数据统计。

在该研究中会使用4个数据集，它们从microsoft的产品中收集，包含：

- (1) Bing Web vertical搜索引擎日志
- (2) Bing News vertical的新闻文章浏览历史
- (3) Windows AppStore的App下载日志
- (4) XBox的Movie/TV观看日志 

所有日志的收集从2013-12 〜 2014 6月，包含了英语系国家：美国、加拿大、英国。

(用户特征: user features)：我们从Bings收集了用户的搜索queries和它们的点击urls来形成用户特征。queries首先会被归一化、获取词干、接着将它们split成unigram特征，urls会被简写成domain-level(例如：www.linkedin.com)来减小特征空间。我们接着使用TF-IDF得分来保持最流行和重要（no-trivial）特征。整体上，**我们选择了300w的unigram特征和500k的domain特征，来产生一个总长度为3500w的用户特征向量**。

(新闻特征：News Features)：我们收集了从Bing News vertical的新闻文章点击。每个News Item通过三部分特征表示。第一部分是，使用字符tri-gram表示编码的标题特征。第二部分是，使用二值特征编码的News的top-level类目（比如：娱乐）特征。第三部分是，每篇文章的命名实体，使用一个NLP parser抽取得到，同样使用tri-gram进行编码。**这会产生一个包含10w维的特征向量**。

(APP特征)：用户APP的下载历史，来自windows AppStore日志。每个App的标题使用字符tri-gram表示，会以二值形式组合上类目特征（比如：游戏）。对于APP描述常变化的特性，我们决定不包含这些特征。**这一部分会产生一个5w维的特征向量**。

（Movie/TV Features）：对于Xbox日志，我们为每个XBox用户收集了Movie/TV观看历史。每个item的标题和描述，会组合成文本特征，接着使用字符型tri-gram编码。该genre也使用二值特征编码。**这一部分会产生5w维特征向量**。

在我们的神经网络框架中，user features被映射到user view上，其余被映射到不同的item views上。出于训练目的，每个user view会被匹配到一个item view上，它包含了用户的完整集合。为了这样做，我们对登陆用户的每个user-item view pair, 以及基于id间的交叉做了抽样。这会为每个user-item view pair产生不同数目的用户。表1描述了在该paper中的一些统计。

# 4. 用于建模用户的DSSM

DSSM的引入是为了增强在搜索上下文中的query document matching。

DSSM的典型结构如图1所示。到DNN的输入(原始文本特征)是一个高维向量，例如，在一个query或一个文档中的terms原始数目（没有归一化）。接着DSSM会将该输入传给两个神经网络，两者都各自不同的输入，会将它们映射到在一个共享语义空间中的语义向量中。对于网页文档排序，DSSM会计算一个query和一个document间的相关得分（对应两个语义向量间的cosine相似度），并通过相似得分进行文档排序。

更正式的，如果x表示输入的term向量，y表示输出向量，$$l_i, i=1,...,N-1$$表示内部的hidden layers，$$W_i$$表示第i个权重矩阵，$$b_i$$表示第i个bias term，我们有：

$$
l_1 = W_1 x \\
l_i = f(W_i l_{i-1} + b_i), i=2,...,N-1 \\
y = f(W_N l_{N-1} + b_N)
$$

...(1)

其中，我们使用tanh函数作为在output layer和hidden layers $$l_i$$上的activation函数：

$$
f(x)= \frac{1  - e^{-2x}} {1+ e^{-2x}}
$$

...(2)

在一个query Q和一个document D间的语义相关得分，接着通过R进行measure：

$$
R(Q, D) = cosine(y_Q, y_D) = \frac{y_Q^T y_D} {||y_Q|| \cdot ||y_D||}
$$

...(3)

其中，$$y_Q$$和$$y_D$$各自是query和document的语义向量。在Web搜索中，给定query，document会通过它们的语义相关得分进行排序。

惯例上，每个word w通过一个one-hot词向量表示，其它维度对应于词汇表size。然而，词汇表size经常在现实中非常大，one-hot向量的词表示会让模型学习开销很大。因此，DSSM使用word hashing layer通过一个letter-trigram向量来表示一个词。例如：给定一个词（web），在添加了词边界符号后（#web#），该词被分割成ngram序列（#we, web, eb#）。接着，该词被表示成一个关于leter-tri-grams的count vector。例如，web的letter-trigram表示为：

在图1中，第一层 matrix $$W_1$$表示letter-trigram矩阵，它从一个term-vector转移到它的letter-trigram count vector上，无需学习。尽管英文words的总数目会增长得相当大，去重后的英文letter-trigrams的总数目通常是有限的。因此，它可以泛化到在训练数据中那些未见过的新词上。

在训练中，假设一个query与该query下被点击的documents相关，DSSM的参数（例如：权重矩阵$$W_i$$），可以使用该信号被训练。例如，给定一个query后一个document的后验概率，可以从两者间的语义相关分数中会通过一个softmax函数进行估计：

$$
P(D|Q) = \frac{exp(\gamma R(Q,D))}{\sum\limits_{D' \in D} exp(\gamma R(Q,D'))}
$$

...(4)

其中$$\gamma$$是softmax的一个平滑因子，它通常会会在一个held-out数据集上进行经验型设置。D表示要排序的候选文档集合。理想的，D应包含所有可能的文档。实际上，对于(query, clicked-document) pair，可以使用$$(Q, D^+)$$表示，其中：Q表示一个query，$$D^{+}$$表示点击的文档，我们通过包含$$D^{+}$$和N个随机选中的未点击文档来近似D，通过$$\lbrace D_j^-; j=1, ..., N\rbrace$$来表示。

在训练中，模型参数被估计成：给定queries，被点击文档的极大似然：

$$
L(\Lambda) = \log \prod\limits_{(Q,D^+)} P(D^+ | Q)
$$

...(5)

其中，$$\Lambda$$表示神经网络的参数集。

# 5.MV-DNN

DSSM可以看成是一个multi-learning框架，其中它会将两种不同的数据视图映射到一个共享视图（shared view）中。在某种程度上，它可以被看成是，在一个更通用的setting中来学习两个不同views间的一个共享mapping。

图2 MV-DNN使用一个DNN来将高维稀疏特征（比如：users、News、App的原始特征）映射到在一个联合语义空间中的低维dense特征中。第一个hidden layer，带有50k units，会完成word hashing。word-hashed特征接着通过多层进行非线性投影。在DNN中的最后一层的neural activities会形成在语义空间中的特征。

在该工作中，我们提出了一个DSSM的扩展，它具有对数据的多个views，我们称之为Multi-view DNN。在该setting中，我们具有v+1个views，一个主视图(pivot view)称为$$X_u$$，其它v个辅助视图(auxiliary views)为：$$X_1, ..., X_v$$。每个$$X_i$$具有它自己的输入域$$X_i \in R^{d_i}$$。每个view也具有它自己的非线性映射层 $$f_i(X_i, W_i)$$，它会将$$X_i$$转换到共享语义空间$$Y_i$$上。训练数据包含了一个样本集合。第j个样本具有主view $$X_{u,j}$$的一个实例，以及一个活动辅助视图$$X_{a,j}$$，其中a是在sample j中active view的索引。所有其它视图输入$$X_{i:i \neq a}$$被设置为0 vectors。该objects是为每个view（比如：相似的求和）发现一个线性映射，在语义空间。





# 实验

详见paper。

# 参考

- 1.[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/frp1159-songA.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/frp1159-songA.pdf)
