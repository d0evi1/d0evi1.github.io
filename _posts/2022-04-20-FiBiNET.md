---
layout: post
title: FiBiNET框架
description: 
modified: 2022-04-20
tags: 
---


weibo在《FiBiNET: Combining Feature Importance and Bilinear feature
Interaction for Click-Through Rate Prediction》提出了FiBiNET。

# 摘要

广告和推荐排名对许多互联网公司如Facebook和新浪微博至关重要。在众多现实世界的广告和推荐排名系统中，点击率（CTR）预测扮演着核心角色。该领域提出了许多模型，如逻辑回归、基于树的模型、基于分解机的模型以及基于深度学习的CTR模型。然而，许多当前的工作以简单的方式计算特征交互，例如**Hadamard积和内积**，并且不太关注特征的重要性。在本文中，提出了一种名为FiBiNET的新模型，它是**Feature Importance（特征重要性）**和**Bilinear feature Interaction（双线性特征交互）**网络的缩写，用于动态学习特征重要性和细粒度特征交互。

- 一方面，FiBiNET可以通过**Squeeze-Excitation网络（SENET）**机制动态学习特征的重要性；
- 另一方面，它能够通过**双线性函数**有效地学习特征交互。

我们在两个真实世界数据集上进行了广泛的实验，并显示我们的浅层模型优于其他浅层模型，如分解机（FM）和领域感知分解机（FFM）。为了进一步提高性能，我们将经典深度神经网络（DNN）组件与浅层模型结合，形成一个深层模型。深度FiBiNET系统性地优于其他最先进的深度模型，如DeepFM和极端深度分解机（XdeepFM）。

# 1.引言

广告和推荐排序对许多互联网公司如Facebook和新浪微博至关重要。这些任务背后的主要技术是点击率预测，也称为CTR。在这个领域提出了许多模型，例如逻辑回归（LR）[17]、多项式2（Poly2）[10]、基于树的模型[7]、基于张量的模型[13]、贝叶斯模型[3]以及基于分解机的模型[9, 10, 19, 20]。

随着深度学习在计算机视觉[5, 14]和自然语言处理[2, 18]等许多研究领域的巨大成功，近年来基于深度学习的CTR模型也被提出[1, 4, 6, 15, 22, 23, 25, 26]。因此，深度学习在CTR预测中的应用也成为该领域的研究趋势。一些基于神经网络的模型已经被提出并取得成果，诸如Factorization-Machine Supported Neural Networks（FNN）[25]、Wide&Deep模型（WDL）[1]、Attentional Factorization Machines（AFM）[23]、DeepFM[4]、XDeepFM[15]等成功案例。

在本文中，我们提出了一种名为FiBiNET的新模型，它是Feature Importance（特征重要性）和Bilinear feature Interaction（双线性特征交互网络）的缩写。该模型旨在动态学习特征重要性和细粒度特征交互。据我们所知，不同特征对目标任务具有不同的重要性。例如，在预测一个人的收入时，特征职业比特征爱好更重要。考虑到这一点，我们引入了Squeeze-and-Excitation网络（SENET）[8]来动态学习特征的权重。此外，特征交互是CTR预测领域的一个关键挑战，许多相关工作采用简单的计算方法如Hadamard积和内积来计算特征交互。我们在本文中提出了一种新的细粒度方法，使用双线性函数来计算特征交互。我们的主要贡献如下：

- 受SENET在计算机视觉领域成功的启发，我们使用SENET机制动态学习特征权重。
- 我们引入了三种类型的双线性交互层，以细粒度方式学习特征交互。这与之前的工作[6, 9, 10, 19, 20, 23]形成对比，后者使用Hadamard积或内积计算特征交互。
- 将SENET机制与双线性特征交互相结合，我们的浅层模型在Criteo和Avazu数据集上的表现超过了FFM等浅层模型的最新水平。为了进一步提高性能，我们将传统的深度神经网络（DNN）组件与浅层模型结合，形成一个深度模型。在Criteo和Avazu数据集上，深度FiBiNET始终优于其他最先进的深度模型。

本文的其余部分组织如下。第2节回顾了与我们所提出的模型相关的工作，接着在第3节介绍我们提出的模型。第4节我们将展示在Criteo和Avazu数据集上的实验探索。最后，在第5节我们讨论实证结果并总结本工作。

# 2.相关工作

## 2.1 因子分解机及其相关变体

分解机器（FM）[19, 20]和场感知分解机器（FFM）[9, 10]是两种最成功的CTR模型。FM模型都使用分解参数来表示变量之间的交互作用。它具有低的时间复杂度和内存消耗。存储方面表现良好，适用于大型稀疏数据。FFM引入了字段感知潜在向量，并在Criteo和Avazu主办的两次竞赛中获胜[9]。然而，FFM受到内存需求的限制，在互联网公司中难以轻松使用。

## 2.2 基于深度学习的CTR模型

深度学习在计算机视觉[5, 14]和自然语言处理[2, 18]等许多研究领域取得了巨大成功。因此，近年来也提出了许多基于深度学习的CTR模型[1, 4, 6, 15, 22, 23, 25, 26]。如何有效地建模特征交互是这些基于神经网络的模型的关键因素。

因子分解机支持的神经网络（FNN）[25]是一种使用FM预训练嵌入层的前向神经网络。然而，FNN只能捕捉高阶特征交互。宽度和深度模型（WDL）[1]最初是为谷歌应用推荐引入的。WDL联合训练宽线性模型和深度神经网络，以结合记忆和泛化的好处来构建推荐系统。然而，在WDL的宽度部分仍需要专业知识进行特征工程，这意味着交叉积变换也需要手动设计。为了减轻特征工程中的手工工作，DeepFM[4]用FM替换了WDL的宽度部分，并在FM和深度组件之间共享特征嵌入。DeepFM被认为是CTR估计领域的一个最先进模型。

深度与交叉网络（DCN）[22]能够以显式方式高效地捕捉有界度数的特征交互。同样，极端深度分解机（xDeepFM）[15]也通过提出一种新颖的压缩交互网络（CIN）部分，以显式方式建模低阶和高阶特征交互。

正如[23]所提到的，FM可能会因为其对所有特征交互的建模都使用相同的权重而受到阻碍，因为并非所有的特征交互行为都具有同等的有用性和预测性。他们提出了注意力分解机器（AFM）[23]模型，该模型使用注意力网络来学习特征交互的权重。深度兴趣网络（DIN）[26]通过兴趣分布表示用户的多样化兴趣，并设计了一种类似注意力的网络结构，根据候选广告局部激活相关兴趣。

## 2.3 SENET模块

胡[8]提出了“挤压与激励网络”（SENET），通过显式建模卷积特征通道之间的相互依赖性，以提高网络表示能力。SENET在图像分类任务中被证明是成功的，并在2017年ILSVRC分类任务中获得了第一名。

除了图像分类之外，还有其他关于SENET的应用[12, 21, 24]。[21]介绍了三种用于语义分割任务的不同变体SE模块。将常见胸部疾病进行分类以及在胸部X光片上定位可疑病变区域是另一个应用领域。[16]通过扩展SENET模块加入全局和局部注意力（GALA）模块，在ILSVRC上获得了最先进的准确率。

# 3.提出的模型

我们的目标是：以细粒度方式学习features的importance和feature intreactions。因此，在CTR预估任务中提出了Feature Importance and Bilinear feature Interaction NETwork(FiBiNET)。

在本节中，我们将描述我们提出的模型的架构，如图1所示。为了清晰起见，我们省略了可以简单合并的逻辑回归部分。我们提出的模型包括以下部分：稀疏输入层、嵌入层、SENET层、双线性交互层、组合层、多个隐藏层和输出层。稀疏输入层和嵌入层与DeepFM[4]相同，它采用稀疏表示法对输入特征进行编码，并将原始特征输入嵌入到密集向量中。SENET层可以将嵌入层转换为SENET-like嵌入特征，有助于提高特征的可区分性。接下来的双线性交互层分别对原始嵌入和SENET-like嵌入进行二阶特征交互建模。随后，这些交叉特征通过组合层连接起来，该组合层融合了双线性交互层的输出。最后，我们将交叉特征输入到一个深度神经网络中，网络输出预测分数。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/2fa0ea6f6593dbb0388c4cf6178052b0a988c6277707e1de100729f44ea1f91c0fb88a64b637bed80d9669ad53aaf264?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 FiBiNET的结构

## 3.1 稀疏输入与嵌入层

sparse input layer和embedding layer会被广泛用于deep learning based的CTR模型中。sparse input layer会为原始的input features采用一个sparse representation。该embedding layer可以将sparse feature嵌入到一个低维、dense的real-value向量上。embedding layer的输出是一个宽拼接的field embedding向量：

$$
E = [e_1, e_2, \cdots, e_i, \cdots, e_f]
$$

其中：

- f表示fields的数目
- $$e_i \in R^k$$表示第i个field的embedding
- k是embedding layer的维度

## 3.2 SENET Layer

据我们所知，**不同的features对于目标任务来说具有许多importances**。例如，当我们预测一个人的收入时，feature“职业”要比feature“喜好”更重要。受计算机视觉中SENET的成功所影响，我们**引入一个SENET机制，让模型更关注feature importance**。对于特定的CTR预估任务，我们可以动态增加importances的权重，并通过SENET机制减少无信息量特征（uninformative features）的weights。

我们将feature embeddings作为输入，SENET会为field embeddings生成weight vector $$A = \lbrace a_1, \cdots, a_i, \cdots, a_f \rbrace$$，接着**将原始embedding E与vector A进行rescale**，得到一个新的embedding（SENET-Like embedding）：

$$V = [v_1, \cdots, v_i, \cdots, v_f]$$

其中：

- $$a_i \in R$$是一个标量，它表示第i个field embedding $$v_i$$的weight，
- $$v_i \in R^k$$表示第i个field的SENET-Like embedding，$$i \in [1,2, \cdots, f], V \in R^{f \times k}$$，其中k是一个embedding size，f是一个fields的数目。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/90a2e8eb5683bded7bb5ce844e060d6ebb752a02212a31c0983ed8a3aeb3ea9c79e0945df2d1249906cc226e6b127dbc?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 SENET layer

如图2所示，SENET由三个steps组成：squeeze step（压缩）、excitation step（激活）、re-weight step（调权）。这三步细节描述如下：

**Squeeze**

该step用于**计算每个field embedding的"汇总统计信息（summary statistics）"**。具体来说，我们使用pooling方法：比如max或mean来将原始embedding $$E = [e_1, \cdots, e_f]$$压缩到一个statistic vector $$Z = [z_1, \cdots, z_i, \cdots, z_f]$$，其中：$$i \in [1, \cdots, f]$$，$$z_i$$是一个scalar value，它表示关于第i个feature representation的全局信息。$$z_i$$可以被计算为以下的全局mean pooling：

$$
z_i = F_{sq}(e_i) = \frac{1}{k} \sum\limits_{t=1}^k e_i^{(k)}
$$

...(1)

在原始SENET paper[8]中的squeeze function是max pooling。然而，我们的实验结果表明：**mean-pooling效果要好于max pooling**。

**Excitation**

该step可以**基于statistic vector Z来学习每个field embedding的weight**。我们使用两个FC layers来学习该weights。第一个FC layer是一个**降维layer**，它具有参数$$W_1$$，使用一个超参数衰减率r，接着使用$$\sigma_1$$作为非线性函数。第二个FC layer会使用参数$$W_2$$来**增加维度**。正式的，field embedding的weight可以如下进行计算：

$$
A = F_{ex}(Z) = \sigma_2(W_2 \sigma_1(W_1 Z))
$$

...(2)

其中：

- $$A \in R^f$$是一个vector
- $$\sigma_1$$和$$\sigma_2$$是activation functions
- $$W_1 \in R^{f \times \frac{f}{r}}, W_2 \in R^{\frac{f}{r} \times f}$$是学习参数，其中r是reduction ratio

**Re-weight**

**在SENET中的最后一步是一个reweight step**，它在原paper(8)中被称为re-scale。它会**在原始field embedding E和field weight vector A间做field-wise乘法**，并输出new embedding(SENET-Like embedding) $$V = \lbrace v_1, \cdots, v_i, \cdots, v_f \rbrace$$。SENET-Like embedding V可以计算如下：

$$
V = F_{ReWeight} (A, E) = [a_1 \cdot e_1, \cdots, a_f \cdot e_f] = [v_1, \cdots, v_f]
$$

...(3)

其中：

$$a_i \in R, e_i \in R^k, v_i \in R^k$$

简短来说，**SENET会使用两个FCs来动态学习features importance**。对于一个特定任务，它会增加important features的weights，并降低uninformative features的features。

## 3.3 Bilinear-Interaction Layer

该Interaction layer是一个用来计算二阶交叉的layer。在Interaction layer中经典的feature interactions是**内积（inner product）和哈达玛积（Hadamard product）**。

- 内积（inner product）：被广泛应用于shallow models中，比如：FM、FFM，
- 哈达玛积（Hadamard product）：被广泛用于深度模型中，比如：AFM和NFM。

内积和哈达玛积的形式分别表示为：

$$
\lbrace (v_i \cdot v_j) x_i x_j \rbrace_{(i,j) \in R_x} \\
\lbrace (v_i \odot v_j) x_i x_j \rbrace_{(i,j) \in R_x}
$$

其中：

- $$R_x = \lbrace (i, j) \rbrace_{i \in \lbrace 1, \cdots, f \rbrace, j \in \lbrace 1, \cdots, f \rbrace, j > i}$$

- $$v_i$$是第i个field embedding vector
- $$\cdot$$表示常规的内积
- $$\odot$$表示哈达玛积，例如：$$[a_1, a_2, a_3] \odot [b_1, b_2, b_3] = [a_1b_1, a_2b_2, a_3b_3]$$

在Interaction layer中的内积和哈达玛积对于有效建模在sparse dataset中的特征交叉过于简单。因此，我们提出了更细粒度的方法，它会**组合内积和哈达玛积使用额外参数来学习feature interactions**。如图3.c所示，在矩阵W和向量$$v_i$$间使用内积(inner product)，在矩阵W和向量$$v_j$$间使用哈达玛积(Hadamard product)。特别的，我们在该layer中提出了三种类型的双线性函数（bilinear functions），我们称为**“Bilinear-Interaction layer”**。以第i个field embedding $$v_i$$和第j个field embedding $$v_j$$作为示例，feature interaction $$p_{ij}$$的结果可以通过如下方式计算：

**a. Field-All Type**

$$
p_{ij} = v_i \cdot W \odot v_j
$$

...(4)

其中：

- $$W \in R^{k \times k}$$
- $$v_i, v_j \in R^k$$是第i个和第j个field embedding，$$1 \leq i \leq f, i \leq j \leq f$$

这里，**W在所有$$v_i, v_j$$的field交叉对（pair）间共享**，在Bilinear-Interaction layer中存在k x k参数，因此我们称该type为“Field-ALL”。

**Field-Each Type**

$$
p_{ij} = v_i \cdot W_i \odot v_j
$$

...(5)

其中：

- $$W_i \in R^{k \times k}, v_i, v_j \in R^k$$是第i和第j个field embedding, $$ 1 \leq i \leq f, i \leq j \leq f$$

这里：

- **$$W_i$$是第i个field的参数矩阵**

在Bilinear-Interaction layer中存在$$f\times k \times k$$，因为我们有f个不同的fields，因此，这里我们称为“Field-Each"。

**c.Field-Interaction Type**

$$
p_{ij} = v_i \cdot W_{ij} \odot v_j
$$

...(6)

其中，$$W_{ij} \in R^{k \times k}$$是field i和field j间的interaction的参数矩阵，$$1 \leq i \leq f, i \leq j \leq f$$。在该layer上的可学习参数的总数目是$$n \times k \times k$$，n是field interactions的数目，它等于$$\frac{f(f-1)}{2}$$。这里我们称该type为“Field-Interaction“。

如图1所示，我们有两个embeddings（original embedding和SENET-like embedding），我们可以为任意embeddings上采用bilinear function或Hadapard product作为特征交叉操作。因此，我们在该layer上具有不同的特征交叉组合。在第4.3节中，我们讨论了bilinear function 和Hadamard product不同组合的效果。另外，我们具有三种不同类型的特征交叉方法（Field-All, Field-Each, Field-Interaction）来应用到我们的模型中。

在本节中Bilinear-Interaction layer可以：

- 从original embedding E输出一个interaction vector $$p = [p_1, \cdots, p_i, \cdots, p_n]$$，
- 从SENET-like embedding V输出一个SENET-Like interaction vector $$q = [q_1, \cdots, q_i, \cdots, q_n]$$，

其中：

- $$p_i, q_i \in R^k$$均是vectors

**3.4 Combination Layer**

combination layer会将在interaction vector p和q进行拼接，并将concatenated vector输入到在FiBiNET的后续layer上。它可以表示成以下形式：

$$
c = F_{concat}(p, q) = [p_1, \cdots, p_n, q_1, \cdots, q_n] = [c_1, \cdots, c_{2n}]
$$

...(7)

如果我们将在vector c中的每个element进行求和，接着使用一个sigmoid function来输出一个prediction value，我们具有一个shallow CTR模型。为了更进一步拿到效果收益，我们会将shallow组件和一个经典的DNN进行组合到一个统一模型中来构成deep network结构，该统一模型被称为deep model。


<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/c6f533b4bfe33778707a50be0f0cd5705c0af0904988eab4d9cbcaca75760f6bbcafac9d84ba2546c218c98c0b8f75ed?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3 用于计算feature interactions的不同方法。 a) 内积（Inner product） b) Hadamard product  c) 我们提出的bilinear interaction。这里在inner product中的$$p_{ij}$$是一个标量，它是在Hadamard product以及bilinear function中的一个vector

## 3.5 Deep Network

deep network由许多FC layers组成，它会隐式捕获高阶特征交叉。如图1所示，deep network的输入是combination layer的输出。假设：$$a^{(0)} = [c1, c_2, \cdots, c_{2n}]$$表示combination layer的输出，其中：$$c_i \in R^k$$，n是field interactions的数目。接着，$$a^{(0)}$$被输入到DNN中，feed forward过程如下：

$$
a^{(l)} = \sigma(W^{(l)} a^{(l - 1)} + b^{(l)})
$$

...(8)

其中：

- l是depth，$$\sigma$$是activation function
- $$W^{(l)}, b^{(l)}, a^{(l)}$$是第l个layer上的模型weight、bias、output

在这该后，一个dense real-value feature vector会被生成，它最终输入到CTR预估的sigmoid function中：$$y_d = \sigma(W^{\mid L \mid + 1} a^{\mid L \mid} + b^{\mid L \mid +1}) $$，其中：$$\mid L \mid$$是DNN的depth。

## 3.6 Output Layer

为了简洁，我们给出模型输出的整个公式：

$$
\hat{y} = \sigma(w_0 + \sum\limits_{i=0}^m w_i x_i + y_d)
$$

...(9)

其中：

- $$\hat{y} \in (0, 1)$$是CTR的predicted value
- $$\sigma$$是sigmoid function
- m是feature size
- x是一个input
- $$w_i$$是linear part的第i个weight
- $$\lbrace w_0, \lbrace w_i\rbrace_{i=1}^m, \lbrace e_i \rbrace_{i=1}^2, \lbrace W^{(i)} \rbrace_{i=1}^{\mid L \mid} \rbrace$$。学习过程的目标是最小化以下的目标函数（cross entropy）：

$$
loss = - \frac{1}{N} \sum\limits_{i=1}^N (y_i log(\hat{y}_i) + (1 - y_i) * log(1 - \hat{y}_i))
$$

...(10)

其中：

- $$y_i$$是第i个样本的ground truth
- $$\hat{y}_i$$是predicted CTR
- N是样本的total size

**3.6.1 与FM和FNN的关系**

假设我们将SENET layer和Bilinar-Interaction layer移除，不难发现我们的模型可以被看成是FNN。当我们进一步移除DNN part，同时使用一个constant sum，接着该shallow FiBiNET会被降级到传统的FM模型。

# 4.实验

略.


# 参考


- 1.[https://arxiv.org/pdf/1905.09433.pdf](https://arxiv.org/pdf/1905.09433.pdf)