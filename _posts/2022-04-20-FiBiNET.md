---
layout: post
title: FiBiNET框架
description: 
modified: 2022-04-20
tags: 
---


weibo在《FiBiNET: Combining Feature Importance and Bilinear feature
Interaction for Click-Through Rate Prediction》提出了FiBiNET。

# 1.摘要

# 3.提出的模型

我们的目标是，以细粒度方式学习features的importance和feature intreactions。因此，在CTR预估任务中提出了Feature Importance and Bilinear feature Interaction NETwork(FiBiNET)。

在本节中，我们会描述在图1中所描述的模型。为了简单，我们忽略LR部分。提出的模型包含了以下部分：

- Sparse input layer
- embedding layer
- SENET layer
- Bilinear-Interaction layer
- combination layer
- multiple hidden layers
- output layer

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/2fa0ea6f6593dbb0388c4cf6178052b0a988c6277707e1de100729f44ea1f91c0fb88a64b637bed80d9669ad53aaf264?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 FiBiNET的结构

sparse input layer和embedding layer与DeepFM【4】中相同，它会为input features采用一个sparse representation，并将raw feature input嵌入到一个dense vector中。

SENET layer可以将一个embedding layer转换成SENET-Like embedding features，它会帮助增强feature辩别力。

随后的Bilinear-Interaction layer会分别在原始embedding和SENET-Like embedding上建模二阶特征交叉。

接着，这些交叉特征会通过一个combination layer进行拼接，它会将Bilinear-Interaction layer的输出进行合并。最后，我们会将cross features进行feed给一个DNN网络，该网络会输入prediction score。

## 3.1 Sparse Input和Embedding layer

sparse input layer和embedding layer会被广泛用于deep learning based的CTR模型中。sparse input layer会为原始的input features采用一个sparse representation。该embedding layer可以将sparse feature嵌入到一个低维、dense的real-value vector上。embedding layer的output是一个宽拼接的field embedding vector：

$$
E = [e_1, e_2, \cdots, e_i, \cdots, e_f]
$$

其中：

- f表示fields的数目
- $$e_i \in R^k$$表示第i个field的embedding
- k是embedding layer的维度

## 3.2 SENET Layer

据我们所知，**不同的features对于目标任务来说具有许多importances**。例如，当我们预测一个人的收入时，feature“职业”要比feature“喜好”更重要。受计算机视觉中SENET的成功所影响，我们**引入一个SENET机制，让模型更关注feature importance**。对于特定的CTR预估任务，我们可以动态增加importances的权重，并通过SENET机制减少无信息量特征（uninformative features）的weights。

我们将feature embeddings作为输入，SENET会为field embeddings生成weight vector $$A = \lbrace a_1, \cdots, a_i, \cdots, a_f \rbrace$$，接着**将原始embedding E与vector A进行rescale**，得到一个新的embedding（SENET-Like embedding）：

$$V = [v_1, \cdots, v_i, \cdots, v_f]$$

其中：

- $$a_i \in R$$是一个标量，它表示第i个field embedding $$v_i$$的weight，
- $$v_i \in R^k$$表示第i个field的SENET-Like embedding，$$i \in [1,2, \cdots, f], V \in R^{f \times k}$$，其中k是一个embedding size，f是一个fields的数目。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/90a2e8eb5683bded7bb5ce844e060d6ebb752a02212a31c0983ed8a3aeb3ea9c79e0945df2d1249906cc226e6b127dbc?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 SENET layer

如图2所示，SENET由三个steps组成：squeeze step（压缩）、excitation step（激活）、re-weight step（调权）。这三步细节描述如下：

**Squeeze**

该step用于**计算每个field embedding的"汇总统计信息（summary statistics）"**。具体来说，我们使用pooling方法：比如max或mean来将原始embedding $$E = [e_1, \cdots, e_f]$$压缩到一个statistic vector $$Z = [z_1, \cdots, z_i, \cdots, z_f]$$，其中：$$i \in [1, \cdots, f]$$，$$z_i$$是一个scalar value，它表示关于第i个feature representation的全局信息。$$z_i$$可以被计算为以下的全局mean pooling：

$$
z_i = F_{sq}(e_i) = \frac{1}{k} \sum\limits_{t=1}^k e_i^{(k)}
$$

...(1)

在原始SENET paper[8]中的squeeze function是max pooling。然而，我们的实验结果表明：**mean-pooling效果要好于max pooling**。

**Excitation**

该step可以**基于statistic vector Z来学习每个field embedding的weight**。我们使用两个FC layers来学习该weights。第一个FC layer是一个**降维layer**，它具有参数$$W_1$$，使用一个超参数衰减率r，接着使用$$\sigma_1$$作为非线性函数。第二个FC layer会使用参数$$W_2$$来**增加维度**。正式的，field embedding的weight可以如下进行计算：

$$
A = F_{ex}(Z) = \sigma_2(W_2 \sigma_1(W_1 Z))
$$

...(2)

其中：

- $$A \in R^f$$是一个vector
- $$\sigma_1$$和$$\sigma_2$$是activation functions
- $$W_1 \in R^{f \times \frac{f}{r}}, W_2 \in R^{\frac{f}{r} \times f}$$是学习参数，其中r是reduction ratio

**Re-weight**

**在SENET中的最后一步是一个reweight step**，它在原paper(8)中被称为re-scale。它会**在原始field embedding E和field weight vector A间做field-wise乘法**，并输出new embedding(SENET-Like embedding) $$V = \lbrace v_1, \cdots, v_i, \cdots, v_f \rbrace$$。SENET-Like embedding V可以计算如下：

$$
V = F_{ReWeight} (A, E) = [a_1 \cdot e_1, \cdots, a_f \cdot e_f] = [v_1, \cdots, v_f]
$$

...(3)

其中：

$$a_i \in R, e_i \in R^k, v_i \in R^k$$

简短来说，**SENET会使用两个FCs来动态学习features importance**。对于一个特定任务，它会增加important features的weights，并降低uninformative features的features。

## 3.3 Bilinear-Interaction Layer

该Interaction layer是一个用来计算二阶交叉的layer。在Interaction layer中经典的feature interactions是**内积（inner product）和哈达玛积（Hadamard product）**。内积被广泛应用于shallow models中，比如：FM、FFM，而哈达玛积（Hadamard product）则被广泛用于深度模型中，比如：AFM和NFM。内积和哈达玛积的形式分别表示为：

$$
\lbrace (v_i \cdot v_j) x_i x_j \rbrace_{(i,j) \in R_x} \\
\lbrace (v_i \odot v_j) x_i x_j \rbrace_{(i,j) \in R_x}
$$

其中：

- $$R_x = \lbrace (i, j) \rbrace_{i \in \lbrace 1, \cdots, f \rbrace, j \in \lbrace 1, \cdots, f \rbrace, j > i}$$

- $$v_i$$是第i个field embedding vector
- $$\cdot$$表示常规的内积
- $$\odot$$表示哈达玛积

例如：$$[a_1, a_2, a_3] \odot [b_1, b_2, b_3] = [a_1b_1, a_2b_2, a_3b_3]$$。在Interaction layer中的内积和哈达玛积对于有效建模在sparse dataset中的特征交叉过于简单。因此，我们提出了更细粒度的方法，它会组合内积和哈达玛积使用额外参数来学习feature interactions。如图3.c所示，在矩阵W和向量$$v_i$$间使用内积，在矩阵W和向量$$v_j$$间使用哈达玛积。特别的，我们在该layer中提出了三种类型的双线性函数（bilinear functions），我们称为“Bilinear-Interaction layer”。以第i个field embedding $$v_i$$和第j个field embedding $$v_j$$作为示例，feature interaction $$p_{ij}$$的结果可以通过如下方式计算：

**a. Field-All Type**

$$
p_{ij} = v_i \cdot W \odot v_j
$$

...(4)

其中：

- $$W \in R^{k \times k}$$
- $$v_i, v_j \in R^k$$是第i个和第j个field embedding，$$1 \leq i \leq f, i \leq j \leq f$$

这里：W在所有$$v_i, v_j$$的field交叉对间共享，在Bilinear-Interaction layer中存在k x k参数，因此我们称该type为“Field-ALL”。

**Field-Each Type**

$$
p_{ij} = v_i \cdot W_i \odot v_j
$$

...(5)

其中：

- $$W_i \in R^{k \times k}, v_i, v_j \in R^k$$是第i和第j个field embedding, $$ 1 \leq i \leq f, i \leq j \leq f$$

这里的$$W_i$$是第i个field的参数矩阵，在Bilinear-Interaction layer中存在$$f\times k \times k$$，因为我们有f个不同的fields，因此，这里我们称为“Field-Each"。

**c.Field-Interaction Type**

$$
p_{ij} = v_i \cdot W_{ij} \odot v_j
$$

...(6)

其中，$$W_{ij} \in R^{k \times k}$$是field i和field j间的interaction的参数矩阵，$$1 \leq i \leq f, i \leq j \leq f$$。在该layer上的可学习参数的总数目是$$n \times k \times k$$，n是field interactions的数目，它等于$$\frac{f(f-1)}{2}$$。这里我们称该type为“Field-Interaction“。

如图1所示，我们有两个embeddings（original embedding和SENET-like embedding），我们可以为任意embeddings上采用bilinear function或Hadapard product作为特征交叉操作。因此，我们在该layer上具有不同的特征交叉组合。在第4.3节中，我们讨论了bilinear function 和Hadamard product不同组合的效果。另外，我们具有三种不同类型的特征交叉方法（Field-All, Field-Each, Field-Interaction）来应用到我们的模型中。

在本节中，Bilinear-Interaction layer可以从original embedding E输出一个interaction vector $$p = [p_1, \cdots, p_i, \cdots, p_n]$$，从SENET-like embedding V输出一个SENET-Like interaction vector $$q = [q_1, \cdots, q_i, \cdots, q_n]$$，其中：$$p_i, q_i \in R^k$$均是vectors。

**3.4 Combination Layer**

combination layer会将在interaction vector p和q进行拼接，并将concatenated vector输入到在FiBiNET的后续layer上。它可以表示成以下形式：

$$
c = F_{concat}(p, q) = [p_1, \cdots, p_n, q_1, \cdots, q_n] = [c_1, \cdots, c_{2n}]
$$

...(7)

如果我们将在vector c中的每个element进行求和，接着使用一个sigmoid function来输出一个prediction value，我们具有一个shallow CTR模型。为了更进一步拿到效果收益，我们会将shallow组件和一个经典的DNN进行组合到一个统一模型中来构成deep network结构，该统一模型被称为deep model。


<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/c6f533b4bfe33778707a50be0f0cd5705c0af0904988eab4d9cbcaca75760f6bbcafac9d84ba2546c218c98c0b8f75ed?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3 用于计算feature interactions的不同方法。 a) 内积（Inner product） b) Hadamard product  c) 我们提出的bilinear interaction。这里在inner product中的$$p_{ij}$$是一个标量，它是在Hadamard product以及bilinear function中的一个vector

## 3.5 Deep Network

deep network由许多FC layers组成，它会隐式捕获高阶特征交叉。如图1所示，deep network的输入是combination layer的输出。假设：$$a^{(0)} = [c1, c_2, \cdots, c_{2n}]$$表示combination layer的输出，其中：$$c_i \in R^k$$，n是field interactions的数目。接着，$$a^{(0)}$$被输入到DNN中，feed forward过程如下：

$$
a^{(l)} = \sigma(W^{(l)} a^{(l - 1)} + b^{(l)})
$$

...(8)

其中：

- l是depth，$$\sigma$$是activation function
- $$W^{(l)}, b^{(l)}, a^{(l)}$$是第l个layer上的模型weight、bias、output

在这该后，一个dense real-value feature vector会被生成，它最终输入到CTR预估的sigmoid function中：$$y_d = \sigma(W^{\mid L \mid + 1} a^{\mid L \mid} + b^{\mid L \mid +1}) $$，其中：$$\mid L \mid$$是DNN的depth。

## 3.6 Output Layer

为了简洁，我们给出模型输出的整个公式：

$$
\hat{y} = \sigma(w_0 + \sum\limits_{i=0}^m w_i x_i + y_d)
$$

...(9)

其中：

- $$\hat{y} \in (0, 1)$$是CTR的predicted value
- $$\sigma$$是sigmoid function
- m是feature size
- x是一个input
- $$w_i$$是linear part的第i个weight
- $$\lbrace w_0, \lbrace w_i\rbrace_{i=1}^m, \lbrace e_i \rbrace_{i=1}^2, \lbrace W^{(i)} \rbrace_{i=1}^{\mid L \mid} \rbrace$$。学习过程的目标是最小化以下的目标函数（cross entropy）：

$$
loss = - \frac{1}{N} \sum\limits_{i=1}^N (y_i log(\hat{y}_i) + (1 - y_i) * log(1 - \hat{y}_i))
$$

...(10)

其中：

- $$y_i$$是第i个样本的ground truth
- $$\hat{y}_i$$是predicted CTR
- N是样本的total size

**3.6.1 与FM和FNN的关系**

假设我们将SENET layer和Bilinar-Interaction layer移除，不难发现我们的模型可以被看成是FNN。当我们进一步移除DNN part，同时使用一个constant sum，接着该shallow FiBiNET会被降级到传统的FM模型。

# 4.实验

略.


# 参考


- 1.[https://arxiv.org/pdf/1905.09433.pdf](https://arxiv.org/pdf/1905.09433.pdf)