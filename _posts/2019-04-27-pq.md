---
layout: post
title: product quantization
description: 
modified: 2017-08-27
tags: [深度学习]
---

# 1.介绍

计算高维向量间的欧氏距离，在许多应用中是一个基本需求。尤其是最近邻搜索问题中被广泛使用。由于维度灾难，最近邻搜索相当昂贵。在D维欧氏空间$$R^D$$上，该问题是：在一个n维vectors的有限集合$$Y \subset R^D$$，寻找element $$NN(x)$$，可以最小化与query vector $$x \in R^D$$间的距离：

$$
NN(x) = \underset{y \in Y}{argmin} \ d(x,y)
$$

...(1)

许多多维索引方法，比如KD-tree或其它branch&bound技术，被提出来减小搜索时间。然而，**对于高维空间，发现这样的方法比起brute-force距离计算（复杂度O(nD)）并没有更高效多少**。

一些算法文献通过执行ANN（近似最近邻）搜索来解决该问题。这些算法的关键点是："只"寻找具有较高概率的NN，来替代概率1. 大多数研究都在欧氏距离上，尽量最近有研究在其它metrics上提出[10]。**在本paper中，我们只考虑欧氏距离**，它可以适用于许多应用。在本case中，一个最流行的ANN算法是**欧氏局部敏感哈希算法（E2LSH）**，它在有限假设的搜索质量上提供了理论保证。它已经被成功用于local descriptors和3D object indexing。然而，对于真实数据，LSH还是通过启发法来执行，会利用vectors的分布。这些方法包括：randomized KD-trees、hierarchical k-means，这两种方法在FLANN选择算法上有实现。

ANN通常会基于search quality和efficiency间的trade-off来进行比较。然而，该trade-off并不能说明indexing结构的内存需要。在E2LSH的case上，内存的使用要比original vectors更高。另外，E2LSH和FLANN需要基于exact L2距离（如果访问速度很重要，它需要在主存中存储indexed vectros）来执行一个final re-ranking step。该constraint会严重限制可被这些算法处理的vectors的数目。最近，**研究者们提出了受限内存使用的方法**。该问题的关键是涉及大量数据，例如：在大规模场景识别[17]中，需要索引数百万到数十亿的图片。[17]中通过单个global GIST descriptor来表示一个图片，该descriptor可以被映射到一个short binary code上。当使用无监督时，会学到这样的mapping，以便在embedded space中由hamming距离定义的neighborhood可以影响在原features的欧氏空间中的neighborhood。**欧氏最近邻的搜索接着被近似成：通过codes间的hamming距离的最近邻搜索**。在[19]中，spectral hashing（SH）的效果要好于由RBM、boosting和LSH生成的binary codes。相似的，Hamming embedding方法[20]会在Bag-of-features的图片搜索框架中使用一个binary signature来重新定义quantized SIFT或GIST descriptors。

在本paper中，**我们使用quantization来构建short codes。目标是使用vector-to-centroid distances来估计距离**，例如：query vector是没有被量化的（quantized），codes只被分配给database vectors。这会减小quantization noise，进而提升搜索质量。为了获得精确的距离，quantization error必须被限制。因此，**centroids的总数目k必须足够大**，

例如：对于64-bit codes使用$$k=2^{64}$$。这会抛出一些问题：如何学到密码本（codebook）以及分配一个vector？

- 首先，要学到该quantizer所需的样本量很大，比如：k的许多倍。
- 第二，该算法本身的复杂度很高。
- 最后，地球上的计算机内存量不足以存储表示centroids的floating point。

**hierarchical k-means(HKM)**提升了learning stage、以及assignment过程的efficiency。然而，前面提到的限制仍存在，特别是：内存使用以及learning set的size。另一个可能是**scalar quantizers**，但他们提供了更差的quantization error特性。对于均匀向量分布，**lattice quantizers**提供了更好的quantization特性，但该条件在现实vectors中很难满足。特别的，这些quantizers执行在indexing任务上要比k-means更差。在本paper中，我们只关注**product quantizers**。据我们所知，这样的semi-structured quantizer从没有在任何最近邻搜索方法中考虑。

我们的方法有两个优点：

- 首先，可能的distances的数目要比competing Hamming embedding方法要更高，因为在这些技术中使用的Hamming space只允许少量distinct distance。
- 第二，作为byproduct方法，我们可以获得一个expected squared distance的估计，它对于e-radius搜索或者lowe's distance ratio criterion来说是必需的。

在[20]使用Hamming space的动机是，高效计算距离。注意，然而，计算Hamming距离的最快方法之一，包含了使用table lookups。我们的方法会使用相同数目的table lookups，来产生相当的效率。

对于非常大的数据集，对所有codes与query vector进行遍历比较开销相当大。因此，**引入一个modified inverted file结构来快速访问最相关vectors**。会使用一个**粗粒度量化器(coarse quantizer)**来实现该inverted file结构。其中，对应于一个cluster(index)的vectors会被存储在一个相关列表中。在该list中的vectors通过short codes（通过product quantizer计算得来）来表示，被用于编码对应于聚类中心的其它vector。

我们的方法的关注点是：在两种vectors上进行验证，称为local SIFT和global GIST descriptors。通过SOTA对比，我们的方法要好于之前的技术（比如：SH， Hamming embedding以及FLANN）。

# 2.背景知识：quantization、product quantizer

关于vector quantization有大量文献提供。在本节，我们聚焦在相关概念上。

## A. Vector quantization

Quantization是一个分解性过程（destructive process），它在信息论中被大量研究。它的目标是，减小representation space的基数（cardinality），特别是当输入数据是real-valued时。

正式的：**一个quantizer是一个函数q**，它将一个D维向量$$x \in R^D$$映射到vector q(x)上：

 $$q(x) \in C = \lbrace c_i; i \in I \rbrace$$

其中：

- index set $$I$$是假设是有限的：$$I=0, \cdots, k-1$$。
- reproduction values $$c_i$$：表示**centroids**。
- reproduction values C的集合：称为size k的**codebook**。

vectors映射到一个给定index i上的集合$$V_i$$，被称为一个**(Voronoi) cell**，定义为：

$$
V_i \triangleq \lbrace x \in R^D : q(x)=c_i \rbrace
$$

...(2)

一个quantizer的k个cells形成了$$R^D$$的一个划分（partition）。通过定义可知：**在同一cell $$V_i$$上的所有vectors，可以通过相同centroid $$c_i$$来构建**。一个quantizer的quality通常通过input vector x和它的reproduction value $$q(x)$$间的MSE来进行测量：

$$
MSE(q) = E_x[d(q(x), x)^2] = \int p(x) d(q(x),x)^2 dx
$$

...(3)

其中，$$d(x,y)=\| x-y \|$$是x和y的欧氏距离，$$p(x)$$是随机变量X的概率分布函数。对于一个专门的概率分布函数，等式(3)数值上使用Monte-Carlo sampling进行计算，作为在一个大数据集样本$$\|q(x)-x\|^2$$上的平均。

为了让quantizer是最优的，必须满足L1oyd  optimality condition的两个特性。

- **1. vector x必须被quantized到它最近的codebook centroid**，根据欧氏距离：

$$
q(x)=\underset{c_i \in C}{argmin} \  d(x,c_i)
$$

...(4)

作为结果，cells通过超参数来限定。

- **2. 重构值(reconstruction value)必须是在Voronoi cell上vectors的期望值**：

$$
c_i = E_x [x | i] = \int_{v_i} p(x | x \in V_i) x dx
$$

...(5)

**Lloy quantizer，它对应于k-means cluster算法，通过迭代式分配一个training set的vectors给centroids、并将这些已分配vectors的centroids进行re-estimating的方式来寻找一个接近最优的codebook**。

下面，我们会假设两个Lloyd conditions成立，正如我们使用k-means来学习该quantizer。注意，然而，k-means只会根据quantization error来找一个local optimum。
 
 下面会使用到的另一个quantity是，当构建一个由通过相应的centroid $$c_i$$得到的cell $$V_i$$的vector时，获得的均方失真$$e(q,c_i)$$。通过$$p_i=P(q(x)=c_i)$$来表示一个vector被分配给centroid $$c_i$$的概率，它可以通过下式计算：
 
 $$
 e(q,c_i) = \frac{1}{p_i} \int_{v_i} d(x,q(x))^2 p(x) dx
 $$
 
 ...(6)
 
 注意，MSE可以通过这些quantities来获得：
 
 $$
 MSE(q) = \sum\limits_{i \in I} p_i e(q, c_i)
 $$
 
 ...(7)
 
**存储index value（没有进一步处理（entropy coding））的内存开销，是$$log_2 k$$ bits**。因此，使用一个k的2阶很方法，因为通过quantizer生成的code以binary memory的方式生成。
 
## B. Product quantizers
 
 假设我们考虑一个128维的vector，例如，SIFT descriptor [23]。一个quantizer会产生64-bits codes，例如，每个component只有0.5 bit，包含了$$k=2^{64}$$的centroids。因此，使用Lloyd算法或HKM并不重要，因为所需的样本数据、以及学习该quantizer的复杂度是：**k的数倍**。为了表示k个centroids要存储$$D \times k$$的floating point值是**不可能的**。
 
 product quantization是一个高效的解决该问题的解法。它是source coding中的常用技术，允许选择要进行联合量化(quantized jointly)的components的数目（例如，24个components的groups可以使用强大的Leech lattice来量化）。
 
 input vector x被split成m个不同的subvectors $$u_j, 1 \leq j \leq m$$，维度为$$D^* = D/m$$，其中D是m的倍数。这些subvectors会使用m个不同的quantizers进行单独量化。一个给定vector x因此根据如下进行映射：
 
 $$
 x_1, \cdots, x_{D^*}, \cdots, x_{D-D^*+1}, \cdots, x_D \rightarrow q_1(u_1(x)), \cdots, q_m(u_m(x))
 $$
 
 ...(8)
 
 其中：
 
 - $$q_j$$是低复杂度的quantizer，它与第j个subvector有关。
 - subquantizer $$q_j$$与index set $$I_j$$、codebook $$C_j$$、以及相应的reproduction values $$c_j,i$$有关。
 
 product quantizer的reproduction通过product index set $$I=I_1 \times \cdots \times I_m$$的一个element进行标识。codebook因此被定义成Cartesian product：
 
 $$
 C = C_1 \times \cdots \times C_m
 $$
 
 ...(9)
 
 以及该set的centroid是m个subquantizers的centroid的拼接（concatenation）。从现在开始，我们假设，所有subquantizers具有相同的有限数目$$k^*$$的reproduction values。在该case中，centroids的总数由下式给定：
 
 $$
 k = (k^*)^m
 $$
 
 ...(10)
 
注意：在极端情况下(m=D)，一个vector x的所有components是被完全独立量化的。这时，**product quantizer就变成了一个scalar quantizer**。其中，与每个component有关的quantization function是不同的。
 
一个product quantizer的力量是：**使用多个centroids的小集合（它们与subquantizers有关）来生成一个更大的centroids集合**。当使用Lloyd算法学习该subquantizers时，会使用有限数目的vectors，在某种程度上，codebook仍会采用数据分布来表示。学习该quantizer的复杂度为： m X 对$$k^*$$个具有$$D^*$$的centroids执行k-means聚类的复杂度。
 
对codebook C显式存储效率不高。相反，我们会存储所有subquantizer的$$m \times k^*$$个centroids，例如：$$m D^* k^*=k^* D$$个floating points值。**对一个element进行量化需要$$k^* D$$个floating point操作**。表1总结了与k-means、HKM、product k-means对应所需的资源。product quantizer很明显是唯一可以被用于当k为大值时可以进行内存索引的方法。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/9e3d218b283bed9e6945480e4944e74482a8b0156c9291aa875e26728c2826e3900aebb8d8037162a82b918e6ccefd55?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=1.jpg&amp;size=750" width="300">

表1
 
 当选择一个常数值$$k^*$$，为了提供较好的quantization属性，每个subvector都应具有一个可对比的energy（平均）。确保该特性的一个方法是，通过将该vector乘以一个随机正交矩阵来进行quantization。然而，对于大多数vector types，这并不是必需的，也不推荐，因为连续的components通常通过构建来关联，并可以更好地与相同的subquantizer一起被量化。由于subspaces是正交的，与product quantizer的平方失真为：

$$
MSE(q) = \sum\limits_j MSE(q_j)
$$

...(11)

其中，$$MSE(q_j)$$是与quantizer $$q_j$$相关的distortion。图1展示了MSE是一个关于不同$$(m, k^*) $$tuples的code length的函数，其中code length为$$l = m log_2 k^*$$，如果$$k^*$$是2的幂。该曲线通过一个128维SIFT descriptors的集合获得，详见第V部分。你可以观察到，对于固定数目的bits，最好使用一个小数目的subquantizers以及更多centroids，要比使用许多subquantizers和更少centroids的要好。当m=1的极端情况下，product quantizer会成为一个常规的k-means codebook。

$$k^*$$的值越高，会增加quantizer的计算开销，如表1所示。它们也会增加存储centroids（$$k^* \times D floating point值$$）的内存使用量，当centroid look-up table不再fit cache内存时，这会进一步降低效率。在这种情况下m=1，超过16 bits来保存这样的开销可追踪将承受不起。使用$$k^*=256$$和$$m=8$$通常是一个合理的选择。

# 3.使用quantization进行搜索

...




# 参考

- 0.[Product quantization for nearest neighbor search](https://hal.inria.fr/inria-00514462v2/document)
