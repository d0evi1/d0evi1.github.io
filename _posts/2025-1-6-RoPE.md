---
layout: post
title: RoPE介绍
description: 
modified: 2025-1-7
tags: 
---

追一科技在《ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING》提出了RoPE的方法：

## 摘要

位置编码最近在Transformer架构中显示出其有效性。它为序列中不同位置元素之间的依赖关系建模提供了有价值的监督。在本文中，我们：

- 首先研究了将**位置信息**整合到基于Transformer的语言模型学习过程中的各种方法。
- 接着，我们提出了一种名为**旋转位置嵌入（Rotary Position Embedding, RoPE）**的新方法，以有效利用位置信息。具体来说，所提出的RoPE**通过旋转矩阵对绝对位置进行编码，同时将显式的相对位置依赖关系引入自注意力机制中**。值得注意的是，RoPE具有一些有价值的特性，包括：**序列长度的灵活性、随着相对距离增加而衰减的token间依赖性，以及为线性自注意力配备相对位置编码的能力**。
- 最后，我们在多种长文本分类基准数据集上评估了增强后的Transformer（称为**RoFormer**）的性能。

实验结果表明，RoFormer始终优于其他替代方法。此外，我们还提供了理论分析来解释一些实验结果。RoFormer已经集成到Huggingface中：[https://huggingface.co/docs/transformers/model_doc/roformer](https://huggingface.co/docs/transformers/model_doc/roformer)。

# 1 引言

**词序对于自然语言理解具有重要价值**。

- 基于循环神经网络（RNN）的模型通过沿时间维度递归计算隐藏状态来编码词的顺序。
- 基于卷积神经网络（CNN）的模型（如Gehring等[2017]）通常被认为是与位置无关的，但最近的研究（Islam等[2020]）表明，常用的填充操作可以隐式地学习位置信息。
- 近年来，基于Transformer（Vaswani等[2017]）构建的**预训练语言模型（PLMs）**在各种自然语言处理（NLP）任务中取得了最先进的性能，包括上下文表示学习（Devlin等[2019]）、机器翻译（Vaswani等[2017]）和语言建模（Radford等[2019]）等。

与基于RNN和CNN的模型不同，PLMs利用自注意力机制从语义上捕捉给定语料的上下文表示。因此，PLMs在并行化方面相比RNN取得了显著改进，并且与CNN相比，能够更好地建模更长的token内关系。

值得注意的是，**当前PLMs的自注意力架构被证明是与位置无关的**（Yun等[2020]）。基于这一观点，研究者们提出了多种方法将位置信息编码到学习过程中。

- 一方面，通*过预定义函数生成的**绝对位置编码**（Vaswani等[2017]）被添加到上下文表示中，而可训练的绝对位置编码（Gehring等[2017]、Devlin等[2019]、Lan等[2020]、Clark等[2020]、Radford等[2019]、Radford和Narasimhan[2018]）也被广泛使用。
- 另一方面，之前的工作（Parikh等[2016]、Shaw等[2018]、Huang等[2018]、Dai等[2019]、Yang等[2019]、Raffel等[2020]、Ke等[2020]、He等[2020]、Huang等[2020]）主要集中在**相对位置编码**上，通常将相对位置信息编码到注意力机制中。
- 此外，Liu等[2020]的作者提出从神经微分方程（Neural ODE，Chen等[2018a]的角度建模**位置编码的依赖性**，Wang等[2020]的作者提出在**复数空间中建模位置信息**。

尽管这些方法有效，但它们通常将位置信息添加到上下文表示中，因此不适合线性自注意力架构。

在本文中，我们提出了一种新方法，即**旋转位置嵌入（Rotary Position Embedding, RoPE）**，将位置信息引入PLMs的学习过程中。具体来说，RoPE通过旋转矩阵对绝对位置进行编码，同时将显式的相对位置依赖关系引入自注意力机制中。值得注意的是，RoPE具有一些优于现有方法的特性，包括序列长度的灵活性、随着相对距离增加而衰减的token间依赖性，以及为线性自注意力配备相对位置编码的能力。在多种长文本分类基准数据集上的实验结果表明，增强后的Transformer（称为**RoFormer**）相比基线方法能够取得更好的性能，从而证明了RoPE的有效性。

简而言之，我们的贡献如下：

- 我们研究了现有的**相对位置编码方法**，发现它们大多基于将位置编码添加到上下文表示的分解思想。我们提出了一种新方法，即**旋转位置嵌入（RoPE）**，将位置信息引入PLMs的学习过程中。其核心思想是通过将上下文表示与旋转矩阵相乘来编码相对位置，并具有清晰的理论解释。
- 我们研究了RoPE的特性，并表明**其随着相对距离的增加而衰减**，这符合自然语言编码的需求。我们还指出，之前的相对位置编码方法与线性自注意力不兼容。
- 我们在多种长文本基准数据集上评估了提出的RoFormer。实验结果表明，RoFormer始终优于其他替代方法。一些预训练语言模型的实验代码已在GitHub上开源：[https://github.com/ZhuiyiTechnology/roformer](https://github.com/ZhuiyiTechnology/roformer)。

本文的其余部分组织如下：在第2节中，我们建立了自注意力架构中位置编码问题的形式化描述，并回顾了之前的工作；在第3节中，我们描述了旋转位置编码（RoPE）并研究了其特性；在第4节中，我们报告了实验结果；最后，在第5节中对本文进行了总结。

# 2 背景与相关工作

## 2.1 初步知识

设 $S_N = \lbrace w_i \rbrace_{i=1}^N$ 为一个包含 N 个输入token的序列，其中：

- $ w_i $ 是第 $ i $ 个元素
- $ S_N $ 对应的词嵌入表示为 $ E_N = \lbrace x_i \rbrace_{i=1}^N $，其中 $ x_i \in R^d $ 是第i个token的d维词嵌入向量（不包含位置信息）。

自注意力机制首先将位置信息融入词嵌入中，并将其转换为查询（query）、键（key）和值（value）表示：

$$
\begin{aligned}
q_m &= f_q(x_m, m) \\
k_n &= f_k(x_n, n) \\
v_n &= f_v(x_n, n),
\end{aligned}
$$

...(1)

其中:

-  $ q_m $、$ k_n $ 和 $ v_n $ 分别通过函数 $ f_q $、$ f_k $ 和 $ f_v $ 融入了第 $ m $ 和第 $ n $ 个位置的信息。

查询和键值用于计算注意力权重，而输出则是对值表示的加权和：

$$
\begin{aligned}
a_{m,n} &= \frac{\exp\left(\frac{q_m^\intercal k_n}{\sqrt{d}}\right)}{\sum_{j=1}^N \exp\left(\frac{q_m^\intercal k_j}{\sqrt{d}}\right)} \\
o_m &= \sum_{n=1}^N a_{m,n} v_n.
\end{aligned}
$$

...(2)

现有的基于Transformer的位置编码方法主要集中在选择合适的函数来构建公式（1）。

### 2.2 绝对位置嵌入
公式（1）的一个典型选择是：

$$
f_{t: t \in \{q, k, v\}}(x_i, i) := W_{t: t \in \{q, k, v\}}(x_i + p_i),
$$

...(3)

其中：

- $ p_i \in \mathbb{R}^d $ 是一个依赖于token $ x_i $ 位置的 $ d $ 维向量。

之前的工作（Devlin等[2019]、Lan等[2020]、Clark等[2020]、Radford等[2019]、Radford和Narasimhan[2018]）引入了一组可训练的向量 $ p_i \in \{p_t\}_{t=1}^L $，其中 $ L $ 是最大序列长度。Vaswani等[2017]的作者提出使用正弦函数生成 $ p_i $：

$$
\begin{aligned}
p_{i,2t} &= \sin\left(\frac{k}{10000^{2t/d}}\right) \\
p_{i,2t+1} &= \cos\left(\frac{k}{10000^{2t/d}}\right),
\end{aligned}
$$

...(4)

其中：

- $ p_{i,2t} $ 是 $ d $ 维向量 $ p_i $ 的第 $ 2t $ 个元素。

在下一节中，我们将展示我们提出的RoPE与这种正弦函数直觉相关。然而，RoPE不是直接将位置信息添加到上下文表示中，而是**通过与正弦函数相乘来融入相对位置信息**。

### 2.3 相对位置嵌入
Shaw等[2018]的作者对公式（1）应用了以下不同的设置：

$$
\begin{aligned}
f_q(x_m) &:= W_q x_m \\
f_k(x_n, n) &:= W_k (x_n + \widetilde{p}^k_r) \\
f_v(x_n, n) &:= W_v (x_n + \widetilde{p}^v_r),
\end{aligned}
$$

...(5)

其中：

- $ \widetilde{p}^k_r $、$ \widetilde{p}^v_r \in \mathbb{R}^d $ 是**可训练的相对位置嵌入**。注意，$ r = \text{clip}(m - n, r_{\text{min}}, r_{\text{max}}) $ 表示位置 $ m $ 和 $ n $ 之间的相对距离。

他们通过**假设超出一定距离的相对位置信息无用**，对相对距离进行了裁剪。Dai等[2019]的作者在保持公式（3）的形式下，提出将公式（2）中的 $ q_m^\intercal k_n $ 分解为：

$$
q_m^\intercal k_n = x_m^\intercal W_q^\intercal W_k x_n + x_m^\intercal W_q^\intercal W_k p_n + p_m^\intercal W_q^\intercal W_k x_n + p_m^\intercal W_q^\intercal W_k p_n,
$$

...(6)

其核心思想是：**将绝对位置嵌入 $ p_n $ 替换为其正弦编码的相对对应项 $ \widetilde{p}_{m-n} $，同时将第三和第四项中的绝对位置 $ p_m $ 替换为两个与查询位置无关的可训练向量 $ u $ 和 $ v $**。此外，$ W_k $ 被区分为基于内容的键向量 $ x_n $ 和基于位置的键向量 $ p_n $，分别表示为 $ W_k $ 和 $ \widetilde{W}_k $，从而得到：

$$
q_m^\intercal k_n = x_m^\intercal W_q^\intercal W_k x_n + x_m^\intercal W_q^\intercal \widetilde{W}_k \widetilde{p}_{m-n} + u^\intercal W_q^\intercal W_k x_n + v^\intercal W_q^\intercal \widetilde{W}_k \widetilde{p}_{m-n}.
$$

...(7)

值得注意的是，值项中的位置信息通过设置 $ f_v(x_j) := W_v x_j $ 被移除。后续工作（Raffel等[2020]、He等[2020]、Ke等[2020]、Huang等[2020]）遵循了这些设置，**仅将相对位置信息编码到注意力权重中**。然而，Raffel等[2020]的作者将公式（6）重新表述为：

$$
q_m^\intercal k_n = x_m^\intercal W_q^\intercal W_k x_n + b_{i,j},
$$

...(8)

其中:

- $ b_{i,j} $ 是一个可训练的偏置项。

Ke等[2020]的作者研究了公式（6）中的中间两项，发现绝对位置与词之间的相关性较弱。Raffel等[2020]的作者提出使用不同的投影矩阵对词或位置进行建模：

$$
q_m^\intercal k_n = x_m^\intercal W_q^\intercal W_k x_n + p_m^\intercal U_q^\intercal U_k p_n + b_{i,j}.
$$

...(9)

He等[2020]的作者认为，两个token的相对位置只能通过公式（6）中的中间两项完全建模。因此，绝对位置嵌入 $ p_m $ 和 $ p_n $ 被简单地替换为相对位置嵌入 $ \widetilde{p}_{m-n} $：

$$
q_m^\intercal k_n = x_m^\intercal W_q^\intercal W_k x_n + x_m^\intercal W_q^\intercal W_k \widetilde{p}_{m-n} + \widetilde{p}_{m-n}^\intercal W_q^\intercal W_k x_n.
$$

...(10)

对四种相对位置嵌入变体的比较（Radford和Narasimhan[2018]）表明，类似于公式（10）的变体在其他三种中效率最高。总的来说，所有这些方法都试图在自注意力设置下基于公式（3）的分解来修改公式（6），这是Vaswani等[2017]最初提出的。它们通常直接将位置信息添加到上下文表示中。与之不同，我们的方法旨在在某些约束下从公式（1）推导出相对位置编码。接下来，我们将展示通过旋转上下文表示融入相对位置信息，推导出的方法更具可解释性。

## 3 提出的方法

在本节中，我们讨论提出的**旋转位置嵌入（Rotary Position Embedding, RoPE）**。我们首先在第3.1节中形式化相对位置编码问题，然后在第3.2节中推导RoPE，并在第3.3节中研究其特性。

### 3.1 形式化
基于Transformer的语言建模通常通过自注意力机制利用各个token的位置信息。如公式（2）所示，$ q_m^\intercal k_n $ 通常用于在不同位置的token之间传递知识。为了**融入相对位置信息**，我们要求查询 $ q_m $ 和键 $ k_n $ 的内积由一个函数 $ g $ 表示，该函数仅以词嵌入 $ x_m $、$ x_n $ 及其相对位置 $ m - n $ 作为输入变量。换句话说，我们希望内积仅以相对形式编码位置信息：

$$
\langle f_q(x_m, m), f_k(x_n, n) \rangle = g(x_m, x_n, m - n).
$$

...(11)

最终目标是：找到一种等效的编码机制来解决函数 $ f_q(x_m, m) $ 和 $ f_k(x_n, n) $，以符合上述关系。

### 3.2 旋转位置嵌入

#### 3.2.1 二维情况
我们从维度 $ d = 2 $ 的简单情况开始。在这些设置下，我们利用**二维平面上向量的几何性质**及其**复数形式**来证明（详见第3.4.1节），公式（11）的一个解为：

$$
\begin{aligned}
f_q(x_m, m) &= (W_q x_m) e^{i m \theta} \\
f_k(x_n, n) &= (W_k x_n) e^{i n \theta} \\
g(x_m, x_n, m - n) &= \text{Re}\left[(W_q x_m) (W_k x_n)^* e^{i (m - n) \theta}\right],
\end{aligned}
$$

...(12)

其中：

- $\text{Re}[\cdot]$ 表示复数的实部，$(W_k x_n)^*$ 表示 $(W_k x_n)$ 的共轭复数。
- $\theta \in \mathbb{R}$ 是一个预设的非零常数。

我们可以进一步将 $ f_{\{q, k\}} $ 写成**矩阵乘法形式**：

$$
f_{\{q, k\}}(x_m, m) = \begin{pmatrix}
\cos m\theta & -\sin m\theta \\
\sin m\theta & \cos m\theta
\end{pmatrix}
\begin{pmatrix}
W_{\lbrace,q,k\rbrace}^{11} & W_{\lbrace,q,k\rbrace}^{12} \\
W_{\lbrace,q,k\rbrace}^{21} & W_{\lbrace,q,k\rbrace}^{22}
\end{pmatrix}
\begin{pmatrix}
x_m^{(1)} \\
x_m^{(2)}
\end{pmatrix},
$$

...(13)

其中：

- $(x_m^{(1)}, x_m^{(2)})$ 是 $ x_m $ 在二维坐标中的表示。
- 类似地，$ g $ 可以视为一个矩阵，从而在二维情况下解决第3.1节中的形式化问题。

具体来说，融入相对位置嵌入非常简单：只需**将仿射变换后的词嵌入向量旋转其位置索引的角度倍数**，从而解释旋转位置嵌入的直觉。

#### 3.2.2 一般形式

为了将二维结果推广到任意 $ x_i \in \mathbb{R}^d $（其中 $ d $ 为偶数），我们将 $ d $ 维空间划分为 $ d/2 $ 个子空间，并利用内积的线性性将它们组合起来，将 $ f_{\{q, k\}} $ 转化为：

$$
f_{\{q, k\}}(x_m, m) = R^d_{\Theta, m} W_{\{q, k\}} x_m,
$$

...(14)

其中:

$$
R^d_{\Theta, m} = \begin{pmatrix}
\cos m\theta_1 & -\sin m\theta_1 & 0 & 0 & \cdots & 0 & 0 \\
\sin m\theta_1 & \cos m\theta_1 & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos m\theta_2 & -\sin m\theta_2 & \cdots & 0 & 0 \\
0 & 0 & \sin m\theta_2 & \cos m\theta_2 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos m\theta_{d/2} & -\sin m\theta_{d/2} \\
0 & 0 & 0 & 0 & \cdots & \sin m\theta_{d/2} & \cos m\theta_{d/2}
\end{pmatrix}
$$

...(15)

是旋转矩阵，其预定义参数为 $\Theta = \{\theta_i = 10000^{-2(i-1)/d}, i \in [1, 2, ..., d/2]\}$。图（1）展示了RoPE的图形化说明。将RoPE应用于公式（2）中的自注意力机制，我们得到：

$$
q_m^\intercal k_n = (R^d_{\Theta, m} W_q x_m)^\intercal (R^d_{\Theta, n} W_k x_n) = x_m^\intercal W_q R^d_{\Theta, n-m} W_k x_n,
$$

...(16)

其中:

- $ R^d_{\Theta, n-m} = (R^d_{\Theta, m})^\intercal R^d_{\Theta, n} $。
- 注意，$ R^d_{\Theta} $ 是一个正交矩阵，这确保了在编码位置信息过程中的稳定性。

此外，由于 $ R^d_{\Theta} $ 的稀疏性，直接应用矩阵乘法如公式（16）所示在计算上并不高效；我们在理论解释中提供了另一种实现方式。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/8b12644274f65e85af5ad897edbaa9c25fa2412cd000081eb29f6a17956e01cfd08285fb0fafaf728b9cf4eeaa7ff49d?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 Rotary Position Embedding(RoPE)的实现

与之前工作中采用的加法性质的位置嵌入方法（即公式（3）到（10））不同，**我们的方法是乘法的**。此外，RoPE通过旋转矩阵乘积自然地融入了相对位置信息，而不是在应用于自注意力时修改加法位置编码的扩展公式中的项。

### 3.3 RoPE的特性

**长期衰减性**：遵循Vaswani等[2017]，我们设置 $\theta_i = 10000^{-2i/d}$。可以证明，这种设置提供了长期衰减特性（详见第3.4.3节），这意味着内积会随着相对位置的增加而衰减。这一特性与直觉一致，即具有较长相对距离的token对应该具有较少的连接。

**RoPE与线性注意力**：自注意力可以改写为更一般的形式：

$$
\text{Attention}(Q, K, V)_m = \frac{\sum_{n=1}^N \text{sim}(q_m, k_n) v_n}{\sum_{n=1}^N \text{sim}(q_m, k_n)},
$$

...(17)

其中原始自注意力选择 $\text{sim}(q_m, k_n) = \exp(q_m^\intercal k_n / \sqrt{d})$。**注意，原始自注意力需要计算每对token的查询和键的内积，其复杂度为 $ O(N^2) $**。遵循Katharopoulos等[2020]，线性注意力将公式（17）重新表述为：

$$
\text{Attention}(Q, K, V)_m = \frac{\sum_{n=1}^N \phi(q_m)^\intercal \varphi(k_n) v_n}{\sum_{n=1}^N \phi(q_m)^\intercal \varphi(k_n)},
$$

...(18)

其中 $\phi(\cdot)$ 和 $\varphi(\cdot)$ 通常是非负函数。Katharopoulos等[2020]的作者提出 $\phi(x) = \varphi(x) = \text{elu}(x) + 1$，并首先利用矩阵乘法的结合性计算键和值的乘积。Shen等[2021]使用softmax函数在内积之前分别对查询和键进行归一化，这等价于 $\phi(q_i) = \text{softmax}(q_i)$ 和 $\phi(k_j) = \exp(k_j)$。有关线性注意力的更多细节，我们鼓励读者参考原始论文。**在本节中，我们重点讨论将RoPE与公式（18）结合**。由于RoPE通过旋转注入位置信息，这保持了隐藏表示的范数不变，因此我们可以通过将旋转矩阵与非负函数的输出相乘来将RoPE与线性注意力结合：

$$
\text{Attention}(Q, K, V)_m = \frac{\sum_{n=1}^N \left(R^d_{\Theta, m} \phi(q_m)\right)^\intercal \left(R^d_{\Theta, n} \varphi(k_n)\right) v_n}{\sum_{n=1}^N \phi(q_m)^\intercal \varphi(k_n)}.
$$

...(19)

值得注意的是，我们保持分母不变以避免除以零的风险，而分子中的求和可能包含负项。尽管公式（19）中每个值 $ v_i $ 的权重并未严格概率归一化，但我们认为计算仍可以建模值的重要性。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/ab97e9b5a7428c7efb907a29ce5676fb3106b69a5c4c5e2e43be4a03dd5a6d7b8f57c83dd5f1e0b958d686b2af721c3f?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图 2

## 3.4 理论解释

### 3.4.1 二维情况下RoPE的推导
在 $ d = 2 $ 的情况下，我们考虑两个词嵌入向量 $ x_q $ 和 $ x_k $，分别对应于查询和键，以及它们的位置 $ m $ 和 $ n $。根据公式（1），它们的位置编码对应为：

$$
\begin{aligned}
q_m &= f_q(x_q, m), \\
k_n &= f_k(x_k, n),
\end{aligned}
$$

...(20)

其中:

- $ q_m $ 和 $ k_n $ 的下标表示编码的位置信息。

假设存在一个函数 $ g $，定义了由 $ f_{\{q, k\}} $ 生成的向量之间的内积：

$$
q_m^\intercal k_n = \langle f_q(x_m, m), f_k(x_n, n) \rangle = g(x_m, x_n, n - m),
$$

...(21)

我们进一步要求以下初始条件成立：

$$
\begin{aligned}
q &= f_q(x_q, 0), \\
k &= f_k(x_k, 0),
\end{aligned}
$$

...(22)

这可以理解为未编码位置信息的向量。在这些设定下，我们尝试找到 $ f_q $ 和 $ f_k $ 的解。首先，我们利用二维向量的几何意义及其复数形式，将公式（20）和（21）中的函数分解为：

$$
\begin{aligned}
f_q(x_q, m) &= R_q(x_q, m) e^{i \Theta_q(x_q, m)}, \\
f_k(x_k, n) &= R_k(x_k, n) e^{i \Theta_k(x_k, n)}, \\
g(x_q, x_k, n - m) &= R_g(x_q, x_k, n - m) e^{i \Theta_g(x_q, x_k, n - m)},
\end{aligned}
$$

...(23)

其中：

- $ R_f $、$ R_g $ 和 $ \Theta_f $、$ \Theta_g $ 分别是 $ f_{\{q, k\}} $ 和 $ g $ 的径向和角度分量。将它们代入公式（21），我们得到以下关系：

$$
\begin{aligned}
R_q(x_q, m) R_k(x_k, n) &= R_g(x_q, x_k, n - m), \\
\Theta_k(x_k, n) - \Theta_q(x_q, m) &= \Theta_g(x_q, x_k, n - m),
\end{aligned}
$$

...(24)

对应的初始条件为：

$$
\begin{aligned}
q &= \|q\| e^{i \theta_q} = R_q(x_q, 0) e^{i \Theta_q(x_q, 0)}, \\
k &= \|k\| e^{i \theta_k} = R_k(x_k, 0) e^{i \Theta_k(x_k, 0)},
\end{aligned}
$$

...(25)

其中：

- $ \|q\| $、$ \|k\| $ 和 $ \theta_q $、$ \theta_k $ 分别是 $ q $ 和 $ k $ 在二维平面上的径向和角度分量。

接下来，我们在公式（24）中设 $ m = n $，并考虑公式（25）中的初始条件：

$$
\begin{aligned}
R_q(x_q, m) R_k(x_k, m) &= R_g(x_q, x_k, 0) = R_q(x_q, 0) R_k(x_k, 0) = \|q\| \|k\|, \\
\Theta_k(x_k, m) - \Theta_q(x_q, m) &= \Theta_g(x_q, x_k, 0) = \Theta_k(x_k, 0) - \Theta_q(x_q, 0) = \theta_k - \theta_q.
\end{aligned}
$$

...(26)

一方面，从公式（26a）可以直接得到 $ R_f $ 的解：

$$
\begin{aligned}
R_q(x_q, m) &= R_q(x_q, 0) = \|q\|, \\
R_k(x_k, n) &= R_k(x_k, 0) = \|k\|, \\
R_g(x_q, x_k, n - m) &= R_g(x_q, x_k, 0) = \|q\| \|k\|,
\end{aligned}
$$

...(27)

这表明径向函数 $ R_q $、$ R_k $ 和 $ R_g $ 与位置信息无关。另一方面，从公式（26b）可以看出，$ \Theta_q(x_q, m) - \theta_q = \Theta_k(x_k, m) - \theta_k $ 表明角度函数不依赖于查询和键，我们设 $ \Theta_f := \Theta_q = \Theta_k $，并将 $ \Theta_f(x_{\{q, k\}}, m) - \theta_{\{q, k\}} $ 表示为位置 $ m $ 的函数，记为 $ \phi(m) $，从而得到：

$$
\Theta_f(x_{\{q, k\}}, m) = \phi(m) + \theta_{\{q, k\}}.
$$

...(28)

进一步，将 $ n = m + 1 $ 代入公式（24）并考虑上述方程，我们得到：

$$
\phi(m + 1) - \phi(m) = \Theta_g(x_q, x_k, 1) + \theta_q - \theta_k,
$$

...(29)

由于右边是一个与 $ m $ 无关的常数，$ \phi(m) $ 在连续整数输入下形成一个等差数列：

$$
\phi(m) = m \theta + \gamma,
$$

...(30)

其中 $ \theta, \gamma \in \mathbb{R} $ 是常数，且 $ \theta $ 非零。总结从公式（27）到（30）的解：

$$
\begin{aligned}
f_q(x_q, m) &= \|q\| e^{i (\theta_q + m \theta + \gamma)} = q e^{i (m \theta + \gamma)}, \\
f_k(x_k, n) &= \|k\| e^{i (\theta_k + n \theta + \gamma)} = k e^{i (n \theta + \gamma)}.
\end{aligned}
$$

...(31)

注意，我们没有对公式（22）中的 $ f_q $ 和 $ f_k $ 施加任何约束，因此 $ f_q(x_m, 0) $ 和 $ f_k(x_n, 0) $ 可以自由选择。为了使我们的结果与公式（3）可比，我们定义：

$$
\begin{aligned}
q &= f_q(x_m, 0) = W_q x_n, \\
k &= f_k(x_n, 0) = W_k x_n.
\end{aligned}
$$

...(32)

然后，我们在最终解中简单地设 $ \gamma = 0 $：

$$
\begin{aligned}
f_q(x_m, m) &= (W_q x_m) e^{i m \theta}, \\
f_k(x_n, n) &= (W_k x_n) e^{i n \theta}.
\end{aligned}
$$

...(33)

### 3.4.2 旋转矩阵乘法的计算高效实现
利用公式（15）中 $ R^d_{\Theta, m} $ 的稀疏性，$ R^d_{\Theta} $ 与 $ x \in \mathbb{R}^d $ 的乘法可以更高效地实现为：

$$
R^d_{\Theta, m} x = \begin{pmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
\vdots \\
x_{d-1} \\
x_d
\end{pmatrix} \otimes \begin{pmatrix}
\cos m \theta_1 \\
\cos m \theta_1 \\
\cos m \theta_2 \\
\cos m \theta_2 \\
\vdots \\
\cos m \theta_{d/2} \\
\cos m \theta_{d/2}
\end{pmatrix} + \begin{pmatrix}
-x_2 \\
x_1 \\
-x_4 \\
x_3 \\
\vdots \\
-x_d \\
x_{d-1}
\end{pmatrix} \otimes \begin{pmatrix}
\sin m \theta_1 \\
\sin m \theta_1 \\
\sin m \theta_2 \\
\sin m \theta_2 \\
\vdots \\
\sin m \theta_{d/2} \\
\sin m \theta_{d/2}
\end{pmatrix}.
$$

...(34)

### 3.4.3 RoPE的长期衰减性
我们可以将向量 $ q = W_q x_m $ 和 $ k = W_k x_n $ 的条目成对分组，公式（16）中的RoPE内积可以写成复数乘法的形式：

$$
(R^d_{\Theta, m} W_q x_m)^\intercal (R^d_{\Theta, n} W_k x_n) = \text{Re} \left[ \sum_{i=0}^{d/2-1} q_{[2i:2i+1]} k^*_{[2i:2i+1]} e^{i (m - n) \theta_i} \right],
$$

...(35)

其中:

- $ q_{[2i:2i+1]} $ 表示 $ q $ 的第 $ 2i $ 到 $ 2i+1 $ 个条目。

记 $ h_i = q_{[2i:2i+1]} k^*_{[2i:2i+1]}, S_j = \sum_{i=0}^{j-1} e^{i (m - n) \theta_i} $，并设 $ h_{d/2} = 0 $ 和 $ S_0 = 0 $，我们可以使用Abel变换将求和重写为：

$$
\sum_{i=0}^{d/2-1} q_{[2i:2i+1]} k^*_{[2i:2i+1]} e^{i (m - n) \theta_i} = \sum_{i=0}^{d/2-1} h_i (S_{i+1} - S_i) = -\sum_{i=0}^{d/2-1} S_{i+1} (h_{i+1} - h_i).
$$

...(36)

因此，

$$
\left| \sum_{i=0}^{d/2-1} q_{[2i:2i+1]} k^*_{[2i:2i+1]} e^{i (m - n) \theta_i} \right| = \left| \sum_{i=0}^{d/2-1} S_{i+1} (h_{i+1} - h_i) \right| \leq \sum_{i=0}^{d/2-1} |S_{i+1}| |h_{i+1} - h_i| \leq \left( \max_i |h_{i+1} - h_i| \right) \sum_{i=0}^{d/2-1} |S_{i+1}|.
$$

...(37)

注意到，通过设置 $ \theta_i = 10000^{-2i/d} $，$ \frac{1}{d/2} \sum_{i=1}^{d/2} \mid S_i \mid $ 的值会随着相对距离 $ m - n $ 的增加而衰减，如图（2）所示。

# 4.实验

略


# 

[https://arxiv.org/pdf/2104.09864](https://arxiv.org/pdf/2104.09864)