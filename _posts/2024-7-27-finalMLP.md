---
layout: post
title: finalMLP介绍
description: 
modified: 2024-7-27
tags: 
---

huawei团队在《FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction》提出了一种FinalMLP方法。

# 摘要

点击率（CTR）预测是在线广告和推荐系统中的一项基本任务。多层感知器（MLP）作为许多深度CTR预测模型的核心组件，已被广泛认识到，**单独使用一个vanilla MLP网络在学习乘法特征交叉方面是低效的**。因此，许多双流交叉模型（例如DeepFM和DCN）通过将MLP网络与另一个专用网络集成，以增强CTR预测能力。由于MLP流（MLP stream）隐式地学习特征交叉，现有研究主要集中在**增强补充流（complementary stream）中的显式特征交叉**。相比之下，我们的实证研究表明，一个经过良好调整的双流MLP模型，**简单地结合两个MLP，甚至可以实现令人惊讶的良好性能，这在现有工作中从未被报道过**。基于这一观察，我们进一步提出了**特征门控（feature gating）和交叉聚合层（interaction aggregation layers）**，这些层可以轻松地插入以构建增强的双流MLP模型，即FinalMLP。这样，它不仅支持差异化的特征输入，而且有效地融合了两个流之间的流级交互(stream-level interactions)。我们在四个开放基准数据集上的评估结果以及我们工业系统中的在线A/B测试表明，FinalMLP比许多复杂的双流CTR模型具有更好的性能。我们的源代码将在 https://reczoo.github.io/FinalMLP 上提供。

# 1.引言

点击率（CTR）预测是在线广告和推荐系统（Cheng et al. 2016; He et al. 2014）中的一项基本任务。CTR预测的准确性不仅直接影响用户参与度，而且显著影响商业提供商的收入。CTR预测的一个关键挑战在于学习特征之间的复杂关系，以便模型在罕见特征交叉的情况下仍然能够很好地泛化。多层感知器（MLP），作为深度学习中强大且多功能的组件，已经成为各种CTR预测模型（Zhu et al. 2021）的核心构建块。**尽管理论上MLP被认为是通用逼近器，但实践中广泛认识到应用vanilla MLP网络学习乘法特征交叉（例如点积）是低效的（Wang et al. 2017, 2021; Rendle et al. 2020）**。

为了增强学习显式特征交叉（二阶或三阶特征）的能力，提出了多种特征交叉网络。典型的例子包括因子分解机（FM）（Rendle 2010）、交叉网络（Wang et al. 2017）、压缩交互网络（CIN）（Lian et al. 2018）、基于自注意力的交互（Song et al. 2019）、自适应因子分解网络（AFN）（Cheng, Shen, and Huang 2020）等。这些网络引入了学习特征交叉的归纳偏差，但根据我们在表3中的实验，它们失去了MLP的表达能力。因此，双流CTR预测模型已被广泛采用，如DeepFM（Guo et al. 2017）、DCN（Wang et al. 2017）、xDeepFM（Lian et al. 2018）和AutoInt+（Song et al. 2019），它们将MLP网络和专用特征交叉网络集成在一起，以增强CTR预测。具体来说，**MLP流隐式地学习特征交叉，而另一流则以补充方式增强显式特征交叉**。由于它们的有效性，双流模型已成为工业部署（Zhang et al. 2021）的流行选择。

尽管许多现有研究已经验证了双流模型相对于单个MLP模型的有效性，但没有一项研究报告了**与简单并行组合两个MLP网络的双流MLP模型（称为DualMLP）**的性能比较。因此，我们的工作首次努力描述DualMLP的性能。我们在开放基准数据集上的实证研究表明，尽管DualMLP简单，但可以实现令人惊讶的良好性能，这与许多精心设计的双流模型相当甚至更好（见我们的实验）。这一观察激发了我们研究这样一个双流MLP模型的潜力，并进一步将其扩展为CTR预测的简单而强大的模型。

事实上，**双流模型可以被视为两个并行网络的集成**。这些双流模型的一个优势是每个流可以从不同的角度学习特征交叉，从而互补以实现更好的性能。例如：

- Wide&Deep（Cheng et al. 2016）和DeepFM（Guo et al. 2017）提出使用一个流来捕获低阶特征交叉，另一个流来学习高阶特征交叉。
- DCN（Wang et al. 2017）和AutoInt+（Song et al. 2019）主张在两个流中分别学习显式特征交叉和隐式特征交叉。
- xDeepFM（Lian et al. 2018）进一步从向量级和位级角度增强特征交叉学习。这些先前的结果验证了两个网络流的差异化（或多样性）对双流模型的有效性有重大影响。

与现有的依靠设计不同网络结构（例如，CrossNet（Wang et al. 2017）和CIN（Lian et al. 2018））以实现流差异化的双流模型相比，**DualMLP的局限性在于两个流都是简单的MLP网络**。我们的初步实验还表明，通过为两个MLP调整不同的网络大小（即，层数或单元数），DualMLP可以实现更好的性能。这一结果促使我们进一步探索如何扩大两个流的差异化，以改善作为基础模型的DualMLP。此外，**现有的双流模型通常通过求和或连接的方式结合两个流，这可能浪费了对高级（即，流级）特征交叉进行建模的机会**。如何更好地融合流输出成为另一个值得进一步探索的研究问题。

为了解决这些问题，在本文中，我们构建了一个增强的双流MLP模型，即FinalMLP，它在两个MLP模块网络上集成了特征门控和交叉聚合层。更具体地说，我们提出了一个**流特定的特征门控层（stream-specific feature gating layer）**，允许获得基于门控的特征重要性权重进行软特征选择（soft feature selection）。也就是说，特征门控可以通过基于可学习参数、用户特征或item特征的条件来从不同视角计算，分别产生全局、user-specific或item-specific的特征重要性权重。通过灵活选择不同的门控条件特征，我们能够为每个流派生出流特定的特征，从而增强两个流的互补特征交叉学习的差异化特征输入。为了融合流输出与流级特征交叉，我们提出了一个基于二阶双线性融合（Lin, RoyChowdhury, 和 Maji 2015; Li et al. 2017）的交叉聚合层。为了降低计算复杂性，我们进一步将计算分解为k个子组，从而实现高效的**多头双线性融合**。特征门控和交叉聚合层都可以轻松地插入到现有的双流模型中。


我们的实验结果在四个开放基准数据集上显示，FinalMLP优于现有的双流模型，并达到了新的最先进性能。此外，我们通过离线评估和在线A/B测试在工业环境中验证了其有效性，其中FinalMLP也显示出比部署的基线显著的性能提升。我们设想，简单而有效的FinalMLP模型可以作为未来双流CTR模型发展的新强基线。本文的主要贡献总结如下：
- 据我们所知，这是第一个实证研究双流MLP模型惊人有效性的工作，这可能与文献中的普遍观点相反。
- 我们提出了FinalMLP，这是一个具有可插拔特征门控和交叉聚合层的增强型双流MLP模型。
- 我们在基准数据集上进行了离线实验，并在生产系统中进行了在线A/B测试，以验证FinalMLP的有效性。

# 2.背景和相关工作

在这一部分，我们简要回顾了CTR预测的框架和代表性的双流模型。

## 2.1 双流CTR模型的框架
我们在图1(b)中展示了双流CTR模型的框架，它包括以下关键组件。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/3b5c76f328f36561b3448cf32e8c0b76e8ba626e646762e668369e78f2097cb25f36d364247ef78700e0ab75b86f2495?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 (a) 流特定特征选择的示意图。 (b) 双流CTR模型的通用框架。 (c) 多头双线性融合。


### 2.1.1 特征嵌入

嵌入是将高维和稀疏的原始特征映射到密集数值表示的常用方法。具体来说，假设原始输入特征是$x = {x_1, \cdots, x_M}$，有M个特征字段，其中：$x_i$是第i个字段的特征。一般来说，$x_i$可以是分类的、多值的或数值特征。每个都可以相应地转换为嵌入向量。感兴趣的读者可以参考（Zhu et al. 2021）以获取有关特征嵌入方法的更多细节。然后，这些特征嵌入将被连接并输入到下一层。

### 2.1.2 特征选择

特征选择是双流CTR模型框架中的一个**可选层**。在实践中，特征选择通常通过离线统计分析或通过差异比较的模型训练来执行（Pechuan, Ponce, 和 de Lourdes Mart ´ ´ınez-Villasenor ˜ 2016）。与硬特征选择不同，这项工作中，我们专注于通过**特征门控机制进行软特征选择**（Huang, Zhang, 和 Zhang 2019; Guan et al. 2021），其目的是**获得特征重要性权重，以帮助放大重要特征，同时抑制噪声特征**。在这项工作中，我们研究了流特定的特征门控，以实现差异化的流输入。

### 2.1.3 双流特征交叉

双流CTR模型的关键特点是：**采用两个并行网络从不同视角学习特征交叉**。基本上，每个流可以采用任何类型的特征交叉网络（例如，FM（Rendle 2010）、CrossNet（Wang et al. 2017）和MLP）。现有工作通常**将两种不同的网络结构应用于两个流，以学习互补的特征交叉**（例如，显式与隐式，位级与向量级）。在这项工作中，我们首次尝试使用两个MLP网络作为两个流。

### 2.1.4 流级融合

流级融合是必要的，以融合两个流的输出以获得最终预测的点击概率$\widehat{y}$。假设$o_1$和$o_2$作为两个输出表示，可以表示为：

$$
\widehat{y} = σ(w^T F(o_1, o_2))
$$

其中：

- F：表示融合操作，通常设置为求和或连接（concatenation）。
- w：表示在必要时将输出维度映射到1的线性函数。
- σ：是sigmoid函数。

现有工作只对流输出进行一阶线性组合，因此无法挖掘流级特征交叉。在这项工作中，我们探索了**二阶双线性函数**用于流级交叉聚合。

### 2.1.5 代表性的双流CTR模型

我们总结了一些代表性的双流模型，这些模型涵盖了CTR预测研究的广泛领域：

- Wide&Deep：Wide&Deep（Cheng et al. 2016）是一个经典的双流特征交叉学习框架，结合了广义线性模型（宽流）和MLP网络（深流）。
- DeepFM：DeepFM（Guo et al. 2017）通过将wide流替换为FM来扩展Wide&Deep，以显式学习二阶特征交叉。
- DCN：在DCN（Wang et al. 2017）中，提出了一个交叉网络作为一条流，以显式方式执行高阶特征交叉，而另一条MLP流隐式学习特征交叉。
- xDeepFM：xDeepFM（Lian et al. 2018）采用压缩交互网络（CIN）以向量级方式捕获高阶特征交叉，并也采用MLP作为另一条流来学习位级特征交叉。
- AutoInt+：AutoInt（Song et al. 2019）应用自注意力网络来学习高阶特征交叉。AutoInt+将AutoInt和MLP作为两个互补的流集成。
- AFN+：AFN（Cheng, Shen, 和 Huang 2020）利用对数变换层来学习自适应阶数特征交叉。AFN+以双流方式将AFN与MLP结合。
- DeepIM：在DeepIM（Yu et al. 2020）中，提出了一个交互机（IM）模块，以高效计算高阶特征交叉。它在两个流中分别使用IM和MLP。
- MaskNet：在MaskNet（Wang, She, 和 Zhang 2021）中，提出了一个MaskBlock，通过结合层归一化、实例引导掩码和前馈层。并行的MaskNet是一个双流模型，它并行使用两个MaskBlocks。
- DCN-V2：DCN-V2（Wang et al. 2021）通过更具表现力的交叉网络改进DCN，以更好地捕获显式特征交叉。它在并行版本中仍然使用MLP作为另一条流。
- EDCN：EDCN（Chen et al. 2021）不是一个严格的双流模型，因为它提出了一个桥接模块和一个调节模块来桥接两个流的隐藏层之间的信息融合。然而，其操作限制了每个流具有相同大小的隐藏层和单元，降低了灵活性。

# 3.我们的模型

在这一部分，我们首先介绍简单的双流MLP基础模型，DualMLP。然后，我们描述两个可插拔模块，特征门控和交叉聚合层，这些模块构成了我们增强的模型，FinalMLP。

## 3.1 双流MLP模型

尽管简单，据我们所知，双流MLP模型以前没有被先前的工作报道过。因此，我们首次尝试研究这样一个模型用于CTR预测，称为DualMLP，它简单地将**两个独立的MLP网络作为两个流组合起来**。
具体来说，双流MLP模型可以表述如下：

$$
o_1 = MLP_1(h_1), \\
o_2 = MLP_2(h_2),
$$

其中：

- $MLP_1$和$MLP_2$：是两个MLP网络。两个MLP的大小（关于隐藏层和单元）可以根据数据进行不同的设置。
- $h_1$和$h_2$：表示特征输入
- $o_1$和$o_2$：分别是两个流的输出表示

在大多数先前的工作（Wang et al. 2017; Guo et al. 2017; Cheng, Shen, 和 Huang 2020）中，**特征输入$h_1$和$h_2$通常设置为相同的**，即特征嵌入e的连接（可选地与一些池化操作一起），即，

$$
h_1 = h_2 = e
$$

同时，流输出通常通过简单的操作（如求和和连接）进行融合，**忽略了流级交互**。下面，我们将介绍两个**可以分别插入到输入和输出中的模块**，以增强双流MLP模型。

## 3.2 流特定的特征选择

许多现有的研究（Guo et al. 2017; Lian et al. 2018; Wang et al. 2017; Song et al. 2019）强调了**结合两种不同特征交叉网络**（例如，隐式与显式，低阶与高阶，位级与向量级）的有效性，以实现准确的CTR预测。我们的目标不是设计专门的网络结构，而是**通过流特定的特征选择来扩大两个流之间的差异，从而产生差异化的特征输入**。

受MMOE（Ma et al. 2018）中使用的门控机制的启发，我们提出了一个流特定的特征门控模块来**软选择流特定的特征，即，为每个流不同地重新加权特征输入**。在MMOE中，门控权重是根据特定任务的特征来重新加权专家输出的。同样，我们通过基于可学习参数、用户特征或item特征的条件来从不同视角进行特征门控，分别产生全局、用户特定或项目特定的特征重要性权重。

具体来说，我们通过上下文感知的特征门控层进行流特定的特征选择，如下所示。

$$
g_1 = Gate_1(x_1), \quad g_2 = Gate_2(x_2), \\
h_1 = \sigma(g_1) \odot e, \quad h_2 = \sigma(g_2) \odot e, 
$$

其中：

- $Gate_i$：表示基于MLP的门控网络
- $x_i$：流特定的条件特征输入
- $g_i$：逐元素门控权重输出

注意，从user/item特征集或将其设置为可学习参数中选择$x_i$是灵活的。**通过使用sigmoid函数σ和一个乘数2将它们转换为[0, 2]范围内的值，平均值为1，我们可以获得特征重要性权重**。给定连接的特征嵌入e，我们可以通过逐元素乘积⊙获得加权特征输出$h_1$和$h_2$。

我们的特征门控模块允许通过**从不同视角设置条件特征$x_i$来为两个流制作差异化的特征输入**。例如，图1(a)展示了一个用户和项目特定特征门控的案例，它分别**从用户和item的视角调节每个流**。这减少了两个相似MLP流之间的“同质”学习，并能够实现更互补的特征交叉学习。

## 3.3 流级交互聚合

### 3.3.1 双线性融合

如前所述，现有工作大多采用求和或连接作为融合层，但这些操作未能捕捉流级特征交叉。受计算机视觉领域广泛研究的双线性池化启发（Lin, RoyChowdhury, 和 Maji 2015; Li et al. 2017），我们提出了一个双线性交叉聚合层来融合流输出和流级特征交叉。如图1(c)所示，预测的点击概率公式如下。

$$ 
\widehat{y} = \sigma(b + w_1^T o_1 + w_2^T o_2 + o_1^T W_3 o_2), 
$$

其中:

-  $ b \in \mathbb{R}, w_1 \in \mathbb{R}^{d_1 \times 1}, w_2 \in \mathbb{R}^{d_2 \times 1}, W_3 \in \mathbb{R}^{d_1 \times d_2} $ 是可学习的权重。
- $ d_1 $ 和 $ d_2 $ 分别表示 $ o_1 $ 和 $ o_2 $ 的维度。
- 双线性项 $ o_1^T W_3 o_2 $ 模拟了 $ o_1 $ 和 $ o_2 $ 之间的二阶交互。特别是，当 $ W_3 $ 是一个单位矩阵时，该项模拟了点积。当将 $ W_3 $ 设置为零矩阵时，它退化为传统的连接融合与线性层，即 $ b + [w_1, w_2]^T [o_1, o_2] $。

有趣的是，双线性融合与常用的FM模型也有联系。具体来说，FM通过以下方式为CTR预测对一个m维输入特征向量x（通过一位/多位特征编码和连接）进行二阶特征交叉建模：

$$
\hat{y} = \sigma(b + w^Tx + x^T_{\text{upper}} P P^T)x
$$

其中：

- $ b \in \mathbb{R}, w \in \mathbb{R}^{m \times 1}, P \in \mathbb{R}^{m \times d} $ 是可学习的权重，其中 $ d \ll m $，upper选择矩阵的严格上三角部分（Rendle 2010）。

正如我们所看到的，当 $ o_1 = o_2 $ 时，FM是双线性融合的一种特殊形式。

然而，当 $ o_1 $ 和 $ o_2 $ 是高维时，计算方程（5）的参数密集且计算成本高。例如，要融合两个1000维的输出，$ W_3 \in \mathbb{R}^{1000 \times 1000} $ 占用了100万个参数，其计算变得昂贵。为了降低计算复杂性，我们在下文中引入了我们的扩展多头双线性融合。

### 3.3.2 多头双线性融合

多头注意力因其能够从不同的表示子空间结合相同的注意力池化知识而受到欢迎。它在最近的成功变换器模型中带来了计算的减少和一致的性能提升（Vaswani et al. 2017）。受其成功的启发，我们将双线性融合扩展到多头版本。具体来说，我们不是直接计算方程（5）中的双线性项，而是将 $ o_1 $ 和 $ o_2 $ 分别分割成k个子空间：

$$
o_1 = [o_{11}, \cdots, o_{1k}], \\
o_2 = [o_{21}, ..., o_{2k}],
$$

其中：

- k 是一个超参数
- $ o_{ij} $ 表示第 i 个输出向量的第 j 个子空间表示（i ∈ {1, 2}）。

类似于多头注意力，我们在每个子空间中执行双线性融合，将 $ o_{1j} $ 和 $ o_{2j} $ 配对为一组。然后，我们通过求和池化聚合子空间计算，得到最终预测的点击概率：

$$
\hat{y} = \sigma(\sum_{j=1}^{k} BF(o_{1j}, o_{2j})),
$$

其中：

- BF 表示方程（5）中不带 sigmoid 激活的双线性融合。

通过与多头注意力相同的子空间计算，我们理论上可以通过 k 的因子减少双线性融合的参数数量和计算复杂性，即从 $ O(d_1d_2) $ 减少到 $ O(\frac{d_1d_2}{k}) $。特别是，当设置 $ k = d_1 = d_2 $ 时，它退化为逐元素乘积融合。如果 $k = 1$，则等于原始的双线性融合。
选择合适的k实现多头学习，以便模型可能获得更好的性能。在实践中，k 个子空间的多头融合在 GPU 中并行计算，这进一步提高了效率。

最后，我们的流特定特征门控和流级交互聚合模块可以插入，以产生增强的双流 MLP 模型，FinalMLP。

### 3.3.3 模型训练

为了训练 FinalMLP，我们应用广泛使用的二元交叉熵损失：

$$
L = -\frac{1}{N} \sum_{y} y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) 
$$

其中：

- y 和 $ \hat{y} $ 分别表示来自总共 N 个样本中的每个样本的真实标签和估计的点击概率。


# 4.实验

## 4.1 实验设置

**数据集**

我们使用四个开放基准数据集进行实验，包括 Criteo、Avazu、MovieLens 和 Frappe。我们重用（Cheng, Shen, 和 Huang 2020）预处理的数据，并遵循相同的数据分割和预处理设置。表1总结了数据集的统计信息。

**评估指标**

我们采用AUC作为CTR预测中最广泛使用的评估指标之一。此外，AUC提高0.1个百分点被认为是CTR预测中的显著改进（Cheng et al. 2016; Cheng, Shen, 和 Huang 2020; Wang et al. 2021）。

**基线**

首先，我们研究以下单一流显式特征交叉网络。

- 一阶：逻辑回归（LR）（Richardson, Dominowska, 和 Ragno 2007）。
- 二阶：FM（Rendle 2010）、AFM（Xiao et al. 2017）、FFM（Juan et al. 2016）、FwFM（Pan et al. 2018）和FmFM（Sun et al. 2021）。
- 三阶：HOFM (3rd)（Blondel et al. 2016）、CrossNet (2L)（Wang et al. 2017）、CrossNetV2 (2L)（Wang et al. 2021）和CIN (2L)（Lian et al. 2018）。我们特别将最大阶数设置为“3rd”或将交互层数设置为“2L”，以获得三阶特征交叉。
- 更高阶：CrossNet（Wang et al. 2017）、CrossNetV2（Wang et al. 2021）、CIN（Lian et al. 2018）、AutoInt（Song et al. 2019）、FiGNN（Li et al. 2019）、AFN（Cheng, Shen, 和 Huang 2020）和SAM（Cheng 和 Xue 2021），这些模型自动学习高阶特征交叉。

然后，我们研究相关工作部分介绍的一系列代表性双流CTR模型。

## 4.2 实现

我们重用基线模型，并基于FuxiCTR（Zhu et al. 2021），一个开源的CTR预测库，实现我们的模型。我们的评估遵循与AFN（Cheng, Shen, 和 Huang 2020）相同的实验设置，将嵌入维度设置为10，批量大小设置为4096，默认MLP大小设置为[400, 400, 400]。对于DualMLP和FinalMLP，我们将两个MLP调整为1到3层，以增强流多样性。我们将学习率设置为1e-3或5e-4。我们通过广泛的网格搜索（平均每模型约30次运行）调整所有研究模型的所有其他超参数（例如，嵌入正则化和dropout率）。我们注意到，通过优化的FuxiCTR实现和充分的超参数调整，我们获得了比（Cheng, Shen, 和 Huang 2020）报告的更好的模型性能。因此，我们报告自己的实验结果，而不是重用他们的数据，以便进行公平比较。为了促进可重复的研究，我们开源了FinalMLP和所有使用的基线代码和运行日志。

**MLP与显式特征交叉**

尽管特征交叉网络已被广泛研究，但MLP与精心设计的特征交叉网络之间缺乏比较。以前的工作提出了许多显式特征交叉网络，例如交叉网络（Wang et al. 2017）、CIN（Lian et al. 2018）、AutoInt（Song et al. 2019）和AFN（Cheng, Shen, 和 Huang 2020），以克服MLP在学习高阶特征交叉方面的局限性。然而，这些研究中的大多数未能直接将显式特征交叉网络与单独的MLP（即DNN或YouTubeDNN（Covington, Adams, 和 Sargin 2016））进行比较，而只是评估了双流模型变体（例如DCN、xDeepFM和AutoInt+）与MLP的有效性。在这项工作中，我们在表3中进行了这样的比较。我们列举了用于一阶、二阶、三阶和更高阶特征交叉的代表性方法。令人惊讶的是，我们观察到MLP可以与精心设计的特征交叉网络并驾齐驱甚至表现更好。例如，MLP在Criteo、MovieLens和Frappe上实现了最佳性能，而在Avazu上获得了次佳性能，与SAM相比AUC差距仅为0.02个百分点。这一观察结果也与（Wang et al. 2021）中报告的结果一致，其中显示经过良好调整的MLP模型（即DNN）能够获得与许多现有模型相当的表现。

总的来说，MLP所取得的强劲性能表明，尽管其结构简单，在学习乘性特征方面的弱点，MLP在隐式学习特征交叉方面非常具有表现力。这也部分解释了为什么现有研究倾向于将显式特征交叉网络与MLP结合作为双流模型用于CTR预测。不幸的是，其实力从未在任何现有工作中明确揭示过。受到上述观察的启发，我们进一步研究了一个未被探索的模型结构的潜力，该结构简单地采用两个MLP作为双流MLP模型。

**DualMLP和FinalMLP与双流基线的比较**

按照现有研究，我们对表2中所示的代表性双流模型进行了彻底比较。从结果中，我们有以下观察：
首先，我们可以看到双流模型通常优于表3中报告的单一流基线，特别是单个MLP模型。这符合现有工作，揭示了双流模型可以学习互补特征，从而实现更好的CTR预测建模。
其次，简单的双流模型DualMLP表现出乎意料地好。通过对两个流的MLP层进行精心调整，DualMLP可以实现与其他复杂双流基线相当甚至更好的性能。据我们所知，DualMLP的强劲性能在文献中从未被报道过。在我们的实验中，我们发现通过在两个流中设置不同的MLP大小来增加流网络多样性可以提高DualMLP的性能。

这激发了我们进一步开发一个增强的双流MLP模型FinalMLP。

第三，通过我们在特征门控和融合方面的可插拔扩展，FinalMLP在四个开放数据集上一致地优于DualMLP以及所有其他比较的双流基线。特别是，FinalMLP在Avazu、MovieLens和Frappe上的AUC分别显著超过了现有的最强双流模型0.12个百分点（DCNv2）、0.23个百分点（xDeepFM）和0.11个百分点（AutoInt+）。这证明了我们的FinalMLP的有效性。截至撰写本文时，FinalMLP在PapersWithCode4和BARS5（Zhu et al. 2022）的CTR预测排行榜上均名列第一。

**消融研究**

在这一部分，消融研究显示了对FinalMLP重要设计的调查。

特征选择和双线性融合的效果

具体来说，我们将FinalMLP与以下变体进行比较：

- DualMLP：简单的双流MLP模型，简单地将两个MLP作为两个流。
- w/o FS：没有通过上下文感知特征门控进行流特定特征选择模块的FinalMLP。
- Sum：在FinalMLP中使用求和融合。
- Concat：在FinalMLP中使用连接融合。
- EWP：在FinalMLP中使用逐元素乘积（即哈达玛积）融合。

消融研究结果在图2中呈现。我们可以观察到，当移除特征选择模块或将双线性融合替换为其他常用的融合操作时，性能会下降。这验证了我们的特征选择和双线性融合模块的有效性。此外，我们注意到双线性融合比特征选择扮演了更重要的角色，因为替换前者会导致更多的性能下降。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/7a7cf857307954216f2c4ddc68cd9c5b20ec58c2aa0e4f6cca72f809e177f5698c6dcfd605c320e4ebcd03c452feab57?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2

多头双线性融合的效果：我们研究了我们的子空间分组技术对双线性融合的影响。表4显示了通过变化双线性融合的子空间数量（即头数k）来改变FinalMLP的性能。OOM表示在设置中发生了内存溢出错误。我们发现，使用更多的参数（即更小的k）进行融合并不总是导致更好的性能。这是因为适当的k可以帮助模型从多个视角学习流级别的特征交互，同时减少冗余交互，类似于多头注意力。通过在实践中调整k，可以在有效性和效率之间取得良好的平衡。

工业评估：我们进一步在为新闻推荐服务的生产系统中评估FinalMLP，该系统每天为数百万用户提供服务。我们首先使用3天用户点击日志的训练数据（包含12亿个样本）进行离线评估。表5显示了AUC结果。与在线部署的深度BaseModel相比，FinalMLP在AUC上提高了超过一个百分点。我们还与EDCN（Chen等人，2021年）进行了比较，EDCN是最近的一项工作，它通过两流网络之间的交互增强了DCN（Wang等人，2017年）。FinalMLP在AUC上比EDCN额外提高了0.44个百分点。此外，我们测试了从接收用户请求到返回预测结果的端到端推理延迟。我们可以看到，通过应用我们的多头双线性融合，延迟可以从使用1个头的70毫秒减少到使用8个头的47毫秒，实现了与在线部署的BaseModel（45毫秒）相同的延迟水平。此外，通过选择适当数量的头，AUC结果也略有提高。

我们最后报告了在7月18日至22日进行的在线A/B测试的结果，结果如表6所示。FinalMLP平均在CTR上实现了1.6%的改进，CTR衡量的是用户点击次数与新闻总展示次数的比率。在我们的生产系统中，这样的改进是显著的。

# 

[https://arxiv.org/pdf/2304.00902](https://arxiv.org/pdf/2304.00902)