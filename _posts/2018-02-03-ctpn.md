---
layout: post
title: CTPN介绍
description: 
modified: 2018-02-03
tags: [PNN]
---

CTPN(Connectionist Text Proposal Network)由Zhi Tian等人提出。

# 总览

CTPN可以在卷积特征图（convolutional feature maps）中直接检测在精密尺度（fine-scale）的text proposals序列中的文本行（text line）。它开发了一个**垂直锚点机制（vertical anchor mechanism）**，可以联合预测关于每个固定宽度proposal的位置（location）和文本/非文本分值（text/none-text score）。序列化的proposals被连接到一个RNN上，无缝地与卷积网络相接，产生一个end-to-end的训练模型。这使得CTPN可以探索丰富的图像文本信息，可以检测相当模糊的文本。CTPN可以在多尺度（multi-scale）和多语言文本（multi-language text）环境下可靠工作，无需进一步后序处理（post-processing）：这与自底向上的方法不同（它们通常需要多步后置过滤（post filtering））。在ICDAR 2013和2015 becnmarks上分别取得0.88和0.61的F-measure值，比最近的较好结果有很大的提升。CTPN的计算效率是0.14s/image，它使用了very deep VGG16 model.

# 一、介绍

在自然图片中读取文本最近在CV界获得广泛关注。这是由于许多实际应用，比如：图片OCR，多语言翻译，图片检索，等等。它包含了两个子任务：文本检测、文本识别。CTPN主要工作集中在文本检测任务上，它比识别更具挑战。文本模式的大量变种，以及高度混杂的背景对于精准文本定位构成巨大挑战。

当前文本检测的主要方法大多数使用了自底向上的pipeline。它们通常从低级字符(low-level character)或笔画（stroke）的检测开始，通常按照一定数量的子阶段：非文本组件过滤（non-text component
filtering）、文本行构造(text line construction)和文本行验证（text line verification）。这种多步的自底向上的方法很复杂，并且健壮性和可靠性差。它们的性能很严重地依赖字符检测的结果、以及连接组件的方法、或者滑动窗口的方法。这些方法通常会探索低级特征（基于SWT，MSER，或者HoG）来将候选文本从背景中区分出来。然而，它们并不健壮，需要通过独立标识私有的笔画或字符，没有文本信息。例如，人们标识一个字符序列比标识单个字符更自信，特别是当一个字符很模糊时。这种限制通常会在字符检测上产生大量非文本组件，从而在下一步中处理它们造成主要难题。再者，这些错误的检测很容易在自底向上的pipeline上按顺序累积。为了解决这些难题，CTPN采用强的deep features来在卷积图中直接检测文本信息。另外还开发了文本锚点机制，可以精准预测在精准尺度上的文本位置。接着，提出了一个in-network recurrent架构来将这些fine-scale text proposals按顺序相连接，并将它们编码成富文本信息。

近几年，CNN在通用目标检测上有优势。state-of-art的方法是Faster Region-CNN（R-CNN）系统，其中RPN（ Region Proposal Network）可以直接从卷积特征图中生成高质量的未知类别目标proposals。接着RPN proposals可以被feed到一个Fast R-CNN模型上以进一步分类（classification）和提炼（refinement），从而在通用目标检测上产生state-of-art的效果。然而，**很难将这些通用目标检测系统直接应用到文本场景检测中，这种情况下通常需要一个更高的位置准确率**。在通用目标检测中，每个object都具有一个定义良好的边界，而在文本中也存在这样的边界，因为一个文本行或词由一定数目的独立字符或笔画构成。对于目标检测，一个典型的正确检测的定义是松散的，比如：在检测表面的边界框和ground truth重叠（overlap）> 0.5(PASCAL standard），因为人们可以很容易地从主要部件上识别一个object。相反地，**读取完整文本是一个细粒度的识别任务，它的一个正确检测必须覆盖文本行或词的完整区域**。因此，文本检测通常需要一个更精准的定位，来产生一个不同的评估标准，比如：text benchmarks中常用的Wolf's standard。

在CTPN中，会通过扩展RPN结构来进行精准文本行定位。并提出了许多技术开发手段来将generic object detection模型优雅地移植来解决文本上的难题。另外进一步提出了一个in-network recurrent机制，可以在卷积图中直接检测文本序列，避免通过一个额外的CNN检测模型来进行后置处理。

## 1.1 

CTPN的主要架构见图1.


<img src="http://pic.yupoo.com/wangdren23/HArkh8L7/medish.jpg">

图1: (a) CTPN的结构. 我们通过VGG16模型的最后一层的卷积图(conv5)紧密地滑动一个3x3的空间窗口。在每行中的序列窗口通过一个Bi-LSTM进行递归连接，其中每个窗口的卷积特征(3x3xC)被用于BLSTM的256D输入（包含两个128D的LSTMs）。RNN layer被连接到一个512D的FC-layer上，后面跟着output layer，它会联合预测text/non-text scores，y坐标以及k个锚点的side-refinement offsets。 (b) CTPN输出预列化的固定宽度的fine-scale text proposals。每个box的颜色表示了text/non-text score。只有正分值的boxes才会被展示。

# 2.相关工作

- 文本检测：
- 目标检测：

略，详见paper

# 3.Connectionist Text Proposal Network

CTPN包括了三个主要部分：

- 在fine-scale proposals中检测文本
- 对text proposals进行recurrent连接（recurrent connectionist text proposals）
- 边缘细化（side-refinement）

## 3.1 Detecting Text in Fine-scale Proposals

与RPN相类似，CTPN本质上是一个完全卷积网络（fully convolutional network）：它允许一个任意size的输入图片。它会通过密集地在卷积特征图(convolutional feature maps)上滑动一个小窗口，并输出一串fine-scale（例如：16-pixel的宽度）的text proposals，如图1（b）所示。

这里采用了一个非常深的16-layer vggNet (VGG16)作为示例来描述 CTPN，它很容易应用于其它的deep模型。CTPN的结构如图1(a)所示。我们使用了一个小的空间窗口（spatial window），3x3，来滑动最后的卷积层中的feature maps（例如：VGG16中的conv5）。conv5的feature maps的size由输入图片的size决定，其中总的stride和receptive field由网络结构来确定。在卷积层中使用一个共享卷积计算的滑动窗口，可以减少基于该方法的计算量。

总之，滑动窗口法采用了多尺度窗口（multi-scale windows）来检测不同size的目标，其中一个固定size的window scale对应于相同size的目标。在faster R-CNN中，提出了一个高效的锚点回归机制，它允许RPN来使用单尺度窗口来检测多尺度目标。单尺度窗口的核心是：通过使用一定数量的锚点（anchors），能预测在一个宽范围尺度和尺度比例（aspect ratios）上的目标。我们希望将这种有效的锚点机制扩展到文本任务上。然而，文本与通用目标十分不同，它必须有一个定义良好的闭合边界和中心，可以从一部分来推测整个目标。它可能包含了多级别的组件：比如笔画，字符，词，文本行和文本区域，它们相互间很容易区分。文本检测是在词或文本行级别定义的，因而，通过将它定义成单个目标，它可以很容易地做出不正确的检测，例如：检测一个词的某部分。因此，直接预测一个文本行或词的位置很难或者不可靠，使得它很难达到一个满意的准确率。图2展示了一个示例，其中RPN直接训练来定位图片中的文本行。

<img src="http://pic.yupoo.com/wangdren23/HAsLZt8Q/medish.jpg">
 
我们寻找文本的唯一特性是，能够很好地泛化成在所有级别上的文本组件。我们观察到，RPN的词检测（word detection）很难精准预测词的水平边缘（horizontal sides），因为一个词中的每个字符是孤立或者分离的，这使得发现一个词的起点和终点容易混淆。很明显，一个文本行是一个序列，它是文本与通用目标之间的主要区别。很自然地将一个文本行考虑成一个fine-scale text proposals的序列，其中每个proposal通常表示成一个文本行的一小部分，例如，一个16-pixel宽的文本片段（text piece）。每个proposal可以包含单个或多个笔画，一个字符的一部分，单个或多个字符，等。我们相信，通过将它的水平位置固定（很难进行预测），可以更精准地预测每个proposal的垂直位置。对比RPN（它只要预测一个目标的4个坐标），这减少了搜索空间。我们开发了一个垂直锚点机制，它可以同时预测一个文本／非文本分值，以及每个fine-scale proposal的y轴位置。对比识别一个独立的字符（很容易混淆），检测一个通用固定宽度的text proposal更可靠。再者，检测在固定宽度的text proposals序列中的一个文本行，可以在多尺度和多尺度比例下可靠运行。

最后，我们按以下方式设计了fine-scale text proposal。我们的检测器(detector)会密集地（densely）检测在conv5中的每个空间位置（spatial location）。**一个text proposal被定义成：具有一个16 pixels的固定宽度（在输入图片上）**。这等同于将该detector密集地通过conv5 maps，其中，总的stride是完整的16 pixels。**接着，我们设计了k个垂直锚点来为每个proposal预测y坐标**。k个锚点具有相同的水平位置，它们都有16 pixels的宽度，但它们的水平位置以k个不同的高度进行区分。在我们的实验中，我们为每个proposal使用了10个锚点，k=10, 它们的高度从11到273个pixels不等（每次除以0.7）。显式的水平坐标通过高度和一个proposal边界框的y轴中心来进行衡量。我们根据每个锚点的边界位置，各自计算了相对预测的水平坐标（v）：

$$
v_c = (c_y - c_y^a)/h^a, v_h = log(h/h^a)
$$ ...(1)

$$
v_c^* = (c_y^* - c_y^a)/h^a, v_h^* = log(h^*/h^a)
$$ ...(2)

其中，

- $$v={v_c, v_h} $$和 $$v^* = {v_c^*, v_h^*}$$分别是相对预测坐标与ground true坐标。
- $$c_y^a$$和$$h^a$$是中心(y轴)和锚点的高度，它们可以从一个输入图片中被预计算好。
- $$c_y$$和h是预测的y轴坐标，$$c_y^*$$和$$h^*$$是ground truth坐标。因此，每个预测的text proposal具有一个size=hx16的边界，如图1(b)和图2(右)所示。通常，一个text proposal会大大小于有效可接受field（228x228）。

检测过程如下。给定一个输入图片，我们具有$$W \times H \times C$$的conv5 features maps（通过使用VGG16模型得到），其中C是feature maps或channels的数目，$$W \times H$$是空间位置（spatial arrangement）。当我们的检测器通过一个3x3的窗口通过conv5进行密集滑动时，每个滑动窗口会采用一个$$3 \times 3 \times C$$的卷积特征，来产生预测。对于每个预测，水平位置（x坐标）和k-anchor位置是固定的，它们可以通过将conv5上的空间窗口位置映射到输入图片上来预先计算好。我们的detector会为k个anchors在每个窗口位置输出text/non-text score和预测的y坐标(v)。检测到的text proposals从那些具有text/non-text score > 0.7的锚点上生成（没有最大限制）。通过设计垂直锚点和fine-scale检测策略，我们的检测器可以通过使用单个尺度的图片来处理多个尺度和比例范围的文本行。这进一步减小了计算量，同时还能精准预测文本行的位置。对比RPN或Faster R-CNN系统，我们的fine-scale检测提供了更详细的监督式信息，可以很自然地产生一个更精准的检测。

## 3.2 Recurrent Connectionist Text Proposals

为了提升位置精度，我们将一个文本行分割成一个fine-scale text proposals序列，然后各自对它们每一个进行预测。**很明显地，如果独立地将它们看成单个孤立的proposal是不健壮的。这会在一些非文本目标上（它们与文本模式具有相类似结构：比如，窗，砖，叶子等）产生许多错误的检测**。也可以丢弃一些包含弱文本信息的模糊模式。在图3(top）上的一些示例。文本具有很强的连续字符，其中连续的上下文信息对于做出可靠决策来说很重要。**可以通过RNN来编码文本信息进行文字识别进行验证**。一些paper的结果展示出，连续上下文信息可以极大地促进在裁减不好的词图片上（cropped word images）的识别任务。

受该工作的启发，我们相信该上下文信息对于我们的检测任务很重要。我们的检测器可以探索这些重要的上下文信息来做出可靠决策。再者，我们的目标是为了在卷积层直接编码该信息，产生一个优雅无缝的关于fine-scale text proposals的in-network连接。RNN可以循环地使用它的hidden layer编码该信息。出于该目的，我们提出设计一个在conv5之上的RNN layer，它采用每个窗口的卷积特征作为连续输入，循环更新在hidden layer中的内部state：$$H_t$$。

$$
H_t = \phi(H_{t-1}, X_t),  t=1,2,...,W
$$
...(3)

其中，$$X_t \in R^{3 \times 3 \times C}$$是来自第t个滑动窗口的输入的conv5 feature。滑动窗口会从左到右密集地移动，为每个row产生t=1,2,...,W的连续特征。W是conv5的宽度。$$H_t$$是recurrent internal state，可以从当前输入($$X_t$$)和先前在$$H_{t-1}$$中编码的state联合计算得到。该recurrence的计算使用一个非线性函数$$\phi$$，它定义了recurrent模型的准确形式。对于我们的RNN layer，我们采用LSTM的结构。LSTM的提出是为了解决梯度消失问题，通过引入三个额外的乘法门：input gate, forget gate和output gate。我们进一步扩展RNN layer，通过使用一个bi-directional LSTM，它允许双向编码recurrent上下文，因而， connectionist receipt field可以覆盖整个图片宽度，比如：228 x width。我们为每个LSTM使用了一个128D的hidden layer，来采用一个256D的RNN hidden layer，$$H_t \in R^{256}$$。

$$H_t$$的内部state被映射到下一个FC layer，以及output layer上用于计算第t个proposal的预测。因此，我们的与RNN layer集合的方式是优雅的，可以产生一个有效的模型，可以进行end-to-end训练，无需额外开销。RNN连接的高效性如图3所示。很明显，它减小了错误检测，同时，可以恢复许多缺失的text proposals（它们包含了非常弱的文本信息）。

<img src="http://pic.yupoo.com/wangdren23/HAti0rB5/medish.jpg">

图3: 上面三个：不使用RNN的CTPN。下面三个：使用RNN连接的CTPN

## 3.3 Side-refinement

fine-scale text proposals可以通过CTPN进行准确检测。文本行构建很简单，通过将那些text/no-text score > 0.7的连续的text proposals相连接即可。文本行的构建如下。首先，为一个proposal $$B_i$$定义一个邻居（$$B_j$$）：$$B_j -> B_i$$，其中：

- (i) $$B_j$$在水平距离上离$$B_i$$最近
- (ii) 该距离小于50 pixels
- (iii) 它们的垂直重叠(vertical overlap) > 0.7

另外，如果$$B_j -> B_i$$和$$B_i -> B_j$$，会将两个proposals被聚集成一个pair。接着，一个文本行会通过连续将具有相同proposal的pairs来进行连接来构建。


<img src="http://pic.yupoo.com/wangdren23/HAuuboLu/medish.jpg">

图4: 红色box：使用side-refinement的CTPN；黄色虚色的box：不使用side-refinement的CTPN。fine-scale proposal box的颜色表示一个text/non-text score

fine-scale detection和RNN连接可以预测在垂直方向上的累积位置。在水平方向上，图片被划分成一串16-pixel宽的proposals序列。当在两个水平侧（ horizontal sides）的text proposals不能准确被一个ground truth文本行区域覆盖时，或者一些side proposals被丢弃时（例如：具有一个较低的text score），这会导致一个不精准的定位，如图4所示。这种不准确在通用目标检测中不是很严格，但在文本检测中不能忽视，尤其是对于那些小尺度文本行或词。为了解决该问题，我们提供了一个side-refinement方法，可以准确地为每个anchor/proposal估计在水平左侧和水平右侧的offset（被称为side-anchor或side-proposal）。与y轴坐标的预测相似，我们计算了相对offset：

$$
o = (x_{side} - c_x^a) / w^a, o^{*} = (x_{side}^{*} - c_x^a) / w^a
$$...(4)

其中，$$x_{side}$$是相对于当前锚点最近水平侧（比如：左或右侧）的x预测坐标。$$x_{side}^{*}$$是ground truth侧在x轴坐标，通过BT 边界框和锚点位置预先计算好。$$c_x^a$$是在x轴的锚点中心。$$w^a$$是锚点宽度，它是固定的，$$w^a=16$$。当将一个检测到的fine-scale text proposals序列连接成一个文本行时，该side-proposals被定义成start proposals和end proposals。在图4中的一些检测样本通过side-refinement进行提升。 side-refinement可以进一步提升位置准确率，在SWT的Multi-Lingual datasets上产生2%的效果提升。注意，side-refinement的offset可以通过我们的模型同时进行预测，如图1所示。它不需要一个额外的后置处理step。

## 3.4 模型输出和Loss functions

CTPN有三个outputs，它们会一起连接到最后的FC layer上，如图1(a)所示。三个outputs同时预测：text/non-text scores(s)、垂直坐标(等式(2)中的$$v={v_c, v_h} $$  )、side-refinement
offset (o)。我们探索了k个anchors来在conv5中的每个空间位置上预测它们，在各自的output layer上产生2k, 2k和k个参数。

我们采用了多任务学习来联合优化模型参数。我们引入了三个loss functions：$$ L_s^{cl}, L_v^{re}, l_o^{re}$$，会各自计算text/non-text score、坐标以及side-refinement的error。有了这些，我们会根据faster R-CNN中的多任务loss，最小化一个关于一第图片的总目标函数（L）：

$$
L(s_i, v_j, o_k) = \frac{1}{N_s} \sum_{i} L_s^{cl}(s_i, s_i^{*}) + \frac{\lambda_1}{N_v} \sum_j L_v^{re}(v_j, v_j^{*}) + \frac{\lambda_2}{N_o} \sum_k L_o^{re}(o_k, o_k^{*})
$$
......(5)

其中，每个anchor是一个训练样本，i是minimatch中的一个anchor的索引。$$s_i$$是anchor i为一个真实文本的预测概率。$$s_i^{*} = {0, 1}$$是ground truth。j是一个关于y坐标回归中合法anchors集合的anchor索引，定义如下。一个合法的anchor是一个已经定义的positive anchor（$$s_j^*=1$$），或者具有一个与ground truth的text proposal具有Intersection-over-Union(IoU) > 0.5 的重合度。$$v_j$$和$$v_j^*$$是第j个anchor相关的y坐标的prediction和ground truth。k是关于一个side-anchor的索引，side-anchor被定义成在距离ground truth文本行限定框左侧或右侧的水平距离（例如：32-pixel）内的一个anchors集合。$$o_k$$和$$o_k^{*}$$分别是在第k个anchor相关的在x轴上的predicted offsets 和ground truth offsets。$$L_v^{re}$$和$$L_o^{re}$$是regression loss。我们会根据之前的工作，通过使用L1 function进行平滑来计算它们。$$\lambda_1$$和$$\lambda_2$$是用于平衡不同任务的loss weights，期望设置为1.0和2.0。$$N_s, N_v, N_o$$是归一化参数，分别表示通过$$L_s^{cl}, L_v^{re}, L_o^{re}$$的总anchors数。

## 3.5 训练和实现细节

CTPN通过标准的BP和SGD来进行end-to-end的训练。与RPN相似，训练样本是anchors，它们的位置可以通过在输入图片中的位置进行预计算得到，因而每个anchor的labels可以从相应的BT box计算得到。

**Training Labels**：对于text/none-text分类，会分配一个二分类label：positive（文本）、negative（非文本）anchor。它们通过计算BT bounding box（通过anchor位置进行划分）的IoU overlap得到。一个positive anchor被定义为：

- i. 一个具有一个与任意GB box有IoU>0.7的重合度(overlap)的anchor
- ii. 一个anchor具有一个与GT box的最高IoU重合度的anchor

通过条件(ii)的定义，即使一个非常小的文本模式也可以分配一个positive anchor。这对于检测小尺度的文本模式很重要，这也是CTPN的一个核心优点。这与通用目标检测很不同。negative anchors被定义成与所有GT boxes具有IoU<0.5重合度的anchors。y坐标回归($$v^{*}$$)的training labels以及offset regression ($$o^*$$)分别通过等式(2)和(4)定义。

**训练数据**：在训练过程中，每个minibatch样本都从单个图片中随机收集。每个mini-batch的anchors数目固定在$$N_s=128$$，正负样本的比例在1:1. 如果一个mini-batch的正样本小于64个，会用负样本进行补齐。我们的模型训练了3000张自然图片，包含229张来自ICDAR 2013训练集的图片。我们收集了其它图片并进行人工标注上相应的文本行的bounding boxes。所有这些自收集的训练样本在所有的benchmarks的任意测试图片没有重合。输入图片将它的short side设置为600进行训练，来保持它原始的比例尺。

**实现细节**：我们根据标准惯例，探索了极深的VGG16模型在ImageNet上的预训练。我们使用高斯分布为(0, 0.01)的随机权重来为new layers（例如：RNN和output layers）进行初始化。模型通过固定前两个convolutional layers的参数来进行end-to-end训练。我们使用0.9的momentum和0.0005的weight decay。learning rate在前16k次迭代设置为0.001, 在之后的4K次迭代使用0.0001的learning rate。我们的模型使用Caffe框架进行实现。

# 评测

略，详见paper。

# 参考

- 1.[https://arxiv.org/pdf/1609.03605.pdf](https://arxiv.org/pdf/1609.03605.pdf)
