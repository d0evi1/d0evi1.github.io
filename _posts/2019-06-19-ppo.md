---
layout: post
title: PPO(openai)介绍
description: 
modified: 2019-06-19
tags: 
---

openai在《Proximal Policy Optimization Algorithms》提出了PPO。我们来看下：

# 摘要

我们提出了一个在RL中关于policy gradient方法的一个新家族，它可以在以下两者间做交替：通过与enviroment进行交互的方式sampling data，以及使用SGA(随机梯度上升)来最优化一个目标函数。标准的policy gradient方法会在每个data sample上执行一个梯度更新（gradient update），我们提出了一个新的目标函数，它可以允许多个关于minibatch updates的epochs。新的方法，我们称之为proximal policy optimization(PPO)，它具有一些TRPO的优点，但更易于实际，更通用，并且具有更好的抽样复杂度（经验上）。我们的实验在许多benchmark任务上测试了PPO，包括仿真机器人运动(simulated robotic locomotion)和Atari游戏，我们展示了PPO的效果要好于其它online policy gradient方法，整体会在样本复杂度、简洁性和Wall-time上达到一个较好的平衡。

# 1.介绍

略

# 2.背景: Policy Optimization

## 2.1 Policy Gradient方法


Policy Gradient方法通过计算一个关于policy gradient的estimator，并将它插入到一个SGA(随机梯度上升）算法上。最常用的gradient estimator具有以下形式：

$$
\hat{g} = \hat{E}_t [ \nabla_{\theta} log \pi_{\theta} (a_t | s_t) \hat{A}_t ]
$$

...(1)

其中：

- $$\pi_{\theta}$$是一个stochastic policy
- $$\hat{A}$$是一个在timestep t时advatage function的estimator
- 期望$$\hat{E}_t[\cdots]$$表示在一个会在sampling和optimization间做交替的算法中，一个有限batch的样本上的经验平均(empirical average)。

那些使用自动微分软件（automatic differentiation software）的实现，通过构建一个目标函数：它的梯度是policy gradient estimator，estimator $$\hat{g}$$通过对以下的目标函数进行微分得到：

$$
L^{PG}(\theta) = \hat{E}_t [log \pi_{\theta} (a_t | s_t) \hat{A}_t]
$$

...(2)

在该loss $$L^{PG}$$上使用相同的trajectory执行多个step的optimization时，这样做并不是空穴来风，经验上它通常会导致具有破坏性的大梯度更新（见6.1节）

## 2.2 Trust Region方法

在TRPO中，目标函数（"surrogate" objective）会服从在policy update的size上的一个constraint的方式最大化。特别的：

$$
\underset{\theta}{maximize} \  \hat{E}_t [\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t ]
$$

服从：

$$
\hat{E}_t [KL[\pi_{\theta_{old}}(\cdot | s_t) , \pi_{\theta}(\cdot|s_t)]] \leq \delta
$$

...(4)

此处，$$\theta_{old}$$是在更新之前policy参数的向量。在对目标函数做一个线性近似、并且对constraint做一个二次方近似后，该问题可以使用共轭梯度算法(conjugate gradient)有效地被近似求解。

TRPO的理论证明建议我们使用一个正则项(penalty)来替代constraint，比如，对一些系数$$\beta$$求解以下没有constraint的最优化问题：

$$
\underset{\theta}{maximize} \ \hat{E}_t [\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t | s_t)} \hat{A}_t - \beta KL[\pi_{old}(\cdot|s_t), \pi_{\theta}(\cdot|s_t)]]
$$

...(5)

这遵循以下事实：一个固定的surrogate objective(它会计算在states上的最大KL)会形成在policy $$\pi$$一个下界（例如：一个pessimistic bound）。TRPO会使用一个hard constraint，而非一个penalty，因为它很难选择单个$$\beta$$值在多个不同问题（或者甚至在单个问题中，其中特征会随学习过程发生变化）上效果好。因而，为了达到我们关于一阶算法的目标（模仿TRPO的单调提升），实验展示了，它不足以简单选择一个固定的penalty系数$$\beta$$，并使用SGD对等式（5）的penalized objective进行最优化；需要额外的修改。


# 3.对Surrogate Objective进行裁减（Clip）

假设$$r_t(\theta)$$表示概率比值$$r_t(\theta) = \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{old}}(a_t \mid s_t)}$$，因而$$r(\theta_{old})=1$$。TRPO会最大化一个"surrogate"目标函数：

$$
L^{CPI}(\theta) = \hat{E}_t [\frac{\pi_{\theta}(a_t |s_t)}{\pi_{\theta_{old}}(a_t | s_t)} \hat{A}_t] = \hat{E}[r_t(\theta) \hat{A}_t]
$$ 

...(6)

上标CPI指的是保守策略迭代(conservative policy iteration)[KL02]，其中该objective是被提出的。**没有constraint后，对$$L^{CPI}$$最大化将会导致一个过大的policy update；因而，我们现在会考虑如何去修改该objective，来惩罚将$$r_t(\theta)$$远离1的那些policy的变更**。

我们提出的主要的objective如下：

$$
L^{CLIP}(\theta) = \hat{E}_t [min(r_t(\theta) \hat{A}_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t)]
$$

...(7)

其中epsilon是一个超参数，比如$$\epsilon=0.2$$。该objective的动机如下。min中的第一项是$$L^{CPI}$$。第二项 $$clip(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t$$，通过将概率进行clipping来修改surrogate objective，它会移除这样的动机：将$$r_t$$移出区间$$[1-\epsilon, 1+\epsilon]$$外。**最终，我们采用clipped和unclipped objective的最小值，因此，最终的objective是在unclipped objective上的一个下界（例如：一个pessimistic bound）**。有了这个scheme，当对该objective做出提升时，我们只能忽略在概率上的变更，如果包含它会使得objective更糟糕。注意：$$L^{CLIP}(\theta) = L^{CPI}(\theta)$$对应围绕$$\theta_{old}$$（比如：r=1）的一阶，然而，当$$\theta$$远移$$\theta_{old}$$时他们变得不同。图1画出了在$$L^{CLIP}$$上的单个项（比如：单个t）；注意，概率 r是在$$1-\epsilon$$或者$$1+\epsilon$$上做裁减取决于advantage是正或负。

<img src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/15e5478505b99ed439e185a0c2a928c273f2bf14d3e07052e77fd8c2f7097cb06ae5d699e3ceefc9e6f30296eadc3989?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=2.jpg&amp;size=750">

图1: 画出了surrogate function $$L^{CLIP}$$的某一项(比如：单个timestep)，作为概率比值r的一个函数，对于正的advantages(左)和负的advantages(右)。每个polit上的红色部分展示了optimization的起点（比如：r=1）。注意：$$L^{CLIP}$$会对所有这些项进行求和

<img src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/89c039226bd56e9dff742a9df08198ccf332d6f586060a253dee0bb322e923a5bc04107b0934789f76c8a363003137d8?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=3.jpg&amp;size=750">

图2: surrogate objectives，因为我们会在初始policy参数$$\theta_{old}$$间插入，updated policy参数，我们会在PPO的一次迭代后计算。updated policy具有与intial policy相差0.02的KL divergence，这一点上$$L^{CLIP}$$是最大的。。。

图2提供了另一个关于surrogate objective $$L^{CLIP}$$的来源。它展示了一些objectives是如何随着我们沿policy update的方向（通过PPO在一个continuous control问题上）变化的。**我们可以看到，$$L^{CLIP}$$是在$$L^{CPI}$$上的一个下界，它带有一个penalty，会对过大的policy update进行惩罚**。

# 4.Adaptive KL Penalty系数

另一种方法**可以被做为是clipped surrogate objective的一个替代选择**，这种方法为：在KL divergence上使用一个penalty，并对该penalty系数自适应(adapt)以便在每次policy update时能完成达到KL divergence $$d_{tagr}$$的一些target value。在我们的实验中，**我们发现，KL penalty比clipped surrogate objective的效果要差，然而，我们在这里仍会包含它，因为它是一个很重要的baseline**。

在该算法是最简单实现中，我们会在每次policy update时执行以下steps：

- 1.使用一些minibatch SGD的epochs，来优化KL-penalized objective：$$L^{KLPEN}(\theta) = \hat{E}_t [\frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{old}}(a_t \mid s_t)} \hat{A}_t - \beta KL [\pi_{\theta_{old}} (\cdot \mid s_t), \pi_{\theta}(\cdot \mid s_t)] ]$$
- 计算 $$d = \hat{E}_t[KL[\pi_{\theta}(\cdot \mid s_t), \pi_{\theta}(\cdot \mid s_t)]]$$
	- if $$d < d_{targ} / 1.5, \beta \leftarrow \beta / 2$$
	- if $$d > d_{targ} \times 1.5,  \beta \leftarrow \beta \times 2$$

更新后的$$\beta$$被用于下一次policy update。有了该scheme，我们会偶尔看到那些KL divergence与$$d_{targ}$$存在很大差异的policy updates，然而，这很少见，**因为$$\beta$$会很快进行调整**。上述关于参数1.5和2的选择是启发式的，但算法对它们非常不敏感。$$\beta$$的初始值是另一个超参数，但实际上不是很重要，因为该算法会很快对它进行调整。

# 5.算法

前一节的surrogate losses可以被计算，并使用一个关于典型的policy gradient实现上一个很小变更的版本进行微分。对于使用自动微分的实现，一个简单的构建loss $$L^{CLIP}$$或$$L^{KLPEN}$$来替代$$L^{PG}$$，会在该objective上执行多个SGA steps。

**大多数用于计算variance-reduced advantage-function的estimators会使用一个学到的state-value function$$V(s)$$**；例如，generalized advantage estimation[Sch+15a]，或者finite-horizon estimators[Mni+16]。**如果要使用一个在policy function和value function间共享参数的神经网络架构，我们必须使用这样一个loss function：它可以结合policy surrogate和一个value function error项**。该objective可以进一步通过添加一个entropy bonus进行扩展来确保足够的探索(exploration)，正如[Wil92, Mni+16]所建议的。通过组合这些项，我们可以获取以下的objective，它可以近似最大化每个迭代：

$$
L_t^{CLIP+VF+S}(\theta) = \hat{E}_t [L_t^{CLIP}(\theta) - c_1 L_t^{VF}(\theta) + c_2 S[\pi_{\theta}](s_t)]
$$

....(9)

其中:

- $$c_1, c_2$$是系数
- S表示一个entropy bonus
- $$L_t^{VF}$$是一个squared-error loss：$$(V_{\theta}(s_t) - V_t^{targ})^2$$

**在[Mni+16]中普及的一种policy gradient实现，很适合使用RNN，可以为T timesteps运行policy（其中：T要比episode length要小很多），并使用收集到的样本进行一个upate**。该种实现需要一个advantage estimator（它不会看到超过timestep T后的）。[Mni+16]中所使用的该estimator为：

$$
\hat{A}_t = -V(s_t) + r_t + \gamma r_{t+1} + \cdots + \gamma^{T-t+1} r_{T-1} + \gamma^{T-t} V(s_T)
$$

...(10)

其中：t指的是time index $$[0, T]$$，在一个长度为T的trajectory segment内。

我们可以使用一个truncated版本的generalized advantage estimation对该方式进行泛化，当$$\gamma=1$$时即可以化简为等式(10)：

$$
\hat{A}_t = \delta_t + (\gamma \lambda) \delta_{t+1} + \cdots + \cdots + (\gamma \lambda)^{T-t+1} \delta_{T-1}, \\
where \  \ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

....(11)(12)

使用固定长度的trajectory segments的一个PPO算法，如下所示。每个迭代中，（并行的）N个actors中的每个都会收集T timesteps的数据。接着，我们会在这些NT timesteps的数据上构建surrogate loss，并在K个epochs上使用minibatch SGD（或者，Adam）来进行optimize。

<img  src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/057f67b5bc7c0d1de82aa795c1a909aa4a8e91312f16aa98b107cff5caeec8950c8d6223541f97870a5a10d185a8d81e?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=1.jpg&amp;size=750">

算法1

# 6.实验





# 参考

- 1.[https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)