---
layout: post
title: LightFM介绍
description: 
modified: 2016-7-13
tags: 
---

lightFM源自于paper: 《Metadata Embeddings for User and Item Cold-start》：

# 一、介绍

构建推荐系统能在冷启动场景中（新用户、新item）运行良好是个挑战。标准的MF模型在该setting中效果很差：很难有效估计user和item的隐因子，因为协同交互数据很稀疏。

基于内容的（CB）方法，可以通过使用item的metadata来解决该问题。因为这些信息是事先预知的，对于新item（没有协同数据可收集）的推荐依然可以计算。不幸的是，在CB模型中没有迁移学习出来：对于每个用户的建模是以孤立的方式估计得到的，并没有利用其它用户数据。因此，CB模型的执行会比MF模型要差。

最后，解决该问题很关键。我们是一家时尚公司，目标是为我们的用户提供便利的方法进行在线时尚品浏览、购买。为了这个目的，我们维护了一个非常大的商品目标：在写入时，我们会跨网络聚合超过800w的时尚items，并会每天添加上万新的商品。

为我们做出推荐有三个因子。首先，我们的系统包含了一个非常大的items数目。这使得我们的数据很稀疏。第二，我们如下进行处理：通常，最相关的items是那些新释放出的collections，允许我们只有一个短时间窗口来收集数据，并提供有效推荐。最后，我们的用户大比例是首次访问的用户（first-time visitors）：我们希望为他们推荐引人注目的推荐，即使只有少量数据。用户和item的冷启动组合，使得纯粹的协同和CB方法不适用。

为了解决该问题，我们使用一个混合content-collaborative模型，称为LightFM，归因于它会对FM进行resemblance。在LightFM中，就像在一个协同过滤模型中一样，users和items被表示成隐向量（embeddings）。然而，正如在一个CB模型一样，这些都通过描述每个商品或用户的内容特征(content features)的函数进行定义。例如，如果该电影“绿野仙踪（Wizard of Oz）”通过以下的features描述："音乐幻想剧(musical fantasy)"、“朱迪·加兰（Judy Garland）”、以及“Wizard of Oz”，那么它的隐表示可以通过对这些features的隐表示进行求和得到。

通过这么做，LightFM可以将CB和CF的推荐的优点进行联合。在该paper中，我们会对该模型公式化，并在两个数据集上进行实验，展示：

- 1.在冷启动和低密度场景，LightFM至少与纯CB模型一样好，实质上，当满足以下二者之一（1）在训练集中提供了协同信息 (2) 在模型中包含了用户特征 时，效果要更好。
- 2.当协同数据很丰富时（warm-start, dense user-item matrix），LightFM至少与MF模型效果一样好。
- 3.通过LightFM生成的Embeddings，可以编码关于features的重要语义信息，可以被用于相关推荐任务：比如：标签推荐。

这对于真实推荐系统来说有许多好处。因为LightFM在dense和sparse数据上均表现良好，它不需要为每种setting构建和维护多个特定机器学习模型。另外，它可以同时使用user和item的metadata，它可以应用于user和item的冷启动场景。

LightFM python版的Github地址为：[https://github.com/lyst/lightfm](https://github.com/lyst/lightfm).

# 2.LightFM

## 2.1 动机

LightFM模型的结构受以下两种考虑的启发：

- 1.该模型必须能从交互数据中学习user和item表示：如果描述为“舞会袍（ball gown）”和"铅笔裙(pencil skirt)"的items均被用户所喜欢，该模型必须能学到ball gowns与pencil skirts相似。
- 2.该模型必须能为新items和users计算推荐

第1点通过使用隐表示方法来解决。如果ball gowns和pencil skirts均被相同的用户所喜欢，它们的embeddings会更接近；如果ball gowns和机车夹克（biker jackets）不会被相同的用户所喜欢，它们的embeddings会更远。

这样的表示允许迁移学习出现。如果对于ball gowns和pencil skirts的表示很相近，我们可以自信地推荐ball gowns给一个刚与pencil skirts交互过的新用户。

在纯CB模型之上使用降维技术（比如：LSI）也可以达到该目换，因为它们只会编码由feature co-occurrence给定的信息，而非用户动作。例如，假设所有浏览过由“飞行员（aviators）”描述的items的用户，同时也浏览了由“旅行者(wayfarer)”描述的items，但这两个features从未在相同的item中同时描述过。这种情况下，对于wayfarers的LSI vector不会与aviators的相似，即使协同信息建议应该这样做。

第2点通过将items和users表示成它们的content features的线性组合。由于content features被认为是：当一个user或一个item进入该系统时，它允许直接做出推荐。这种结构很容易理解。“牛仔夹克（denim jacket）”的表示看成是denim的表示和jacket的表示的求和（sum）；一个来自美国的女性用户（a female user from the US）的表示是US的表示和female users的表示的求和。

## 2.2 模型

为了公式化描述该模型，假设U是用户集，I是items集合，$$F^U$$是user features的集合，$$F^i$$是item features的集合。每个用户与多个items交互，正向或者负向。所有user-item交叉pair $$(u,i) \in U \times I$$是正交互$$S^+$$和负交互$$S^-$$的联合。

Users和items通过它们的features进行完全描述。每个user u通过一个特征集合描述 $$f_u \subset F^U$$。为每个item i它们的特征为$$f_i \subset F^I$$。features是提前知道的，可以表示user和item的metadata。

该模型通过d维的user和item的feature embeddings $$e_f^U$$和$$e_f^I$$为每个feature f进行参数化。每个feature也可以通过一个标量bias项(对于user features是$$b_f^U$$，对于item features则是$$b_f^I$$)描述。

user u的隐表示，通过对它的features的隐向量进行求和来表示：

$$
q_u = \sum_{j \in f_u} e_j^U
$$

item i的隐表示类似，如下：

$$
p_i = \sum_{j \in f_i} e_j^I
$$

user u的bias项，通过对features的biases进行求和得到：

$$
b_u = \sum_{j \in f_u} b_j^U
$$

item i的bias项如下：

$$
b_i = \sum_{j \in f_i} b_j^I
$$

该模型对于user u 和 item i的预测，接着通过user向量和item向量的点乘，加上对应的偏置给出：

$$
\hat{r_{ui}} = f(q_u \cdot p_i + b_u + b_i)
$$

...(1)

有许多函数适合$$f(\cdot)$$。一个identity函数也能对预测评分很好地运行；在本paper中，我们只对二分类数据预测感兴趣，因而选择sigmoid：

$$
f(x)= \frac{1} {1 + exp(-x)}
$$

模型的最优化目标是，最大化在该参数上的条件似然。该似然如下：

$$
L(e^U, e^I, b^U, b^I) = \prod_{(u,i)\in S^+} \hat{r_{ui}} \times \prod_{(u,i)\in S^-} (1-\hat{r_{ui}}
$$

...(2)

使用ASGD进行训练。4线程。learning rate使用ADAGRAD。

## 2.3 与其它模型关系

LightFM与协同MF模型间的关系，由user和item的feature sets的结构决定。如果feature sets只包含了每个user和item的指示变量，LightFM相当于MF模型。如果feature sets也包含了metadata features，它们被至少一个item或user所共享，那么LightFM就扩展了MF模型：通过让feature的隐因子来解释用户交互的部分结构。

这在三方面很重要。

- 1.在大多数应用中，metadata features要比users或items还要少，因为使用一个确定类型/类目的结构，或者因为维护一个固定size的关于最常用项的字典，当使用原始文本特征时。这意味着，从受限的训练数据中，需要估计更少的参数，减小overfitting机率并提升泛化效果。
- 2.指示变量的隐向量不能为新的、冷启动users和items进行估计。将它们表示成metadata features的组合，可以从训练集中被估计，并做出冷启动预测。
- 3.如果只有指定变量，LightFM与标准MF模型相当。

当只有metadata特征、没有指示变量时，模型通常不会缩减到一个纯CB模型。LightFM通过对协同交叉矩阵进行因子分解来估计feature embeddings；这不同于CB模型：它会对纯内容共现矩阵进行因子分解。

一个特别的case是，当每个用户通过一个指示变量描述时，并且只与一个item交互时，此时LightFM会变为CB。在该setting中，user vector等价于在LSI公式中的一个document vector，只有在product descriptions中共同出现过的features具有相似的embeddings。

事实上，LightFM一方面包含了在sparse data的纯CB模型，另一方面包含了在dense data上的MF模型。事实上，经验表明，至少与每种场景的单一模型一样好。

## 参考

[Label Partitioning For Sublinear Ranking](http://www.thespermwhale.com/jaseweston/papers/label_partitioner.pdf)


