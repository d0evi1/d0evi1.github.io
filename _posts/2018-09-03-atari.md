---
layout: post
title: ALE atari介绍
description: 
modified: 2018-09-03
tags: 
---

关于ALE (Arcade Learning Environment)的介绍来自于Marlos C. Machado发表的paper:《Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents》

# 1.介绍

ALE (电玩学习环境：Arcade Learning Environment)： ALE提供了一个关于Atari 2600游戏的数百个游戏环境的接口，这些游戏每个都是不同的、很有趣。ALE提供了对reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, 和 intrinsic motivation的研究挑战。更重要的是，它为这些问题提供了一个严格的testbed来评估和比较方法。我们使用AI技术(reinforcement learning/learning)通过开发和测试domain-independent agents来展示ALE。我们也提供了一个evaluation技术，在超过55个不同游戏上有结果。

# 

略...

# 接口

ALE在Stella（一个开源的Atari 2600模拟器）上构建。它允许用户通过接收joystick动作、发送screen/RAM信息、并模拟平台的方式来与Atari 2600交互。ALE提供了一个游戏处理层（game-handling layer），它通过标记累积得分、以及游戏是否已经结束，可以将每个游戏转化成一个标准的增强学习问题。缺省的，每个observation包含了单个游戏屏幕（game screen: frame）：一个关于7bit像素的2D数组，160 pixels宽 x 210 pixels高。action space包含了18个离散（discrete）的actions，它们通过操纵杆控制器（joystick controller）来定义。game-handling layer也指定了需要玩一个特定游戏的关于actions的最小集合。当运行时，该仿真器会每秒生成60帧，最高速度的仿真可以达到每秒6000帧。在每个time-step上的reward通过game basis来定义，通常通过在帧之间的得分(score/points)的不同来指定。一个episode会在reset命令后的第一帧(frame)处开始，当游戏结束时终止。game-handling layer也提供了在预定义帧数后终止episode的能力。user因此可以通过单个公共接口来访问数十个游戏，并可以很简单地增加新游戏。

<img src="http://pic.yupoo.com/wangdren23_v/3d359b94/8c6488ed.jpg">

图1 18个action

ALE也提供了保存(save)和恢复(restore)仿真器的状态(state)的功能。当发出一个save-state命令时，ALE会保存关于当前游戏所有相关数据，包括RAM、寄存器（registers）、地址计数器（address counters）的内容。restore-state命令会resets该游戏到之前saved state时的状态。这允许ALE作为一个生成模型来研究主题：planning、model-based RL。

<img src="http://pic.yupoo.com/wangdren23_v/cfd9ad81/8e243ae1.jpg">

图2 load/save

# 3.Benchmark结果

Planning 和 reinforcement learning是可以在ALE framework中研究的两个主要AI问题。在benchmark results中我们的目标是两者：第一，这些结果提供了一个对于传统技术的baseline performance，并确立了一个与更高级方法的比较点。第二，这些结果可以做经验上的validation。

## 3.1 RL

我们使用SARSA($$\lambda$$)来提供benchmark结果，这是一种model-free RL的传统技术。注意，在RL setting中，agent不会访问一个关于游戏动态性（game dynamics）的模型。在每个time-step上，agent会选择一个action并接收一个reward和一个observation，该agent的目标是：最大化它的累积回报（acumulated reward）。在这些实验中，我们会讨论：线性函数近似、replacing traces, e-greedy exploration。

## 3.1.1 特征构建

**Basic**：Basic方法来自于Naddaf’s BASS (2010)，会对Atari 2600 screen上的颜色进行编码。Basic方法首先移除了图片背景色，它通过在每个像素位置的颜色频率存储在一个histogram中。每个游戏背景是离线预计算好的，使用从sample trajectories中收集到的18000个observations。sample trajectories根据一个人工提供（human-provided）的trajectory，取随机数目的steps、并且随机均匀选择actions的方式来生成。该screen接着被划分成16x14 tiles。Basic会为每个128种颜色、每个tiles生成一个binary feature，共28672个features。

**BASS**：与BASIC相似。首先，BASS的特征集是pairwise组合。第二，BASS使用一个更小的、 8色的encoding来确保pairwise组合数目保持可跟踪。

**DISCO**：DISCO方法的目标是检测在Atari 2600 screen中的对象。为了这样做，与Basic方法生成的sample trajectories相似，它会首先预处理来自sample trajectories的36000个observations。DISCO会执行背景减少steps。接着抽取的对象标记(label)成classes。在实际训练期间，DISCO会infer所检测对象的class label，并将它们的位置和速度使用tile coding进行编码。

**LSH**：LSH方法会将原始的Atari 2600 screens使用Locally sensitive hashing映射到关于binary features的一个小集合上。

**RAM**：RAM方法会使用整个不同的observation space。它直接将Atari 2600的1024位内存进行observes。RAM的每一位可以当成一个binary feature提供。

### 3.1.2 评估技术

我们首先构建两个集合：一个用于training、一个用于testing。我们使用training games来进行调参，testing games用于评估。我们的training set包含了5个游戏：Asterix, Beam Rider, Freeway, Seaquest 和 Space Invaders。参数搜索涉及到发现SARSA算法最适合的参数值：比如：learning-rate、exploration rate、discount factor、decay rate $$\lambda$$。我们也会搜索特征生成参数的空间，例如：Bass agent的abstraction level等。我们的testing set通过从381个游戏中半随机选择。这些游戏中，128个游戏有它的wikipedia介绍页，具有单人模式、没有成人主题、可以在ALE中进行模拟。50个游戏被随机选中来形成test set。

在每个游戏上，每个方法的评估执行如下。一个episode从reset命令后的第一帧开始，当游戏结束条件被检测到、或者在5分钟后（即18000帧）玩后时结束。在episode期间，agent会每5帧进行acts，或者等价于gameplay的每秒12次。RL的实验(trial)包含了5000个training episodes，以及500个evalution episodes。agent的效果通过在evaluation episodes期间的平均得分进行measure。对于每个游戏，我们会在30个实验上报告我们的平均效果。

#  ...

略

# 参考

- 1.[https://arxiv.org/abs/1709.06009](https://arxiv.org/abs/1709.06009)
- 2.[http://arcadelearningenvironment.org](http://arcadelearningenvironment.org)
- 3.[https://github.com/openai/atari-py/blob/master/doc/manual/manual.pdf](https://github.com/openai/atari-py/blob/master/doc/manual/manual.pdf)