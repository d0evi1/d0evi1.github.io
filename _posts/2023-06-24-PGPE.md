---
layout: post
title: PEPG/PGPE介绍
description: 
modified: 2023-06-24
tags: 
---

Frank S等人在《Parameter-exploring Policy Gradients》中提出了PEPG/PGPE算法：

# 1.介绍

Policy gradient方法，这么称呼是因为：他们可以在policy space中进行搜索，而非从一个value function上直接对policy进行派生。

# 2.方法

在本节中，我们源自于在Markovian environment中的关于情景强化学习（episodic reinforcement learning）的通用框架，提出PGPE算法。特别的，我们会强调下PGPE和其它policy gradient方法（比如：REINFORCE）之间的差异。在2.3节中，我们会引入symmetric sampling，并解释它会提升收敛速度。

## 2.1 Policy Gradients with Parameter-Based Exploration

考虑一个agent，它的action $$a_t$$是基于在time t时的state $$s_t$$，产生在下一step中的state $$s_{t+1}$$。我们会对continuous state spaces和action spaces感兴趣，这对于大多数技术系统的控制来说是需要的。

我们假设：environment是Markovian，**在下一states $$s_{t+1}$$上的条件概率分布**，整体通过前一state-action pair来决定，即：

$$
s_{t+1} \sim p(s_{t+1} \mid s_t, a_t)
$$

我们也会假设这样的一个随机策略（stochastic policy），**在actions上的分布**依赖于**当前state**以及**关于agent参数的real valued vector $$\theta$$**：

$$
a_t \sim p(a_t \mid s_t, \theta)
$$

最后，我们假设：**每个state-action pair会生成一个Markovian reward标量**：

$$
r_t(a_t, s_t)
$$

我们将**由一个agent产生的state-action pairs的长度为T的state-action pairs序列**称为**一个history**（文献中的其它地方，这样的序列被称为 trajectories 或 roll-outs）：

$$
h = [s_{1:T}, a_{1:T}]
$$

给定以上的公式，我们会使用每个history h与一个cumulative reward r进行关联，并对每个time step上的rewards进行求和：

$$
r(h) = \sum\limits_{t=1}^T r_t
$$

在这样的setting中，reinforcement learning的目标是：发现参数$$\theta$$，最大化agent的expected reward：

$$
J(\theta) = \int_H p(h | \theta) r(h) dh
$$

...(1)

一个最大化$$J(\theta)$$的明显方法是：估计$$\nabla_{\theta} J$$，并使用它来执行梯度下降最优化（gradient ascent optimization）。注意：对于一个特定的history的reward是与$$\theta$$独立的，我们可以使用标准恒等式 $$\nabla_x y(x) = y(x) \nabla_x logx$$来获取：

$$
\nabla_{\theta} J(\theta) = \int_H p(h | \theta) \ \nabla_{\theta} log p(h | \theta)\ r(h)\ dh
$$

...(2)

由于environment是马尔可夫过程（Markovian）, 并且states是在给定agent的actions选择下给定参数下是条件独立的，我们可以写成：

$$
p(h | \theta) = p(s_1) \prod\limits_{t=1}^T p(s_{t+1} | s_t, a_t) p(a_t | s_t, \theta)
$$

将它代入到等式(2)生成：

$$
\nabla_{\theta} J(\theta) = \int_H p(h | \theta) \sum\limits_{t=1}^T \nabla_{\theta} p(a_t | s_t, \theta) r(h) dh
$$

...(3)

很明显，在histories的整个空间上进行积分是不可行的，我们因此对采样方法进行resort：

$$
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum\limits_{n=1}^N \sum\limits_{t=1}^T \nabla_{\theta} p(a_t^n | s_t^n, \theta) \ r(h^n)
$$

...(4)

其中：

- histories $$h^i$$会根据$$p(h^i \mid \theta)$$被选中。

接着该问题是：如何建模$$p(a_t \mid s_t, \theta)$$？

在policy gradient方法（比如：REINFORCE）中，参数$$\theta$$会被用于决定一个probabilistic policy：$$\pi_{\theta} (a_t \mid s_t) = p(a_t \mid s_t, \theta)$$。一个常见的policy model可以是一个参数化函数近似器，它的outputs定义了选中不同actions的概率。在本case中，通过在每个time step上根据policy分布选择一个action，来生成histories，接着，最终的gradient会通过对policy根据参数进行微分来计算。然而，**在每个time step上从policy抽样，会导致在histories上的抽样具有一个高variance，因此会导致一个有噪声的gradient估计**。

PGPE通过**使用一个在参数$$\theta$$上的概率分布来替换probibilistic policy，来解决variance问题**，例如：

$$
p(a_t | s_t, \rho) = \int_{\Theta} p(\theta | \rho) \sigma_{F_{\theta}(s_t), a_t} d\theta
$$

...(5)

其中：

- $$\rho$$：是决定在$$\theta$$上分布的参数
- $$F_{\theta}(s_t)$$：是(deterministic) action，指在state $$s_t$$下被具有参数$$\theta$$的模型选中
- $$\delta$$：是狄拉克δ函数（Dirac delta function）

该方法的优点是：actions是deterministic，因此整个history可以从单个参数的样本来生成。这种在samples-per-history上的简化法，会减小在gradient estimate中的variance。作为一个辅加好处，**参数梯度（parameter gradient）会通过直接参数扰动（perturbations）进行估计**，无需对任何导数（derivatives）进行backpropagate，它会允许使用无微分的controllers。

对于一个**给定$$\rho$$的期望收益（expected reward）**是：

$$
J(\rho) = \int_{\theta} \int_H p(h, \theta | \rho) r(h)  dh d\theta
$$

...(6)

对expected return根据$$\rho$$进行微分，并在之前应用log tricks，我们可以得到：

$$
\nabla_{\rho} J(\rho) =  \int_{\theta} \int_H p(h, \theta | \rho) \nabla_{\rho} log p(h, \theta | \rho) r(h) dh d\theta
$$

...(7)

注意，h条件独立于$$\theta \mid \rho$$，我们有：$$p(h, \theta \mid \rho) = p(h \mid \theta) p(\theta \mid \rho)$$，因此：$$\nabla_{\rho} log p(h,\theta \mid \rho) = \nabla_{\rho} log p(\theta \mid \rho)$$。将它代入等式(7)生成：

$$
\nabla_{\rho} J(\rho) = \int_{\theta} \int_H p(h, \theta) p(\theta | \rho) \nabla_{\rho} log p(\theta | \rho) r(h) dh d\theta
$$

...(8)

接着再次使用**抽样方法**，这次首先从$$p(\theta \mid \rho)$$选择$$\theta$$，接着运行agent来从$$p(h \mid \theta)$$来生成h。该过程会生成以下的gradient estimator：

$$
\nabla_{\rho} J(\rho) \approx \frac{1}{N} \sum\limits_{n=1}^N \nabla_{\rho} log p(\theta | \rho) r(h^n)
$$

...(9)

假设：$$\rho$$包含了一个关于均值 $$\lbrace \mu_i \rbrace$$、标准差$$\lbrace \sigma_i \rbrace$$的集合，它决定了在$$\theta$$中每个参数$$\theta_i$$的独立正态分布，对于$$log p(\theta \mid \rho)$$的导数会对$$\mu_i$$和$$\sigma_{i}$$分别求导：

$$
\nabla_{\mu_i} log p(\theta | \rho) = \frac{\theta_i - \mu_i}{\sigma_i^2} \\
\nabla_{\sigma_i} log p(\theta | \rho) = \frac{(\theta_i - \mu_i)^2 - \sigma_i^2}{\sigma_i^3}
$$

...(10)

它代入等式（9）中来逼近$$\mu$$和$$\sigma$$的gradients.

## 2.2 使用一个baseline来Sampling

给定足够的样本，等式（9）会决定reward gradient到任意accuracy。然而，每个样本需要rolling out一整个state-action history，它代价很高。**根据Williams(1992)，我们可以通过抽取单个样本$$\theta$$并将它的reward r与一个baseline reward b（通过在之前样本中给定的一个移动平均）进行对比，来获得一个成本更低的gradient estimate**。

直觉上，如果r>b，我们会调节$$\rho$$，以便增加$$\theta$$的概率，当r < b时则相反。

如果我们在positive gradient的方向上使用一个step size $$\alpha_i = \alpha \sigma_i^2$$（其中：$$\alpha$$是个常数），我们会获得以下的**参数更新等式**：

$$
\Delta_{\mu_i} \alpha (r-b)(\theta_i - \mu_i) \\
\Delta_{\sigma_i}  = \alpha (r-b) \frac{(\theta_i - \mu_i)^2 - \sigma_i^2}{\sigma_i}
$$

...(11)

## 2.3 对称采样（Symmetric sampling）

对于大多数场景，使用一个baseline采样很高效、并且是合理准确的，但它具有一些缺点。特别的，**如果reward分布是非常倾斜的，那么在sample reward和baseline reward间的对比会误导向**。一个更健壮的梯度近似法：**可以通过测量在在当前均值两侧的两个symmetric samples间的reward差异**来发现。

也就是说，我们：

- 选择一个来自分布$$N(0, \sigma)$$的扰动（perturbation）$$\epsilon$$
- 接着创建symmetric参数样本：$$\theta^+ = \mu + \epsilon$$和$$\theta^- = \mu - \epsilon$$
- 定义：$$r^+$$为在给定$$\theta^+$$下的reward；$$r^-$$为给定$$\theta^-$$下的reward

我们可以将两个样本插入到等式(9)中，并利用等式(10)来获取：

$$
\nabla_{\mu_i} J(\rho) \approx \frac{\epsilon_i (r^+ - r^-)}{2\sigma_i^2}
$$

...(12)

它会对在有限差分法（ finite difference methods）中使用的中心差分近似法（central difference approximation）进行重新组装。如同之前一样使用相同的step sizes，对于$$\mu$$项给出以下的更新公式：

$$
\Delta \mu_i = \frac{\alpha \epsilon_i(r^+ - r^-)}{2}
$$

...(13)

对于标准差来说，该更新会更关系密切些。由于$$\theta^+$$和$$\theta^-$$是在一个定给$$\sigma$$下通过相同概率构建得到，在它们间的difference不能被用来估计$$\sigma$$ gradient。作为替代，我们会采用两个rewards的均值$$\frac{r^+ + r^-}{2}$$，并将它对比baseline reward b。该方法会生成：

$$
\Delta \sigma_i = \alpha (\frac{r^+ + r^-}{2} - b)(\frac{\epsilon_i^2 - \sigma_i^2}{\sigma_i})
$$

...(14)

对比起第2.2节中的方法，**symmetric sampling可以消除误导baseline的问题，因此，提升$$\mu$$ gradient estimates**。它也可以提升$$\sigma$$ gradient estimates，因此，两个样本在当前概率下等概率，因此可以互相增强作为修改$$\sigma$$的受益的predictors。**尽管symmetric sampling需要在每次update时有两倍的histories，我们的实验表明：它可以在收敛质量和时间上给出一个很好的提升**。

作为一个最终refinement，我们会通过引入一个normalization term，让step size与rewards的规模（可能未知）独立。假设：如果已知，m是agent可以接受到的最大reward；或者未知，则是至今接收到的最大reward。我们通过将它们除以在m和对称样本的平均reward间的difference归一化$$\mu$$ updates；我们则通过除以在m和baseline b间的differnece来归一化$$\sigma$$ updates。给定：

$$
\Delta_{\mu_i} = \frac{\alpha \epsilon_i(r^+ - r^-)}{2m - r^+ - r^-} \\
\Delta_{\sigma_i} = \frac{\alpha}{m-b}(\frac{(r^+ + r^-)}{2} - b)(\frac{\epsilon^2 - \sigma_i^2}{\sigma_i})
$$

...(15)

PGPE的伪代码如算法1所示。注意：出于简洁，reward normalizeation项已经被忽略。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/f90c6a5e986214b1994a05856bcda7fdb6adfac1e59ba5cba0aaad4607fd32e0c410941869552614c0e2e3c5a0748bf9?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

算法1



# 

- 1.[https://mediatum.ub.tum.de/doc/1287490/file.pdf](https://mediatum.ub.tum.de/doc/1287490/file.pdf)