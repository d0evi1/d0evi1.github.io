---
layout: post
title: FiBiNET++介绍
description: 
modified: 2022-04-21
tags: 
---

weibo在《FiBiNet++:Improving FiBiNet by Greatly Reducing
Model Size for CTR Prediction》提出了FiBiNET++。

# 3.提出的模型

FiBiNet++的结构如图1所示。我们会对当前的FiBiNET进行讨论，特别是bi-linear function，它会导致大量的不必要参数。因此，在原始FiBiNet上的SENet模块之上的bi-linear function会被移除，只留下bi-linear function的左部分。另外，我们发现在FiBiNet++模型中移除时，FiBiNet的线性部分时会有好的表现。在网络结构上的这两种变化会直接减小model size，我们会在第3.5节中讨论细节。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/370c637847212454b5b3085056eff35610d6913016481197295449241a31e3e4e605facee859f0e5a896358f8be5e0b6?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 FiBiNet++结构

如图1所示，原始的feature embedding会首先归一化（normalized）。原始feature embedding在被送到后续组件前会首先被归一化。接着，bilinear+模块会建模feature interactions并且SENet+模块会计算bit-wise feature importance。两个branches会被拼接作为后续MLP layers的输入。

## 3.1 特征归一化（Feature Normalization）

Normalization技术已经被认为是在deep learning中非常有效的组成部分。受[10]的启发，我们会引入feature normalization到FiBiNet++中来增强模型的训练稳定性和效果，具体如下：

$$
N(V) = concat[N(v_1), N(v_2), \cdots, N(v_f)] \in R^{1 \times fd}
$$

...(7)

其中，$$N(\cdot)$$是对于数值型feature的layer normalization、以及对于**类别型feature的batch normalization操作**。

$$
N(v_i) = LN(v_i) if x_i \in S_c \\
BN(v_i) if x_i \in S_n
$$

...(8)

其中：

- $$S_c$$是类别型features的集合
- $$S_n$$是数值型features的集合

## 3.2 Bi-Linear+模块

FiBiNet会通过bi-linear function建模在feature $$x_i$$和$$f_{x_j}$$间交叉，它会引入一个额外学到的matrix W，如下所示：

$$
p_{i,j} = v_i \circ W \otimes v_j \in R^{1 \times d}
$$

...(9)

其中：

- $$\circ$$和$$\otimes$$表示inner product和element-wise hardmard product。
- matrix W可以是使用以下三种之一的参数：'field all type'、' field each type'、'field interaction type'。

尽管它对于通过bi-linear function建模feature interactions是有效的，我们会讨论：hadamard product会带来大量不必要的参数。为了有效减少model size，我们会使用以下两种方法将bilinear升级到bi-linear+模块。首先，hadamard product会通过另一个inner product可以替换：

$$
p_{i,j} = v_i \circ W \circ v_j \in R^{1 \times 1}
$$

...(10)

很容易看到，对于每个feature交叉，$$p_{i,j}$$的参数会从d维vector减少到1 bit。假设，input实例具有f个fields，并且我们在bi-linear feature interactions之后有以下vector：

$$
P = concat[p_{1,2}, p_{1,3}, \cdots, p_{f-1,f}] \in R^{1 \times \frac{f \times (f-1)}{2}}
$$

...(11)

为了更进一步减小参数数目，我们会引入一个压缩版MPL layer，会在vector P上进行stacking，如下所示：

$$
H^{CML} = \sigmoid_1 (W_1 P) \in R^{1 \times m}
$$

...(12)

其中：

- $$W_1 \in R^{m \times \frac{f \times (f-1)}{2}}$$是一个具有较小size m的thin MLP layer的学习矩阵。
- $$\sigma_1(\cdot)$$是一个identity function，它没有线性转换，因为我们发现：当使用非线性函数时，模型效果会递减

## 3.3 SENet+模块

SENet模块由三个steps组成：squeeze、excitation和reweight，在CTR预估领域首先由FiBiNet提出，可以动态计算feature importance。我们将它升级成SENet+用来增强模型效果。SENet+包含了4阶段：squeeze、excitation、reweight和fuse。尽管我们提出的SENet+模块如同原始SENet具有相似的三个阶段，每个step会被改进以便于增强模型效果。

**Squeeze**

SENet会通过mean pooling从每个feature embedding上收集关于“summary statistics”的one bit信息。然而，我们认为更多的input信息会有利于模型效果。因此，我们会通过提供更有用的信息来改进原始的squeeze step。特别的，我们会首先将每个normalized feature embedding $$v_i \in R^{1 \times d}$$分段成g个groups（g是一个超参数），如下：

$$
v_i = concat[v_{i,1}, v_{i,2}, \cdots, v_{i,g}]
$$

...(13)

其中，$$v_{i,j} \in R^{1 \times \frac{d}{g}}$$意味着在第i个feature的第j个group的信息。假设：$$k=\frac{d}{g}$$表示每个group的size。接着，我们选择最大值$$z_{i,j}^{max}$$以及在$$v_{i,j}$$中的平均pooling value $$z_{i,j}^{avg}$$作为该group的representative信息：

$$
z_{i,j}^{max} = max_t \lbrace v_{i,j}^t \rbrace_{t=1}^k \\
z_{i,j}^{avg} = \frac{1}{k} \sum\limits_{t=1}^k v_{i,j}^t
$$

...(14)

每个group的concatenated representative信息会形成feature embedding $$v_i$$的“summary statistic” $$Z_i$$：

$$
Z_i = concat [z_{i,1}^{max}, z_{i,1}^{avg}, z_{i,2}^{max}, z_{i,2}^{avg}, \cdots, z_{i,g}^{max}, z_{i,g}^{avg}] \in R^{1 \times 2g}
$$

...(15)

最终，我们可以将每个feature的summary statistic进行concatenate成SENet+模块的input：

$$
Z = concat [Z_1, Z_2, \cdots, Z_f] \in R^{1 \times 2gf}
$$

...(16)

**Excitation**

在SENet中的excitation阶段会根据statistic vector Z来计算每个feature的weight，它是一个field-wise attention。然而，我们会通过将field-wise attention更改为一个更细粒度的bit-wise attention。相似的，我们会使用两个FC layers来学习weights，如下：

$$
A = \sigma_3 (W_3 \sigma_2(W_2 Z)) \in R^{1 \times fd}
$$

...(17)

其中：

- $$W_2 \in R^{\frac{2af}{r} \times 2gf}$$表示第一个FC layer的学习参数，它是一个thin layer，并且r是reduction ratio。
- $$W_3 \in R^{fd \times \frac{2gf}{r}}$$表示第二个FC layer的学习参数，它是一个size为fd的wider layer.

这里$$\sigma(\cdot)$$是一个$$ReLu(\cdot)$$，$$\sigma_3(\cdot)$$是一个没有非线性变换的identity function。这种方式下，在input embedding的每个bit可以动态学习相应的由A提供的attention score。

**Reweight**

re-weight阶段会在原始field embedding和学到的attention scores间进行element-wise multiplication，如下所示：

$$
V^w = A \otimes N(V) \in R^{1 \times fd}
$$

...(18)

其中：

- $$otimes$$是一个在两个vectors间的element-wise multiplicaiton
- N(V)表示在normalization之后的原始embedding

**Fuse**

一个额外的“fuse”阶段会被引入进来，以便更好将original feature embedding中的信息和weighted embedding中的信息进行融合。我们首先使用skip-connection来将两个embedding进行merge：

$$
v_i^s = v_i^o \otimes v_i^w
$$

...(19)

其中：

- $$v_i^o$$: 表示第i个normalized feature embedding
- $$v_i^w$$: 表示在re-weight step之后的embedding
- $$oplus$$：是一个element-wise addition操作

接着，另一个feature normalization会被应到在feature embedding $$v_i^s$$中来得到一个更好的representation:

$$
v_i^u = LN(v_i^s)
$$

...(20)

注意，不管什么类型的feature(数值型feature/类别型feature)，我们会采用layer normalization。最终，我们将所有fused embeddings进行拼接作为SENet+模块的output：

$$
V^{SENet+} = concat[v_1^u, v_2^u, \cdots, v_f^u] \in R^{1 \times fd}
$$

...(21)

## 3.4 Concatenation layer

假设：

- $$H^{CML}$$：表示在bi-linear++模块中compression MLP layer的output
- $$V^{SENet+}$$：表示SENet+模块中的weighted feature embedding

我们将它们进行concatenate在一起来形成以下的MLP layers的输出：

$$
H_0 = concat[H^{CML}, V^{SENet+}]
$$

...(22)

## 3.5 讨论

在本节中，我们会讨论FiBiNet和FiBiNet++间的model size不同。注意，只有non-embedding参数会被考虑，它只表示model复杂度。

FiBiNet的主要参数来自于两个部分：一个是在第一个MLP layer间的connection以及两个bi-linear modules的output；另一个是linear part。假设我们表示h=400作为第一个MLP layer的size，f=50是fields number，d=10是feature embedding size，t=100w是feature number。因此，在这两parts中的参数数目大约是1080w：

$$
T^{FiBiNet} = ... = 1080w
$$

...(23)

而对于FiBiNet++，模型参数的主要部分来自于三个部分：第一个MLP layer和由SENet+模块生成的embedding间的connection（第一部分）；第一个MLP layer和compression MLP layer间的part（第二部分）；compression MLP layer间的参数和bi-linear feature interaction结果（第3部分）。假设：m=50表示compression MLP layer的size。我们有：这些组件的参数数目：

$$
T^{FiBiNet++} = ... = 28w
$$

...(24)

我们可以看到以上方法，可以极大减少model size，从1080w -> 28w，将近有39倍的模型压缩。另外，fields number f越大，可以达到更大的模型压缩率。

# 4.实验

略

# 参考


- 1.[https://arxiv.org/pdf/2209.05016.pdf](https://arxiv.org/pdf/2209.05016.pdf)