---
layout: post
title: wechat TCN介绍
description: 
modified: 2025-1-19
tags: 
---

wechat在《Introducing Context Information in Lifelong Sequential Modeling using Temporal Convolutional Networks》提出了TCN实现：


### 摘要

终身序列建模（Lifelong Sequential Modeling, LSM）在社交媒体推荐系统中的重要性日益增长。这一过程中的一个关键组件是**注意力模块**，它从序列中提取与候选item相关的兴趣表示。通常，注意力模块以点对点的方式工作，仅关注序列中单个item与候选item的相关性。然而，**邻近item中的上下文信息对于更准确地评估每个item的重要性是有用的**，这一点尚未被考虑。在本研究中，我们引入了一种新颖的网络，该网络采用时间卷积网络（Temporal Convolutional Network, TCN）为整个终身序列中的每个item生成**上下文感知(context-aware)的表示**。这些改进的表示随后被用于注意力模块中，以生成上下文感知的兴趣表示。基于这一TCN框架，我们提出了一个增强模块，该模块包括多个TCN层及其相应的注意力模块，以捕捉不同上下文范围内的兴趣表示。此外，我们还引入了一个轻量级的子网络，根据用户的基本profile特征创建卷积滤波器。这些个性化滤波器随后被应用于TCN层中，以替代原始的全局滤波器，从而生成更具用户特定性的表示。我们在一个公共数据集和一个专有数据集上进行了实验。结果表明，所提出的网络在预测准确性和在线性能指标方面优于现有方法。

### 1 引言

点击率（Click-Through Rate, CTR）预测是当今社交媒体平台上推荐系统的一项基本任务。其目标是预测：用户点击推荐内容的可能性。预测的准确性在很大程度上依赖于对用户兴趣与候选内容之间关系的理解。

近年来，深度神经网络（Deep Neural Networks, DNNs）在大多数场景中显著提高了CTR预测的准确性。成功的关键在于这些网络能够对用户的历史行为序列进行建模。这一过程的核心是**注意力机制**，它为序列中的每个item与候选item之间的相关性提供评分。这些评分被称为注意力分数，随后用于对序列进行加权求和，以生成候选item的最终兴趣表示。为了提高注意力机制的效率，已有大量研究工作展开[47, 48]。

随着用户行为的丰富化，用户历史行为序列的长度显著增加，**甚至可能扩展到终身范围**。序列长度的增加导致注意力机制的计算成本显著上升。在终身序列建模（Lifelong Sequential Modeling, LSM）中，一种有效减轻计算负担的方法是将注意力机制分为两个单元：通用搜索单元（General Search Unit, GSU）和精确搜索单元（Exact Search Unit, ESU）[32]。GSU的作用是从终身序列中筛选出与候选item最相关的item，随后ESU从GSU筛选出的item中提取用户兴趣表示。这种划分使模型能够处理更长的序列，这些序列包含更丰富的用户兴趣信息，从而进一步提高CTR预测的准确性。

然而，大多数先前的工作将注意力视为点对点的评分过程，通常分析候选item与序列中每个单独item之间的相关性。这种方法忽略了**相邻item在理解用户意图方面提供的宝贵信息**。这在当今的社交媒体平台（如TikTok、YouTube和微信视频号）上尤为重要，因为用户通常会连续消费一系列内容。例如，如图1所示，在连续消费流生成的用户行为序列中，用户对前三个item的观看时间非常长，但从第四个item开始，观看时间急剧下降。从点对点的角度来看，第三个item可能看起来是令人满意的，因为用户观看了很长时间。然而，从上下文感知的角度来看，第三个item可能并不那么受欢迎，因为它是用户愿意长时间观看的最后一个item。第三个item的某些属性可能导致用户失去兴趣，从而导致后续item的观看时间减少。因此，模型采用上下文感知的LSM至关重要，它考虑了序列中每个item的相邻item所包含的上下文信息。这种方法确保了对用户兴趣与候选item之间关系的更全面理解，并能提供更连续和相关的推荐结果。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/020623d76c70dd266a92e4ca21925136c7fcea9de2508fe7e5d711f48de4fb151c5f4ee9ba72e8662b6310e21a853210?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1

为了实现上下文感知的LSM，我们提出了上下文感知兴趣网络（Context-Aware Interest Network, CAIN）。CAIN首先通过对历史行为序列的时间轴进行卷积操作（称为时间卷积网络，Temporal Convolutional Network, TCN）[2]，提取序列中每个item的上下文感知表示。据我们所知，这是第一个将TCN引入LSM的网络。TCN具有两大优势：

- 与循环神经网络（Recurrent Neural Networks, RNN）[9, 20]和自注意力机制（Self-Attention）[41]等方法相比，**TCN更轻量且计算效率更高**；
- 此外，通过调整卷积的滤波器大小，可以**轻松控制上下文长度**。

TCN的输出表示随后被用于后续的注意力模块中，以提取与候选item相关的上下文感知兴趣表示，而不是使用原始的item表示。基于这一TCN框架，我们在CAIN中嵌入了多范围兴趣聚合器（Multi-Scope Interest Aggregator, MSIA）模块。MSIA模块包含多个堆叠的TCN层，这些层逐渐扩展输出表示的感受野。每一层的输出被发送到其对应的注意力模块中，以提取与候选item相关的不同上下文范围的兴趣表示。由于序列长度通过TCN层逐渐减少，后续卷积层的注意力计算成本也随之降低。最后，为了增强卷积操作的个性化能力，我们提出了个性化提取器生成（Personalized Extractor Generation, PEG）模块。该模块根据用户的基本资料特征为不同用户生成卷积滤波器。我们在所有TCN层中使用PEG模块生成的滤波器，而不是为所有用户使用统一的卷积滤波器。这使得TCN的输出表示更具用户特定性，从而进一步提高最终兴趣表示的代表性。

我们在一个公共数据集和从微信视频平台用户流量日志中收集的工业数据集上进行了广泛的实验。结果表明，与现有方法相比，所提出的CAIN在CTR预测准确性方面表现更优。此外，结果显示TCN框架和MSIA模块具有高度适应性，并在多种具有不同注意力设计的LSM基线模型上提供了性能提升。值得注意的是，CAIN在在线A/B测试中也取得了显著改进。这些发现证明了CAIN在复杂环境中增强CTR预测的有效性和鲁棒性。

### 2 相关工作

#### 2.1 序列建模

基于深度学习的模型在工业应用中取得了显著进展，例如在线广告和推荐系统[7, 18, 26, 32, 34, 40, 42–44, 48]。用户历史行为序列的建模（称为序列建模，Sequential Modeling, SM）对于这些模型理解用户意图并实现个性化预测至关重要。在这一领域已有大量研究工作[4, 6, 13, 32, 47, 48]。

随着用户行为变得更加复杂，历史行为序列的长度显著增加。因此，近年来更多的工作集中在终身序列建模（LSM）上。Sim[32]和UBR4CTR[33]是两种引入两阶段框架来建模用户终身序列的方法，包括通用搜索单元（GSU）和精确搜索单元（ESU）。GSU从整个用户行为历史中检索与目标item最相关的前k个item，然后将这些item输入ESU进行后续的注意力计算。该框架**严重依赖于预训练的嵌入**，这可能会降低GSU和ESU阶段之间的一致性。为了解决这个问题，ETA[6]提出使用SimHash[5]检索相关item，并通过局部敏感哈希（Locality-Sensitive Hashing, LSH）在ESU中对item嵌入进行编码。SDIM[3]也被提出，为候选item和行为item生成哈希签名，然后收集具有匹配哈希签名的行为item来表示用户兴趣。这两种方法都允许两个阶段共享相同的嵌入以提高一致性。此外，TWINS[4]通过引入CP-GSU来增强一致性，CP-GSU不仅检索与目标相关的行为，还检索ESU认为重要的行为。此外，一些工作将两阶段框架升级为三级注意力金字塔[21]，以进一步增强阶段之间的一致性。

然而，大多数工作将LSM视为点对点的过程，仅关注序列中单个item与候选item之间的关系。它们忽略了序列中相邻item提供的上下文信息的重要性。在本文中，我们的目标是实现一种上下文感知的LSM，考虑序列中每个item的相邻item。

## 2.2 上下文感知建模

上下文感知建模方法在自然语言处理（NLP）、计算机视觉（CV）和语音识别（SR）领域得到了广泛应用。长短期记忆网络（LSTM）[20] 和门控循环单元（GRU）[9] 是经典的循环神经网络（RNN）模型，被广泛用于各种 NLP 和 SR 任务 [17, 27, 29, 45?, 46]。近年来，Transformer 模型和自注意力机制（Self-Attention）[41] 已成为 NLP 领域的基础组件，其特点是仅基于注意力机制的编码器和解码器。GPT [35] 和 BERT [11] 是构建在该模块上的两个著名模型。除了基于 RNN 和基于注意力的方法外，还存在其他上下文感知建模方法。时间卷积网络（TCN）[2] 就是其中之一。卷积神经网络（CNN）在 CV 任务中占据主导地位 [14, 15, 19, 23, 36–38]，因为卷积操作非常适合处理图像，通过自然地考虑目标像素的邻近像素来生成高级特征图。TCN 沿时间维度进行卷积操作，使生成的表示具有上下文感知能力 [8, 12, 24, 25, 30]。

研究人员已将 RNN 和自注意力机制引入序列建模（SM）中，用于点击率（CTR）预测。CA-RNN [28] 和 CRNNs [39] 是两种使用 RNN 预测用户历史记录中下一个item概率的方法。DEIN [47] 使用 GRU 提取每个用户的兴趣状态，并利用 AUGGRU 对目标item的兴趣演变进行建模。考虑到序列由会话组成，DSIN [13] 使用自注意力机制提取用户在每次会话中的兴趣，然后应用 Bi-LSTM 对用户兴趣在会话间的演变进行建模。

然而，尽管这些方法为 CTR 任务带来了一定的改进，但由于**计算负担沉重**，它们在扩展到终身序列建模（LSM）时面临挑战。因此，在 LSM 设置中，需要一种**更轻量级且计算效率更高的方法**来捕捉上下文信息。

## 3 预备知识

本文探讨上下文感知终身序列建模（Context-Aware Lifelong Sequential Modeling, LSM）的主题。与传统 LSM 不同，传统方法在注意力过程中通常将序列中的item视为孤立行为以评估其与候选item的相关性，而上下文感知 LSM 方法则考虑序列中每个item的上下文。本文定义的上下文由序列中每个item周围的相邻item组成，通常表示用户在该item前后直接交互过的item。

### 形式化定义

令：$\overset{\rightarrow}{𝐿𝐻} = \lbrace lh_1, lh_2, \cdots, lh_n \rbrace $表示用户的终身行为序列。以item $lh_t$（称为中心item）为中心的上下文可表示为：

$$
\text{ctx}_t = \{𝑙ℎ_{t-c_l}, 𝑙ℎ_{t-c_l+1}, · · · , 𝑙ℎ_{t-1}, 𝑙ℎ_{t+1}, · · · , 𝑙ℎ_{t+c_l-1}, 𝑙ℎ_{t+c_l}\}, \tag{1}
$$

其中 $c_l$ 表示上下文长度。

### 特征分类

本文模型中，每个用户包含三类特征：

- **基础画像特征**：记为 {𝐵}；
- **短期行为序列**：记为 $\overset{\rightarrow}{𝐻}= \lbrace h_1, h_2, \cdots, h_n \rbrace$
- **终身行为序列**：记为 $\overset{\rightarrow}{𝐿𝐻} = \lbrace lh_1, lh_2, \cdots, lh_n \rbrace $

需注意：
- 终身和短期行为序列中的每个item（$lh_n$ 或 $h_n$）均包含其item ID 和附加信息（如用户观看时长）；
- 对于user-item对 $\langle u_i, v_i \rangle$，模型目标为预测用户 $u_i$ 对item $v_i$ 的点击率（CTR）：

$$
p_i = P(y_i = 1 \,|\, u_i, v_i, B, \overset{\rightarrow}{𝐻}^t, \overset{\rightarrow}{LH}_t; \theta), \tag{2}
$$

其中：

- $\theta$ 为模型参数。

### 损失函数

主网络通过交叉熵损失函数优化：

$$
\mathcal{L}_{CTR} = -\frac{1}{BS} \sum_{i=1}^{BS} \left[ y_i \cdot \log(p_i) + (1 - y_i) \cdot \log(1 - p_i) \right], \tag{3}
$$

其中：
- $y_i \in \{0, 1\}$ 表示用户真实反馈（点击=1，未点击=0）；
- $BS$ 表示训练批次中的样本对总数。

## 4 方法论

本文提出了一种新颖的上下文感知兴趣网络（**CAIN**），使模型在进行终身序列建模（LSM）时能够考虑用户历史序列中每个item的上下文信息。CAIN 通过时间卷积网络（TCN）计算序列中item的上下文感知表示，并将这些表示输入后续注意力模块（替代原始item表示），从而提取针对候选item的上下文感知兴趣表示。

基于 TCN 框架，我们进一步在 CAIN 中引入两个核心模块以提升性能：
1. **多范围兴趣聚合器（MSIA）**：包含多个 TCN 层，用于提取不同上下文长度下的表示，并将其输入对应的注意力模块以提取不同上下文范围的兴趣表示；
2. **个性化提取器生成器（PEG）**：包含轻量级网络，根据用户基础画像特征生成卷积滤波器。

图2 展示了 CAIN 的总体架构及其与传统 LSM 网络的对比。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/92b53257065eb6ff76e65816f86cd97320504feeea7f17796086831507d40af19e3a51a8b1acf0d8e771a8b1fde9e9ed?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2-1.jpg&amp;size=750">

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/37dbd1a2221e9720aecb578cad646bbedbce378600a00fbbdde8cbc811163b9828c56189873e109b79a1927140e0c56a?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2-2.jpg&amp;size=750">

图2

## 4.1 上下文信息提取

从终身序列中提取用户对候选item兴趣的常见方法是执行注意力过程。在此过程中，候选item（记为$v$）作为query项，与序列中的item $\langle v, lh_t \rangle$（其中$lh_t \in \mathbf{LH}$）形成配对。每个$\langle v, lh_t \rangle$对根据注意力模块中的网络分配注意力分数：

$$
s_t = \text{Attn}(v^e, lh^e_t; \theta_a), \tag{4}
$$

其中：

- $v^e$和$lh^e_t$分别表示$v$和$lh_t$的item表示
- $\theta_a$表示注意力模块参数
- $s_i$为注意力分数。这些分数将用于后续的检索或加权求和阶段。

值得注意的是，上述注意力过程主要关注$v$与$lh_t$之间的关系。然而，在现实场景中，用户行为往往具有连续性——用户会连续交互一系列item。这凸显了考虑上下文（即序列中每个中心item前后的相邻item）对全面理解行为及其与候选item相关性的重要性。

在提出的CAIN中，我们通过在注意力模块前引入时间卷积网络（TCN）[2]实现上下文感知的LSM。

### 4.1.1 表示提取

与循环神经网络（RNN）[9, 20]或自注意力[41]相比，TCN具有两大优势：
1. **轻量高效**：TCN仅需单次矩阵乘法即可沿序列滑动卷积核，而RNN和自注意力计算量显著更大（尤其是RNN包含无法并行的操作）；
2. **显式控制上下文长度**：RNN和自注意力的上下文长度通常隐式学习，可能影响泛化能力（实验部分将深入探讨）。

TCN操作可视为**在序列上滑动的线性层**。给定：上下文长度$cl$，卷积核$W_C$的尺寸为$2cl+1$（覆盖中心item$lh_t$两侧的上下文）。

令：$ctx^e_t$表示上下文$ctx_t$内的item表示，每个卷积窗口的计算为：

$$
cr_t = \left( ctxe_t \cup lhe_t \right) \times W^C + b_c, \tag{5}
$$

其中：

- $cr_t$为$lh_t$的**上下文感知表示输出**
- $b_c$为偏置项

对于序列首尾缺乏足够上下文项的元素，采用零填充（zero-padding）以满足指定上下文长度。

### 4.1.2 表示替换

经TCN处理后，原始item表示$lhe_t$被转换为：包含中心item及其上下文信息的$cr_t$。为获取用户对候选item的上下文感知兴趣表示，我们将注意力模块中的$lhe_t$替换为$cr_t$，公式(4)更新为：

$$
s_t = \text{Attn}(ve, cr_t; \theta_a). \tag{6}
$$

相似的，在其它过程中，我们将$lhe_t$替换为$cr_t$，该修改确保注意力模块考虑用户交互的全局上下文，从而生成更精准的上下文感知兴趣表示。

---

## 4.2 多范围兴趣聚合器

不同长度的上下文可提供多样化的序列洞察：

- 长上下文反映item对用户的深层影响（如兴趣迁移/衰减）
- 短上下文捕捉用户行为的即时变化

为提取多范围上下文下的兴趣表示，我们提出**多范围兴趣聚合器（MSIA）**，其架构如图3所示。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/179a9f702cf00eaba96007da3d731d23b85501c13fd81c0391921bb328a828ea93e8611a2fe8aad4fbe9ff6044166651?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

### 4.2.1 堆叠层

通过堆叠TCN层可高效扩展上下文范围。**随着网络加深，TCN层的感受野(receptive field)逐步增大**。设：第$n$层的上下文长度为$cl_n$，当卷积步长=1时，第$n+1$层的上下文长度为：

$$
cl_{n+1} = cl_n + fs_{n+1} - 1, \tag{7}
$$

其中：$fs_{n+1}$为第$n+1$层的卷积核尺寸。

通过此方式，**每层输出包含不同范围的上下文信息**，且深层输出的表征性因TCN非线性操作而增强。

为降低深层计算成本，除首层外其他层的步长>1。这显著缩短输出序列长度，减少后续注意力模块的计算量。同时，步长>1加速上下文范围的扩展，以更少层数实现更大范围。首层步长固定为1以保证终身序列中每个中心item均有独立表示，而深层因关注整体上下文信息，允许合并输入表示。

### 4.2.2 独立注意力

为提取不同上下文范围对候选item的兴趣表示，我们为每层TCN输出配备独立参数的注意力模块（支持并行计算）。注意力模块分为两类：

1. **主注意力**：仅应用于首层TCN输出，使用终身注意力金字塔（LAP）[21]提取兴趣表示$IR_1$（与基线方法兼容，也可替换为ETA [6]/SDIM [3]）；
2. **辅助注意力**：应用于后续TCN层输出。因步长>1导致输出序列较短，且关注整体上下文信息而非单个item，采用带线性投影的目标注意力：

$$
sa_t = \frac{(W^Q_n v^e) \cdot (W^K_n cr^n_t)^\top}{\sqrt{d}}, \tag{8}
$$

$$
IR_n = \sum_{t=1}^{T_n} sa^n_t \cdot (W^V_n cr^n_t), \tag{9}
$$

其中：

- $W^Q_n, W^K_n, W^V_n$为投影权重
- $d$为内部维度
- $cr^n_t$为第$n$层TCN输出

最终，MSIA模块将所有兴趣表示$IR_n$（含$IR_1$）拼接，形成多范围上下文的集成表示，平衡细粒度item信息与宏观上下文理解。

## 4.3 个性化提取器生成

传统卷积滤波器通常在不同输入间共享参数，其隐含假设是：输入数据服从相似分布。然而，当使用TCN处理用户历史序列时，该假设可能不成立。用户行为在不同类型用户间差异显著，且item对后续行为的影响程度也存在巨大差异。例如：

- **高活跃用户**可能主要基于item内容决定观看行为，受先前展示item的影响极小（即使存在负面体验）；
- **低活跃用户**的行为更易受外部因素影响，对给定item前的展示item更敏感。

为使上下文提取更具个性化，我们提出**个性化提取器生成器（PEG）**模块（受个性化冷启动模块POSO [10]启发）。PEG模块为每个用户生成专属卷积滤波器，适配其独特行为模式，其架构如图4所示。

### 4.3.1 滤波器生成
PEG模块包含轻量子网络，用于为每个用户生成个性化卷积滤波器$W^{PC}_n$。该子网络由两个全连接层构成，通过捕捉用户基础画像特征的交互关系，将输入映射到滤波器参数空间。生成过程形式化表示为：

$$
O_n = \text{ReLU}(W^{P1}_n \times I + b^{P1}_n), \tag{10}
$$

$$
W^{PC}_n = \text{Reshape}(W^{P2}_n \times O_n + b^{P2}_n), \tag{11}
$$

其中：
- $W^{P1}_n, b^{P1}_n$：第一层全连接的权重与偏置
- $W^{P2}_n, b^{P2}_n$：第二层全连接的权重与偏置
- $I$：从集合$B$中提取的用户基础画像特征表示

PEG模块使用的基础画像特征包含：
- **人口统计信息**：年龄、性别、地域、教育背景等
- **行为统计信息**：展示item数、点击item数、用户最常交互作者（如最常观看作者）

实验部分将详细讨论这些特征对模型性能的影响。

### 4.3.2 个性化卷积
生成个性化滤波器后，我们将其替换各TCN层中的原始全局滤波器。尽管可采用"全局+个性化滤波器组合"方案，但实验表明完全替换方案效果更优。因此，公式(5)在CAIN中被改写为：

$$
cr_t = \left( \text{concat}(ctx^e_t, lh^e_t) \right) \times W^{PC} + b_c. \tag{12}
$$

通过个性化卷积滤波器，模型能更精准捕捉用户行为的独特模式，从而提升预测精度。


# 

[https://arxiv.org/pdf/2502.12634](https://arxiv.org/pdf/2502.12634)