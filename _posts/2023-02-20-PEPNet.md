---
layout: post
title: PEPNet介绍
description: 
modified: 2023-02-20
tags: 
---


kuaishou在《PEPNet: Parameter and Embedding Personalized Network for
Infusing with Personalized Prior Information》中提出了PEPNet：


# 1.抽要

随着在线服务（电商/在线视频）的内容页面和展示样式的增加，工业级推荐系统经常面临着multi-domain和multi-task的推荐挑战。multi-task和multi-domain推荐的核心是，在给定不同的用户行为时，能精准捕获在不同domains中的用户兴趣。在本paper中，我们在multi-domain环境下的multi-task推荐上**提出了一种即插即用（plug-and-play）的 Parameter and Embedding Personalized Network (PEPNet) **。PEPNet会将具有强bias的features作为input，并**动态地对模型中的bottom-layer embeddings和top-layer DNN hidden units通过一个gate机制进行缩放（scale）**。通过将个性化先验（personalized priors）映射，将weights归一化范围(0, 2)，PEPNet会同时引入参数个性化和embedding个性化。PPNet（Parameter Personalized Network ）会影响DNN参数，来对多个任务上的相互依赖目标进行权衡。我们会做出一系列特征的工程优化，将Kuaishou训练框架与在线部署环境相结合。我们已经成功在Kuaishou apps上进行部署，服务了3亿的日活用户。

# 1.介绍

略

# 2.方法

该部分会介绍详细设计，用来缓解“双跷跷板（double seesaw）”问题。我们会详述问题公式、以及PEPNet的网络结构。

## 2.1 问题公式

这里我们定义了我们研究的概念和问题settings。该模型会使用sparse/dense inputs，比如：用户历史行为、用户profile features、item features、context features等。预估目标$$\hat{y}_i$$是user u在domain d的第i个task上对item p的偏好，它的计算如下：

$$
\hat{y}_i = f(\lbrace E(u_1), \cdots, E(u_t) \bigoplus E(p_1), \cdots, E(p_j) \bigoplus E(c_1), \cdots, E(c_k) \rbrace_d)
$$

...(1)

其中：

- $$u_1, \cdots, u_t$$：表示**user-side features**，它包含了：用户的历史行为、user profile和user ID等。
- $$p_1, \cdots, p_j$$：表示**target item features**，它包含了：item ID(iid)、author ID(aid)等
- $$c_i, \cdots, c_k$$：表示**其它features**，它会包含context feature和combine feature
- $$\lbrace \rbrace_d$$表示来自domain d的样本
- $$E(*)$$：表示**sparse/dense features**，会通过在分桶算法之后的embedding layer被映射到可学习embedding中，
- $$\bigoplus$$：表示 **concatenation**

对于一个真实应用，item candidate pool和用户part会在多个场景共享。由于不同的消费目标，用户在不同场景对于相同item会改变他们的行为趋势。为了更好捕获对于不同行为的用户偏好，并增强在多个场景的联系，推荐系统需要同时为多个domains D做出多任务预估。注意，模型输入是$$\lbrace x, y_i, D \rbrace$$，

其中：

- x：是上述提到的feature
- $$y_i$$：是每个任务的label
- $$d \in D$$：是domain indicator，它表示样本收集来自哪个domain

- Input：表示sparse/dense inputs，比如：用户历史行为、用户profile features、item features和其它context features
- Output：一个推荐模型，它会估计用户在多个domains上的多个目标，例如：点赞（like）、关注（follow）、转发（forward）等

## 2.2 网络结构

图3展示了PEPNet的网络结构。该模型由三个部分组成，我们一一解释。

- **GateNU（Gate Neural Unit）**：Gate NU是EPNet和PPNet的基本单元，它是一个基于先验信息生成的门控结构。
- **EPNet（Embedding个性化网络：Embedding Personalized Network）**：EPNet会采用domain信息作为input，并使用Gate NU来进行domain-specific个性化，增强模型的bottom layer的能力来表示跨域的features。
- **PPNet（参数个性化网络：Parameter Personalized Network）**。 PPNet使用用户信息和item信息来生成gates，并调整在不同task towers上每个layer的参数，并对模型的top layer上相互依赖目标进行权衡。


<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/08788b83b93c3edc73b2ab2e936b205361691088699ad017c97155530e045ab7cc9ada9e9b7777f6324f7a524642c951?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.png&amp;size=750">

图3  PEPNet包含了Gate NU, EPNet, PPNet。Gate NU是基础单元，它会使用先验信息来生成门控和增强的合法信号。EPNet会在Embedding layer上增加模型的domain-awareness，并将PPNet进行stacking到每个任务的DNN tower上来增强task personalization. 在多个domains上会估计相同集合的multi-targets。PEPNet可以被插入到任意网络中。颜色如上所示。

### 2.2.1 Gate Neural Unit(Gate NU)

受LHUC算法的启发，其关键思想是：**为每个speaker学习一个特定的hidden unit贡献，PEPNet会引入一个门控机制，称为“Gate Neural Unit”，它对于不同用户来说，网络参数是个性化的**。 Gate Neural Unit（简称为Gate NU），包含了两个nueral network layers。

- $$x^{(0)}$$：表示Gate NU的input
- $$W^{(0)}$$：表示第一层network layer的weight
- $$b^{(0)}$$：表示第一层network layer的bias

Relu被选择作为第一层该函数的activation function。第一层公式如下：

$$
x_1 = Relu(x^{(0)} W^{(0)} + b^{(0)})
$$

...(2)

接着， Gate NU会使用Sigmoid function来生成gate，它会将output限制为[0, 1]。

- $$\gamma$$是参超数，设置为2
- $$W^{(1)}$$和$$b^{(1)}$$是第二个layer的weight和bias

第二层的公式化如下：

$$
x_2 = \gamma * Sigmoid(x^{(1)} W^{(1)} + b^{(1)}), x_2 \in [0, \gamma]
$$

...(3)

根据等式1和等式2，**Gate NU会使用先验信息$$x^{(0)}$$来生成gating vector，并使用超参数$$\gamma$$来进一步放大有效信号**。接着，我们会详述如何使用该gating机制来组合EPNet和PPNet。

### 2.2.2 Embedding Personalized Network（EPNet）

出于计算和内存开销考虑，EPNet模型会共享相同的embedding layer，其中：

$$
E(*) = E(SF) \oplus E(DF)
$$

...(4)

- SF是sparse features，DF是dense features。
- $$E(*)$$是共享底层结构，它实际上会有许多缺点，关注共享却忽略了多个domains的不同

**对于共享的EPNet，我们会使用domain-side features $$E(df) \in R^k$$作为input，比如：domain ID和统计特征**。

对于在第i个domain的特定数据样本，我们将**其余feature表示为$$E(*) \in R^d$$**，其中：

- d是input维度
- **$$V_{ep}$$是embedding layer的Gate NU**

EPNet的输出$$\sigma_{domain} \in R^d$$给定如下：

$$
\sigma_{domain} = V_{ep}(E(df))
$$

...(5)

我们使用一个额外的Gate NU network来将embedding进行变换，并将多个domains的分布进行对齐（align），无需变更原始的embedding layers。转换后的embedding（transformed embedding）如下：

$$
O_{ep} = \sigma_{domain} \otimes E(*)
$$

...(6)

其中：

- $$O_{ep} \in R^d$$: 输出
- $$\otimes$$: 是element-wise乘法

### 2.2.3 Parameter Personalized Network(PPNet)

为了**增强关于task-specific个性化的信息**，我们使用user/item/author-side feature(uf/if/af)作为PPNet的输入，比如：user ID, item ID, author ID，以及side information features，比如：user age/gender, item tag/topic/popularity等。特别的，详细的PPNet结构如下：

$$
O_{prior} = E(uf) \oplus E(if) \oplus E(af) \\
\sigma_{task} = V_{pp} (O_{prior} \oplus (\oslash(O_{ep})))
$$

...(7)

其中:

- $$E(uf) \in R^u, E(if) \in R^i, E(af) \in R^a$$

PPNet会将EPNet的output拼接在一起，**features $$O_{prior}$$具有很强的个性化先验**，它会给出模型关于先验信息的更多感知。关于个性化的先验信息可以通过一个来自user ID、item ID以及author ID的扩展来获得，其中：author表示kuaishou短视频的创作者。**为了不影响在EPNet中已经更新的embedding，我们会在EPNet的output上执行stop gradient $$\oslash$$操作**。在传统模型中，所有hidden units会被相等对待，并传给下一layer中。我们使用element-wise乘法来选择和放大合法信号，如下：

$$
O_{pp} = \sigma_{task} \otimes H
$$

...(8)

其中：

- H是在task towers上每个DNN layer上的hidden unit。

在多任务学习中的参数共享可以极大减小DNN参数的size，但在多个共享targets间的一些信息会丢失，导致不均衡的效果。**例如，预估Follow和Like的任务会共享DNN参数，但Follow任务具有更少的正样本**。前两者的梯度会累计，Follow的一些信号会被Like所覆盖。因此**对于每个任务，我们会在将PPNet $$O_{pp}^l$$插入到每个DNN task tower的第l个layer中，来增强任务个性化的先验信息**，如下所示：

$$
O_{pp}^{(l)} = \sigma_{task}^{(l)} \otimes H^{(l)} \\
O_{pp}^{(l+1)} = f(O_{pp}^{(l)} W^{(l)} + b^{(l)}), l \in \lbrace 1, \cdots, n \rbrace
$$

...(9)

其中：

- n是每个task tower的DNN layers的数目
- f是activation function。

对于第n-1个layers，activation function f会使用Relu。最后一个layer是Sigmoid，它没有放大系数$$\gamma$$，这与Gate NU不同。在获得last layer上的多个domains上的多个targets的预估得分后，会使用binary cross-entropy进行最优化。

## 2.3 工程优化策略

为了部署PEPNet，我们做出以下的工程优化策略：

- Feature score消除策略：由于每个ID映射到一个embedding vector，可以快速填满服务的内存资源。为了确保系统进行长时间执行，我们设计了一个特殊参数服务器，来达到一个无冲突（conflict-free）、高效内存的全局共享embedding表（memory-efficient Global Shared Embedding Table） (GSET）。GSET使用feature score elimination策略来控制内存footprint，使得总是在一个预设的阈值之下。然而，传统的cache elimination策略使用LFU和LRU，只考虑了条目的频率信息，主要用于最大化cache命中率。

- DNN/Embedding layer Updating: 由于系统采用在线学习，它在训练时会长时间累积数据。我们将训练数据的最小单元称为一个pass，每个pass会更新online inference模型。由于存在大量users、authors以及items，这会导致user ID、item ID、author ID的features的快速膨胀。平台的一些ID features会超期或变得很冷门，因此将所有ID features进行存储是不高效的。它会盲目增大系统的冗余，带来额外的存储和计算开销。我们会增加两个特征选择策略（feature eviction）。一个是指定一个feature的特定数目，当超过时会抛弃。另一个是设置ID featrues的过期时间，保证ID features可以频繁更新，并删除那些没有达到所需更新数的ids。相似的，当模型被更新时，我们会检查相应的embedding，只更新变化的embedding。

- Training策略：由于在kuaishou的短视频场景的商业特性，ID features会快速变化。实际上，我们会发现：embedding的更新会比DNN模型参数更频繁。在线学习场景中，为了更好捕获在bottom-layer embeddings的变化，并稳定更新top-layer DNN参数，我们会分别更新embedding part和DNN参数部分，并使用不同的update策略。在bottom-layer embedding中，我们会使用AdaGrad optimizer，学习率会设置为0.05。而DNN参数会通过Adam optimizer进行更新，学习率为：5.0e-06.

# 3.离线实验

略

- 1.[https://arxiv.org/pdf/2302.01115.pdf](https://arxiv.org/pdf/2302.01115.pdf)