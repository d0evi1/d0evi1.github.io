---
layout: post
title: BST介绍
description: 
modified: 2019-05-27
tags: 
---

阿里在paper《Behavior Sequence Transformer for E-commerce
Recommendation in Alibaba》中提出了BST模型，我们可以借鉴一下：

# 1.介绍

# 2.架构

在rank stage中，我们会将推荐任务建模成CTR预测问题，它的定义如下：给定一个user点击的行为序列 $$S(u) = \lbrace v_1, v_2, \cdots, v_n \rbrace$$，我们需要学习一个函数F，来预测u点击target item $$v_t$$的概率（例如：candidate item）。其它Features包括user profile、context、item、以及cross features。

我们会在WDL之上构建BST，总体架构如图1所示。从图1中知，我们可以看到，它遵循流行的embedding & MLP范式，其中，在feed到MLP之前，过去点击的items和相关features会首先嵌入到低维vectors中。BST和WDL的关键区别是：会添加transformer layer，通过捕获底层的序列信号（underlying sequential signals）来为用户点击items学习更好的表示。在以下部分，我们会引入一个bottom-up方式的BST核心组件：embeddding layer、transformer layer、MLP。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/ec92ef167d6918363521d4177b098c41bb72719b5a717dfa4b380f8d171e9e2a05232dc9b0ff3d4c4e33ef4d0fc1aec1?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=1.jpg&amp;size=750">

图1 BST的架构总览。BST会使用用户的行为序列作为输入，包括：target item，以及"Other features"。它首先会将这些input features嵌入成低维vectors。为了更好地捕获在行为序行中的items间关系，会使用transformer layer来为该序列中的每个item学习更深的表示。接着通过将Other features的embeddings和transformer layer的output进行concatenate，使用3-layer MLP来学习hidden features间的交叉，最后使用sigmoid function来生成最终的output。注意，"Positional Features"会被包含在"Sequence Item Features"中。

## 2.1 Embedding Layer

第一个组件是embedding layer，它会将所有input features嵌入到一个fixed-size的低维vectors中。在我们的场景中，存在许多features，像：user profile features、item features、context features、以及不同features的组合（例如：cross features）。由于本工作聚焦于建模带transformer的行为序列，出于简洁性，我们会将所有这些features表示为“Other features”，并给出表1的一些示例。如图1所示，我们将图1左侧的“Other features”进行concatenate，并将它们嵌入到低维vectors中。对于这些features，我们会创建一个embedding matrix $$W_o \in R^{\mid D \mid \times d_0}$$，其中：$$d_0$$是维度size。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/c0bf2642ef2b4e3bbea45e0cbbf0d0313ca685a11b439903a87a47e59f8afaea299d62ceadc54889f985c40eafe232ea?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=t1.jpg&amp;size=750">

表1 图1左侧的"Other Features"。我们实际使用更多features，出于简洁，只列了许多有效的特征

另外，我们也会获得在行为序列中每个item的embedding。如图1所示，我们使用两种features类型来表示一个item：“Sequence Item Features”（红色）和“Positional Features”（暗蓝色），**其中：“Sequence Item Features”包括：item_id和category_id**。其中，一个item通常具有成百上千个features，在一个行为序列中选择所有features来表示该item代价太高。过往工作[15]介绍的，**item_id和category_id对于表现来说足够好**，在嵌入用户行为序列时，我们选择这两个特征作为sparse features来表示的每个item。"Positional Features"对应于以下的“positinal embedding”。接着，对于每个item，我们会将Sequence Item Features和Positional Features进行concatenate在一起，并创建一个embedding matrix：

$$
W_V \in R^{\mid V \mid \times d_V}
$$

其中：

- $$d_V$$是embedding的dimension size
- $$\mid V \mid$$是items数目

我们使用$$e_i \in R^{d_v}$$来表示在一个给定behavior sequence中的第i个item的embedding。

**Positional embedding**

**在[13]中，作者提出了一个positional embedding来捕获句子中的order信息**。同样的，在用户的行为序列中存在order。因而，在它们被投影成一个低维向量前，我们会添加“position”作为在bottom layer中每个item的一个input feature。注意，item $$v_i$$的position value会被计算成：

$$
pos(v_i) = t(v_t) - t(v_i)
$$

其中：

- $$t(v_t)$$表示推荐时间（ecommending time）
- $$t(v_i)$$是当用户点击item $$v_i$$的timestamp

我们会采用该方法，**因为在我们的场景中它的效果要好于在[13]中的sin和cos函数**。

## 2.2 Transformer layer

在这部分，我们会引入Transformer layer，它会通过捕获在行为序行中其它items的关系，来为每个item学习一个deeper表示。

**Self-attention layer**

scaled dot-product attention[13]的定义如下：

**
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d}})V
**

...(1)

其中：

- Q表示queries
- K是keys
- V是values

在我们的场景中，self-attention操作会将items的embeddings作为input，并通过线性投影将它们转成三个matrices，接着将它们feed到一个attention layer中。根据[13]，我们使用multi-head attention：

$$
S = MH(E) = Concat(head_1, head_2, \cdots, head_h) W^H \\
head_i = Attention(EW^Q, EW^K, EW^V)
$$

...(2)(3)

 其中，投影矩阵$$W^Q, W^K, W^V \in R^{d \times d}$$，E是所有items的embedding matrics，h是heads的数目.

**Point-wise Feed-Forward Network**

根据[13]，我们会将point-wise Feed-Forward Networks(FFN)来进一步增强模型的非线性（non-linearity），定义如下：

$$
F = FFN(S)
$$

...(6)

为了避免overfitting，并能层次化学习有意义的features，我们在self-attention和FFN中同时使用dropout和LeakyReLU，接着，self-attention和FFN layers的overall output如下：

$$
S' = LayerNom(S + Dropout(MH(S)) \\
F = LayerNomr(S' + Dropout(LeakyReLU(S'W^{(1)} + b^{(1)}) W^{(2)} + b^{(2)}))
$$

...(5)(6)

其中，$$W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}$$是可学习的参数，LayerNomr是标准的normalization layer。

**Stacking the self-attention block**

在经过第一个self-attention block之后，它会将所有之前的items的embeddings进行聚合，为了进一步建模在item sequences下的复杂关系，我们将self-building blocks和第b个block进行stack，定义如下：

$$
s^b = SA(F^{(b-1)}) \\
F^b = FFN(S^b), \forall i \in 1, 2, \cdots, n
$$

...(7) (8)

实际上，我们观察到，对比起b=2, 3（见表4），在我们的实验中b=1可以获取更好的效果。出于效率的目的，我们不再尝试更大的b。

## 2.3 MLP layers和loss function

通过将Other features的embeddings和应用在target item上的transfomer layer的output进行concatenate，我们接着使用三个fully connected layers来进一步学习在dense features间的交叉，它在工作界RS中是标准实现。

为了预测一个用户是否在target item $$v_t$$上有点击，我们会将它建模成一个二分类问题，接着我们使用sigmoid函数作为output unit。为了训练该模型，我们使用cross-entropy loss：

$$
L = - \frac{1}{N} \sum\limits_{(x,y) \in D} (y log p(x) + (1-y) log(1-p(x)))
$$

...(9)

其中，D表示所有的样本，$$y \in \lbrace 0, 1\rbrace$$表示用户是否点击了某个item，p(x)是在sigmoid unit之后的network的output，表示sample x被点击的预测概率。

# 3.实验

在本节中，我们会展示实验结果。

## 3.1 Settings

**Dataset**

数据集从taobao APP中构造得到。我们会构建一个8天内的基于用户行为的离线数据集。我们使用前7天作为训练数据，后一天作为test data。dataset的统计如表2所示。我们可以看到，dataset相当大与稀疏。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/ebda97f7a860890f1322a170d2acdf02d8221fac4913550b3400b87730c4117806bc9d8dbf911fe856c21ce3593d9481?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=t2.jpg&amp;size=750">

表2

**Baseline**

为了展示BST的效果，我们会使用两个模型进行对比：WDL[2]和DIN[17]。另外，我们创建了一个baseline方法，它会将sequential信息包含到WDL中，表示成WDL(+Seq)，它会以平均的方式将过去点击的items的embeddings进行聚合。我们的framework会在WDL之上进行构建，使用Transfomer添加序列建模，而DIN只会使用attention机制捕获在target item与过去点击items间的相似度。

**评估指标**

对于offline结果，我们使用AUC进行online A/B test，我们会使用CTR和平均RT来评估所有模型。TR是响应时间（response time）的简称，它表示给定一个query生成推荐结果的时间开销，例如：一个用户对taobao的一次请求。我们使用平均RT作为指标来评估在在线生产环境中的不同效率。

**Settings**

我们的模型使用python2.7+tensorflow 1.4实现，使用"adagrad"作为optimizer。另外，我们会使用表3的模型参数。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/17f372075b00d20059bb2e6455f0a95b0baf8214a1dccfe34bc448d37ea4ddb10d7a80ed6be17e2b283cc173768a56cd?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=t3.jpg&amp;size=750">

表3

## 3.2 结果分析

结果如表4所示。我们可以看到BST对比baseline的优势。特别的，离线实验的AUC提升从0.7734（WDL）和0.7866（DIN）到了0.7894（BST）。当对比WDL和WDL(+Seq)时，我们可以看到将sequential信息以简单平均方式包括其中的效果。这意味着有了self-attention的帮助，BST可以提供强大的能力来捕获在用户行为序列下的sequential signal。注意，从我们的实际经验看，**offline AUC的小增益可以导致在online CTR上的大收益**。相似的现象也在google的WDL[2]中有报告。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/0789fc389e456a16d5608e46cfad10c00cf16babb0aafb90702763c2ce3d3a66a4df36bea8557b6fd3aaafea5443c8da?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=t4.jpg&amp;size=750">

表4

另外，**除了efficiency，BST的平均RT与WDL和DIN接近，这可以确保在实际大规模RS中部署像Transformer这样的复杂模型的可行性**。

最后，我们也展示了在2.2节中对self-attention进行stacking的影响。从表4中，我们可以看到b=1可以获得最好的offline AUC。这可能会归因于这样的事实：在用户行为序列中存在顺序依赖（sequential dependency）不像在机器翻译任务中的句子那样复杂，更小数目的blocks就足够获得好的效果。在[7]中有类似的观察。因此，我们选择b=1来在生产环境中部署BST，表4只上报了b=1的online CTR gain。

# 4.相关工作

在本节，我们简单回顾了在deep learning CTR方法的相关工作。由于WDL的提出【2】，提出了一系列工作来使用deep learning-based方法来提升CTR，比如：DeepFM、XDeepFM、Deep&Cross networks【16】等。然而，所有这些之前的工作主要关注于特征组合（feature combinations）或者neural network的不同结构，忽略了在真实推荐场景中用户行为序列的顺序特性。最近，DIN提出attention机制来处理用户行为序列。我们的模型与DIN的不同之处是，使用Transformer来处理在用户行为序列中每个item的一个更深表示，而DIN只会捕获与之前点击的items与target item间的不同相似性。换句话说，使用transformer的模型更合适捕获序列信号（sequential signals）。在[7,12]中，transformer模型提出以seq-to-seq的方式来解决sequential推荐问题，它们的架构与我们的CTR预测不同。

# 5.结论

本paper中，我们呈现了使用transfomer到taobao推荐中的技术细节。通过使用捕获sequential关系的强大能力，我们通过大量实验展示了在建模用户行为序列中transfomer的优越性。另外，我们也呈现了taobao在生产环境上的部署细节。

# 参考

- 1.[https://arxiv.org/pdf/1905.06874.pdf](https://arxiv.org/pdf/1905.06874.pdf)