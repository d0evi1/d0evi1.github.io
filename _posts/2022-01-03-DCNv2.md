---
layout: post
title: DCNv2介绍
description: 
modified: 2017-11-12
tags: [DCN]
---

google在《DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems》提出了一个DCNv2的改进模型：

# 摘要

学习有效的特征交叉是构建推荐系统的关键。然而，稀疏且庞大的特征空间需要**穷尽搜索来识别有效的交叉**。深度交叉网络（DCN）被提出以自动且高效地学习有界阶(bounded-degree)的预测性特征交互。不幸的是，在为**数十亿训练样本**提供服务的Web规模流量模型中，DCN在其交叉网络中表现出有限的表现力，难以学习更具预测性的特征交互。尽管取得了显著的研究进展，但许多生产中的深度学习模型仍然**依赖传统的前馈神经网络**低效地学习特征交叉。

鉴于DCN的优缺点以及现有的特征交互学习方法，我们提出了一个改进的框架DCN-V2，以使DCN在大规模工业环境中更加实用。通过广泛的超参数搜索和模型调优的全面实验研究，我们观察到DCN-V2在流行的基准数据集上超越了所有最先进的算法。改进后的DCN-V2更具表现力，同时在特征交互学习中保持成本效益，特别是与**低秩架构**结合时。DCN-V2简单易用，可以作为构建块轻松采用，并在Google的许多Web规模学习排序系统中实现了显著的离线准确性和在线业务指标提升。

# 1 引言

学习排名（LTR）[4, 27] 仍然是现代机器学习和深度学习中最重要的问题之一。它在搜索、推荐系统[17, 39, 41]和计算广告[2, 3]等领域有着广泛的应用。在LTR模型的关键组件中，学习有效的特征交叉继续吸引着学术界[26, 35, 46]和工业界[1, 6, 13, 34, 50]的大量关注。

有效的特征交叉对许多模型的成功至关重要。它们提供了超出单个特征的额外交互信息。例如，“国家”和“语言”的组合比其中任何一个都更有信息量。在线性模型时代，机器学习从业者依靠手动识别这些特征交叉[43]来增加模型的表达能力。不幸的是，这在web规模应用中涉及一个庞大而稀疏的组合搜索空间，数据大多是类别型的。在这样的环境下进行搜索是费力的，通常需要领域专业知识，并使模型更难泛化。

后来，嵌入技术被广泛采用，将高维稀疏向量投影到**低维dense向量**。因子分解机（FMs）[36, 37]利用嵌入技术并通过两个潜向量的内积构建**成对特征交互**。与线性模型中的传统特征交叉相比，FM具有更强的泛化能力。

在过去十年中，随着更多的计算能力和海量数据的出现，工业界的LTR模型逐渐从线性模型和基于FM的模型迁移到**深度神经网络（DNN）**。这显著提高了搜索和推荐系统的模型性能[6, 13, 50]。人们普遍认为DNN是**通用函数逼近器**，可以潜在地学习各种特征交互[31, 47, 49]。然而，**最近的研究[1, 50]发现，DNN甚至近似建模二阶或三阶特征交叉都是低效的**。

为了更准确地捕捉有效的特征交叉，常见的补救措施是：**通过更宽或更深的网络进一步增加模型容量**。这自然是一把双刃剑，我们在提高模型性能的同时使模型变得更慢。在许多生产环境中，这些模型处理极高的查询每秒（QPS），因此对实时推理有非常严格的延迟要求。可能，服务系统已经被推到极限，无法承受更大的模型。此外，更深层次的模型通常会引入可训练性问题，使模型更难训练。

这凸显了设计一个能够高效且有效地学习预测性特征交互的模型的关键需求，特别是在资源受限的环境中处理来自数十亿用户的实时流量。许多最近的工作[1, 6, 13, 26, 34, 35, 46, 50]试图解决这一挑战。共同的主题是：**利用从DNN中学到的隐式高阶交叉，以及在线性模型中被发现有效的显式且有界阶的特征交叉**。隐式交叉意味着交互是通过端到端函数学习的，没有任何明确的公式来建模这种交叉。另一方面，显式交叉是通过一个具有可控交互阶数的明确公式建模的。我们将在第2节详细讨论这些模型。

在这些工作中，深度交叉网络（DCN）[50]是有效且优雅的，但在大规模工业系统中实现DCN面临许多挑战。其**交叉网络的表达能力受限的**。由交叉网络产生的**多项式类由$O(input\ size)$个参数表征**，大大限制了其在建模**随机交叉模式(random cross pattern)**方面的灵活性。此外，交叉网络和DNN之间的分配能力不平衡。当将DCN应用于大规模生产数据时，这种差距显著增加。大部分参数将用于在DNN中学习隐式交叉。

在本文中，我们提出了一个新的模型DCN-V2，以改进原始的DCN模型。我们已经成功地在谷歌的许多学习排名系统中部署了DCN-V2，并在离线模型准确性和在线业务指标方面取得了显著收益。DCN-V2首先通过交叉层学习输入的显式特征交互（通常是嵌入层），然后与深度网络结合学习互补的隐式交互。**DCN-V2的核心是交叉层**，它继承了DCN交叉网络的简单结构，但**在学习显式且有界阶(bounded-degree)的交叉特征方面显著更具表达能力**。本文研究了以点击为正标签的数据集，但DCN-V2是标签无关的，可以应用于任何学习排名系统。本文的主要贡献有五个方面：

- 我们提出了一种新颖的模型——DCN-V2，用于学习有效的显式和隐式特征交叉。与现有方法相比，我们的模型更具表达能力，同时保持高效和简单。
- 观察到DCN-V2中学习矩阵的低秩特性，我们提出利用低秩技术在一个子空间中近似特征交叉，以实现更好的性能和延迟权衡。此外，我们提出了一种基于**混合专家架构**[19, 45]的技术，将**矩阵进一步分解为多个较小的子空间**。这些子空间通过**门控机制**聚合在一起。
- 我们进行了广泛的实验研究，使用合成数据集展示了**传统ReLU基神经网络在学习高阶特征交叉时的低效性**。
- 通过全面的实验分析，我们证明了我们提出的DCN-V2模型在Criteo和MovieLen-1M基准数据集上显著优于最先进的算法。
- 我们提供了一个案例研究，并分享了在大规模工业排序系统中实现DCN-V2的经验教训，这些经验带来了显著的离线和在线收益。

本文的组织结构如下。第2节总结了相关工作。第3节描述了我们提出的模型架构DCN-V2及其内存高效的版本。第4节分析了DCN-V2。第5节提出了几个研究问题，这些问题在第6节的合成数据集和第7节的公共数据集上的全面实验中得到了回答。第8节描述了在大规模web推荐系统中实现DCN-V2的过程。

2 相关工作

近期特征交互学习工作的核心思想是利用显式和隐式（来自DNNs）的特征交叉。为了建模显式交叉，大多数近期工作引入了**乘法操作（$x_1 \times x_2$）**，这在DNN中效率不高，并设计了一个函数$f(x_1,x_2)$来有效地显式建模特征$x_1$和$x_2$之间的成对交互。我们根据它们如何结合显式和隐式组件来组织工作。

**并行结构**。一条工作线受到wide&deep模型[6]的启发，联合训练两个并行的网络，其中wide组件接受原始特征的交叉作为输入；而deep组件是一个DNN模型。然而，为wide组件选择交叉特征又回到了线性模型的特征工程问题。尽管如此，wide&deep模型已经**激发了许多工作采用这种并行架构并改进wide组件**。

**DeepFM**[13]通过采用FM模型自动化了wide组件中的特征交互学习。DCN[50]引入了一个交叉网络，该网络自动且高效地学习显式且有界的特征交互。xDeepFM[26]通过生成多个特征图增加了DCN的表达能力，每个特征图编码当前级别和输入级别之间所有特征对的交互。此外，它还将每个特征嵌入$x_{i}$视为一个单元，而不是将其元素$x_{i}$视为一个单元。不幸的是，其计算成本显著较高（参数的10倍），这使得它在工业规模的应中不切实际。此外，**DeepFM和xDeepFM都需要所有特征嵌入的大小相等**，这是另一个在应用于工业数据时的限制，因为词汇表大小（分类特征的大小）从O(10)到数百万不等。

**AutoInt**[46]利用多头自注意力机制和残差连接。InterHAt[25]进一步采用了分层注意力。

**堆叠结构**。另一条工作线在嵌入层和DNN模型之间引入了一个交互层，该层创建**显式的特征交叉。这个交互层在早期阶段捕获特征交互**，有助于后续隐藏层的学习。Product-based neural network(PNN)[35]引入了内积(IPNN)和外积(OPNN)层作为成对交互层。OPNN的一个缺点是其计算成本高。Neural FM(NFM)[16]通过用Hadamard积替换内积扩展了FM；DLRM[34]遵循FM通过内积计算特征交叉；**这些模型只能创建最多2阶的显式交叉**。AFN[7]将特征转换到对数空间并自适应地学习任意阶的特征交互。类似于DeepFM和xDeepFM，它们只接受大小相等的嵌入。

尽管取得了许多进展，但我们广泛的实验（第7节）表明DCN仍然是一个强大的基线。我们认为这归功于其简单的结构，这有助于优化。然而，正如所讨论的，其有限的表达能力阻止了它在网络规模系统中学习更有效的特征交叉。接下来，我们将介绍一种新的架构，它继承了DCN的简单结构，同时增加了其表达能力。

# 3 提出的架构：DCN-V2

本节描述了一种新颖的模型架构(DCN-V2)，用于学习显式和隐式特征交互。DCN-V2从一个嵌入层开始，接着是一个**包含多个交叉层的交叉网络**，该网络对显式特征交互进行建模，然后与一个深度网络结合，对隐式特征交互进行建模。DCN-V2中的改进对于将DCN投入高度优化的生产系统至关重要。DCN-V2显著提高了DCN[50]在建模复杂显式交叉项方面的表达能力，同时保持了其优雅的公式以便于部署。DCN-V2研究的函数类是DCN所建模的函数类的严格超集。总体模型架构如图1所示，有两种方式将交叉网络与深度网络结合：（1）堆叠和（2）并行。此外，观察到交叉层的低秩特性，我们提出利用低秩技术的混合来在模型性能和效率之间实现更健康的权衡。


图1 DCN-V2的可视化图示。$\odot$表示公式(1)中的交叉运算，即：$x_{𝑙+1} = x_0 \odot (𝑊_𝑙 x_𝑙 + b_𝑙) + x_𝑙$。

## 3.1 嵌入层

嵌入层接收类别型（sparse）特征和dense特征的组合输入，输出$\mathbf{x}_0 \in \mathbb{R}^d$。对于第$i$个类别型特征，我们通过$\mathbf{x}_{\text{embed},i} = W_{\text{embed},i}\mathbf{e}_i$将其从高维稀疏空间投影到低维稠密空间，其中：

- $\mathbf{e}_i \in \{0,1\}^{v_i}$为one-hot向量
- $W \in \mathbb{R}^{e_i \times v_i}$为可学习投影矩阵  
- $\mathbf{x}_{\text{embed},i} \in \mathbb{R}^{e_i}$为稠密嵌入向量
- $v_i$和$e_i$分别表示词表大小和嵌入维度

对于多值特征，我们使用嵌入向量的均值作为最终向量。输出层将所有嵌入向量与归一化后的稠密特征拼接：
$$
\mathbf{x}_0 = [\mathbf{x}_{\text{embed},1}; \ldots; \mathbf{x}_{\text{embed},n}; \mathbf{x}_{\text{dense}}]
$$

与许多需要$e_i = e_j \forall i,j$的研究[13,16,26,34,35,46]不同，我们的模型支持任意嵌入维度。这对工业级推荐系统尤为重要——词表规模可能从$O(10)$到$O(10^5)$不等。此外，我们的框架不仅限于上述嵌入方法，也可兼容哈希等其他嵌入技术。

## 3.2 Cross网络

DCN-V2的核心在于显式构建特征交叉的交叉层。第$(l+1)$层交叉层计算如式(1)所示：
$$
\mathbf{x}_{l+1} = \mathbf{x}_0 \odot (W_l\mathbf{x}_l + \mathbf{b}_l) + \mathbf{x}_l \tag{1}
$$
其中：
- $\mathbf{x}_0 \in \mathbb{R}^d$是包含原始一阶特征的基底层，通常设为嵌入层输入
- $\mathbf{x}_l, \mathbf{x}_{l+1} \in \mathbb{R}^d$分别表示第$(l+1)$层的输入和输出
- $W_l \in \mathbb{R}^{d \times d}$和$\mathbf{b}_l \in \mathbb{R}^d$为可学习参数


图2 [交叉层可视化图示]

对于$l$层交叉网络，最高多项式阶数为$l+1$，且包含直至最高阶的所有特征交叉组合（详见第4.1节从比特级和特征级的分析）。当$W = \mathbf{1}\mathbf{w}^\top$（$\mathbf{1}$为全1向量）时，DCN-V2退化为DCN。交叉层只能生成有界阶数的多项式函数类，其他复杂函数空间仅能近似1。因此我们引入深度网络来补充建模数据内在分布。

## 3.3 Deep网络

第$l$层深度网络计算公式为：
$$
\mathbf{h}_{l+1} = f(W_l\mathbf{h}_l + \mathbf{b}_l)
$$
其中：
- $\mathbf{h}_l \in \mathbb{R}^{d_l}$, $\mathbf{h}_{l+1} \in \mathbb{R}^{d_{l+1}}$分别为第$l$层的输入输出
- $W_l \in \mathbb{R}^{d_l \times d_{l+1}}$为权重矩阵
- $\mathbf{b}_l \in \mathbb{R}^{d_{l+1}}$为偏置向量
- $f(\cdot)$采用ReLU激活函数（其他激活函数也适用）

## 3.4 Deep & Cross组合结构

我们探索两种组合结构：
1. **堆叠结构**（图1a）：输入$\mathbf{x}_0$先经过交叉网络再输入深度网络，最终层$\mathbf{x}_{\text{final}} = \mathbf{h}_{L_d}$, $\mathbf{h}_0 = \mathbf{x}_{L_c}$，建模方式为$f_{\text{deep}} \circ f_{\text{cross}}$
2. **并行结构**（图1b）：输入$\mathbf{x}_0$并行输入两个网络，最终拼接输出$\mathbf{x}_{\text{final}} = [\mathbf{x}_{L_c}; \mathbf{h}_{L_d}]$，建模方式为$f_{\text{cross}} + f_{\text{deep}}$

预测值$\hat{y}_i$计算为：

$$
\hat{y}_i = \sigma(\mathbf{w}_{\text{logit}}^\top \mathbf{x}_{\text{final}})
$$

其中:

- $\mathbf{w}_{\text{logit}}$为logit权重向量
- $\sigma(x)=1/(1+\exp(-x))$

损失函数采用排序学习系统中常用的对数损失（适用于二分类如点击预测）：

$$
\text{loss} = -\frac{1}{N}\sum_{i=1}^N y_i\log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i) + \lambda\sum_l \|W_l\|_2^2
$$

其中：

- $\hat{y}_i$为预测值
- $y_i$为真实标签
- $N$为样本总数
- $\lambda$为$L_2$正则化系数

## 3.5 高性价比的低秩DCN混合架构

在实际生产模型中，模型容量常受限于计算资源与严格的延迟要求。我们通常需要在保持精度的前提下**降低成本**。低秩技术[12]被广泛用于降低计算成本[5,9,14,20,51,52]，其**通过两个瘦高矩阵$U,V \in \mathbb{R}^{d \times r}$来近似稠密矩阵$M \in \mathbb{R}^{d \times d}$**。当$r \leq d/2$时可降低计算成本，但**该技术仅在矩阵在奇异值（singular values）上存在较大间隙（gap）或快速谱衰减时最有效**。实践中我们确实观察到学习到的矩阵具有数值低秩特性。

图3a展示了生产模型中DCN-V2（式(1)）权重矩阵$W$的奇异值衰减模式。与初始矩阵相比，学习后的矩阵呈现更快的谱衰减。定义容忍度为$T$的数值秩$R_T$为$\text{argmin}_k (\sigma_k < T \cdot \sigma_1)$，其中$\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_n$为奇异值。在机器学习领域，即使容忍度$T$较高，模型仍能保持优异性能$^2$。

图3：左图：学习到的DCN-v2权重矩阵奇异值衰减（+表示随机初始化矩阵，×表示训练后矩阵）；右图：低秩混合交叉层结构

因此对$W$施加低秩结构具有充分动机。式(2)给出第$(l+1)$层低秩交叉层的计算：

$$
\mathbf{x}_{l+1} = \mathbf{x}_0 \odot \left( U_l V_l^\top \mathbf{x}_i + \mathbf{b}_l \right) + \mathbf{x}_i \tag{2}
$$

其中$U_l, V_l \in \mathbb{R}^{d \times r}$且$r \ll d$。该式有两种解释：

- 1. 在子空间中进行特征交叉学习
- 2. 将输入$\mathbf{x}$投影到低维空间$\mathbb{R}^r$后再重构回$\mathbb{R}^d$

这两种解释启发了以下改进：

**解释1** 启发我们采用**混合专家(MoE)**[10,19,30,45]思想。不同于单一专家（式(2)），我们使用多个专家在不同子空间学习特征交叉，并**通过门控机制动态组合**。式(3)和图3b展示了低秩混合交叉层：

$$
\mathbf{x}_{l+1} = \sum_{i=1}^K G_i(\mathbf{x}_l)E_i(\mathbf{x}_l) + \mathbf{x}_l \\
E_i(\mathbf{x}_l) = \mathbf{x}_0 \odot \left( U_l^i V_l^{i\top} \mathbf{x}_l + \mathbf{b}_l \right) \tag{3}
$$

其中：
- $K$为专家数量
- $G_i(\cdot): \mathbb{R}^d \mapsto \mathbb{R}$为门控函数（常用sigmoid/softmax）
- $E_i(\cdot): \mathbb{R}^d \mapsto \mathbb{R}^d$为第$i$个特征交叉专家
当$G(\cdot) \equiv 1$时，式(3)退化为式(2)

**解释2** 启发我们利用**投影空间的低维特性**。在从$d'$维重构到$d$维（$d' \ll d$）之前，先在投影空间进行非线性变换以精炼表示[11]：

$$
E_i(\mathbf{x}_l) = \mathbf{x}_0 \odot \left( U_l^i \cdot g\left( C_l^i \cdot g\left( V_l^{i\top} \mathbf{x}_l \right) \right) + \mathbf{b}_l \right) \tag{4}
$$

其中：

- $g(\cdot)$为任意非线性激活函数。

**讨论** 

本节核心是在固定内存/时间预算下有效学习特征交叉。从式(1)到式(4)，每个公式在参数量固定的情况下都定义了更丰富的函数类。

不同于多数在训练后进行的模型压缩技术，我们的方法在训练前就植入结构先验，并联合学习所有参数。由于交叉层是下面非线性系统的一个组成部分：

$$
f(\mathbf{x}) = (f_k(W_k) \circ \cdots \circ f_1(W_1))(\mathbf{x})
$$

其中：

$$
(𝑓_{𝑖+1} \circ 𝑓_𝑖)(\cdot) := 𝑓_{𝑖+1}(𝑓_𝑖(\cdot))
$$

其整体系统的训练动态性会受影响，如果你感兴趣，可以观察全局统计量（如Jacobian和Hessian矩阵）是如何被影响的，相关研究将留待未来工作。

## 3.6 复杂度分析

设：

- d表示嵌入大小，
- $L_{c}$ 表示交叉层数，
- K表示低秩DCN专家数。

此外，为简单起见，我们假设每个专家都有相同的较小维度r（秩的上限）。交叉网络的时间和空间复杂度为 $O\left(d^{2} L_{c}\right)$，而对于低秩DCN混合（DCN-Mix），当 $r K\ll d $ 时，它是高效的，复杂度为 $O\left(2 d r K L_{C}\right)$。

# 4 模型分析

本节从多项式逼近的角度分析DCN-V2，并与相关工作建立联系。我们采用文献[50]中的符号体系。

## 符号定义

设嵌入向量 $\mathbf{x} = [x_1; x_2; \ldots; x_k] = [x_1, x_2, \ldots, x_d] \in \mathbb{R}^d$ 为列向量，其中 $x_i \in \mathbb{R}^{e_i}$ 表示第$i$个特征嵌入，$x_i$ 表示$\mathbf{x}$中的第$i$个元素。定义多重指标 $\boldsymbol{\alpha} = [\alpha_1, \ldots, \alpha_d] \in \mathbb{N}^d$，且 $|\boldsymbol{\alpha}| = \sum_{i=1}^d \alpha_i$。

定义全1向量 $\mathbf{1}$ 和单位矩阵 $\mathbf{I}$。矩阵用大写字母表示，粗体小写字母表示向量，普通小写字母表示标量。

## 4.1 多项式逼近

我们从两个角度分析DCN-V2的多项式逼近特性：
1) 将每个元素(比特)$x_i$视为单位，分析元素间交互(Theorem 4.1)
2) 将每个特征嵌入$x_i$视为单位，仅分析特征间交互(Theorem 4.2)

### 定理4.1 (比特级)

假设$l$层交叉网络的输入为$\mathbf{x} \in \mathbb{R}^d$，输出为$f_l(\mathbf{x}) = \mathbf{1}^\top \mathbf{x}_l$，第$i$层定义为：
$$\mathbf{x}_i = \mathbf{x} \odot \mathbf{W}^{(i-1)}\mathbf{x}_{i-1} + \mathbf{x}_{i-1}$$

则多元多项式$f_l(\mathbf{x})$可表示为以下多项式类：
$$\left\{ \sum_{\boldsymbol{\alpha}} c_{\boldsymbol{\alpha}} \left( \mathbf{W}^{(1)}, \ldots, \mathbf{W}^{(l)} \right) x_1^{\alpha_1} x_2^{\alpha_2} \ldots x_d^{\alpha_d} \;\Bigg|\; 0 \leq |\boldsymbol{\alpha}| \leq l+1, \boldsymbol{\alpha} \in \mathbb{N}^d \right\}$$

其中系数$c_{\boldsymbol{\alpha}}$为：
$$c_{\boldsymbol{\alpha}} = \sum_{j \in C_{|\boldsymbol{\alpha}|-1}^l} \sum_{i \in P_{\boldsymbol{\alpha}}} \prod_{k=1}^{|\boldsymbol{\alpha}|-1} w_{jk}^{(i_k i_{k+1})}$$
这里$w_{jk}^{(k)}$表示矩阵$\mathbf{W}^{(k)}$的第$(i,j)$个元素，$P_{\boldsymbol{\alpha}}$是满足$\cup_i \{i, \ldots, i_{\alpha_i}\}$（$\alpha_i \neq 0$）的所有排列组合。

### 定理4.2 (特征级)

在相同设定下，假设输入$\mathbf{x} = [x_1; \ldots; x_k]$包含$k$个特征嵌入，将每个$x_i$视为单位。则$l$层交叉网络的输出$\mathbf{x}_l$能生成所有阶数不超过$l+1$的特征交互。具体地，对于特征索引集合$I$中的特征（允许重复索引），设$P_I = \text{Permutations}(I)$，则其$p$阶交互为：
$$\sum_{i \in P_I} \sum_{j \in C_{p-1}} x_{i_1} \odot \left( \mathbf{W}^{(j_1)}_{i_1,i_2} x_{i_2} \odot \ldots \odot \mathbf{W}^{(j_k)}_{i_k,i_{k+1}} x_{i_{l+1}} \right)$$

从比特级和特征级两个角度看，$l$层交叉网络都能生成阶数不超过$l+1$的所有特征交互。相比DCN-V，DCN-V2用更多参数刻画了相同的多项式类，具有更强的表达能力。此外，DCN-V2的特征交互既可以从比特级也可以从特征级进行解释，而DCN仅支持比特级交互[26,46,50]。

## 4.2 与相关工作对比

我们研究DCN-V2与其他SOTA特征交互学习方法的联系，仅关注各模型的特征交互组件，忽略DNN部分。为便于比较，假设所有特征嵌入维度相同为$e$。

### DCN

我们的模型主要受DCN[50]启发。从高效投影视角看，DCN隐式生成所有成对交叉后投影到低维空间；DCN-V2采用不同的投影结构：

$$
\mathbf{x}^\top_{\text{DCN}} = \mathbf{x}_{\text{pairs}} 
\begin{bmatrix}
w_0 & 0 & \cdots & 0 \\
0 & w & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & w_d
\end{bmatrix}, \quad
\mathbf{x}^\top_{\text{DCN-V2}} = \mathbf{x}_{\text{pairs}}
\begin{bmatrix}
w_1 & 0 & \cdots & 0 \\
0 & w_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & w_d
\end{bmatrix}
$$

其中$\mathbf{x}_{\text{pairs}} = [x_i \tilde{x}_j]_{\forall i,j}$包含$x_0$与$\tilde{x}$间所有$d^2$个成对交互；$\mathbf{w} \in \mathbb{R}^d$是DCN-V的权重向量，$\mathbf{w}_i \in \mathbb{R}^d$是DCN-V2权重矩阵的第$i$列(式(1))。

### DLRM与DeepFM

二者本质上是去掉DNN组件的二阶FM(忽略微小差异)。因此我们简化分析与FM比较，其公式为：
$$\mathbf{x}^\top \boldsymbol{\beta} + \sum_{i<j} w_{ij} \langle x_i, x_j \rangle$$

这等价于无残差项的1层DCN-V2(式(1))配合结构化权重矩阵：
$$
\mathbf{1}^\top 
\begin{bmatrix}
0 & w_{12}\mathbf{I} & \cdots & w_{1k}\mathbf{I} \\
0 & 0 & \cdots & w_{2k}\mathbf{I} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_k
\end{bmatrix}
+
\begin{bmatrix}
\beta_1 \\ \beta_2 \\ \vdots \\ \beta_k
\end{bmatrix}
$$

### xDeepFM

第$k$层第$h$个特征图定义为：
$$x_{k,h,*} = \sum_{i=1}^{k-1} \sum_{j=1}^m w_{k,h}^{ij} (x_{k-1,i,*} \odot x_j)$$

第1层第$h$个特征图等价于无残差项的1层DCN-V2：
$$x_{1,h,*} = [\mathbf{I}, \mathbf{I}, \ldots, \mathbf{I}] (\mathbf{x} \odot (\mathbf{W}\mathbf{x})) = \sum_{i=1}^k x_i \odot (\mathbf{W}_{i,:}\mathbf{x})$$
其中$(i,j)$块$\mathbf{W}_{i,j} = w_{ij}\mathbf{I}$，$\mathbf{W}_{i,:} = [W_{i,1}, \ldots, W_{i,k}]$。

### AutoInt

交互层采用多头自注意力机制。为简化分析，假设使用单头注意力(多头情况可通过拼接交叉层比较)。

从高层视角看，AutoInt第1层输出$\mathbf{e}_x = [e_{x1}; e_{x2}; \ldots; e_{xk}]$，其中$e_{xi}$编码第$i$个特征的所有二阶交互。然后将$\mathbf{e}_x$输入第2层学习高阶交互，这与DCN-V2相同。

从底层视角(忽略残差项)：
$$
e_{xi} = \text{ReLU} \sum_{j=1}^k \frac{\exp(\langle \mathbf{W}_q x_i, \mathbf{W}_k x_j \rangle)}{\sum_j \exp(\langle \mathbf{W}_q x_i, \mathbf{W}_k x_j \rangle)} (\mathbf{W}_v x_j)
= \text{ReLU} \sum_{j=1}^k \text{softmax}(x_i^\top \mathbf{W}_f x_j) \mathbf{W}_v x_j
$$
其中$\langle\cdot,\cdot\rangle$表示内积，$\mathbf{W}_e = \mathbf{W}_q\mathbf{W}_k$。而在DCN-V2中：
$$e_{xi} = \sum_{j=1}^k x_i \odot (\mathbf{W}_{i,j} x_j) = x_i \odot (\mathbf{W}_{i,:}\mathbf{x})$$
(式(5))，其中$\mathbf{W}_{i,j}$表示$\mathbf{W}$的第$(i,j)$块。显然差异在于特征交互的建模方式：AutoInt认为非线性来自ReLU(·)，而我们认为每个求和项都有贡献；DCN-V2采用$x_i \odot \mathbf{W}_{i,j}x_j$的形式。

### PNN

内积版本(IPNN)与FM类似。外积版本(OPNN)先显式生成所有$d^2$个成对交互，再用$d' \times d^2$稠密矩阵投影到低维空间$d'$。与之不同，DCN-V2通过结构化矩阵隐式生成交互。


# 参考

[https://arxiv.org/pdf/2008.13535](https://arxiv.org/pdf/2008.13535)