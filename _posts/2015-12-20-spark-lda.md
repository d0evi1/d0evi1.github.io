---
layout: post
title: mllib中lda源码分析(一)
description: 
modified: 2015-12-20
tags: [lda]
---

在对MLlib中的lda进行源码分析之前，我们先熟悉下blei的LDA paper以及相关的概念。

# 介绍

首先，先做下约定。

- 一个**词(word)**是离散数据是基本单位，被定义成一个来自词汇表的item，由{1,...,V}进行索引。我们瘵词表示成one-hot格式。这样，使用上标来表示components，在词汇表中第v个词被表示成一个V维的向量w，其中<img src="http://www.forkosh.com/mathtex.cgi?w^v=1, w^u=0(u\neqv)">。
- 一个**文档(document)**是一个N个词组成的序列，表示成**W=(w1,w2,...,wN)**，其中wn是序列上的第n个词。
- 一个**语料(corpus)**是一个关于M个文档的集合，被表示成**D={W1,W2,...,Wm}**.

(blei论文中用黑体的w表示文档，为便于区分，此处用大写W)。

我们希望找到一个关于该语料的概率模型，它不仅可以将高概率分配给该语料的组成文档，还可以将高概率分配给其它“相似”文档。

# LDA

LDA是一个关于语料的**生成概率模型**。基本思想是，文档被表示成在隐主题(latent topics)上的随机混合，其中每个主题(topic)都以一个在词(words)上的分布进行表示。

对于在语料D中的每个文档W，LDA假设如下的生成过程：

- 1.选择 N ~ Poisson(ξ)
- 2.选择 θ ~ Dir(α)
- 3.对于每个文档（N个词汇wn）: 
	- (a) 选择一个主题zn ~ Multinomial(θ)
	- (b) 再在主题zn的基础上，从一个多项分布概率（multinomial probability）：<img src="http://www.forkosh.com/mathtex.cgi?P(w_n| z_n, \beta)">上选中一个词wn

在该基础模型上，做了一些简化假设。

- 首先，Dirichlet分布的维度k(主题变量z的维度)，假设是已知并且确定的。
- 第二，词概率由一个k x V的矩阵β进行参数化，其中<img src="http://www.forkosh.com/mathtex.cgi?\beta_{ij}=p(w^j=1|z^i=1)">, 目前当成是一个待估计的确定量。
- 最后，Poisson猜想是不严格的，可以使用更多的实际文档长度分布。注意N对于所有其它数据生成变量(θ和z)是独立的。它是个辅助变量，我们通常忽略它的随机性。

一个k维的Dirichlet随机变量θ，它的取值为(k-1)-simplex，(一个k维向量θ，它在泛化三角形(k-1)-simplex之内，其中：<img src="http://www.forkosh.com/mathtex.cgi?\theta_i \geq 0, \sum_{i=1}^k\theta_i=1">) ，在该simplex上具有以下的概率密度：

<img src="http://www.forkosh.com/mathtex.cgi?p(\theta|\alpha)=\frac{\Gamma(\sum_{i=1}^{k}\alpha_{i})}{\prod_{i=1}^{k}\Gamma(\alpha_{i})}{\theta_{1}}^{\alpha_{1}-1}...{\theta_{k}}^{\alpha_{k}-1}">    ......(1)

其中，**参数α是一个k维的vector，相应的components上: αi > 0**, 其中 Γ(x)为Gamma函数。Dirichlet是在simplex上的一个合适分布——它在指数族内，具有有限维的充分统计量（finite dimensional sufficient statistics），与multinomial分布共轭。在paper第5部分，这些属性将有助于LDA的inference和parameter estimation算法。

给定参数 α 和 β，我们可以给出**关于一个主题混合θ，一个关于N个主题的z集合(每个词都有一个主题)，一个N个词W的的联合分布**：

<img src="http://www.forkosh.com/mathtex.cgi?p(\theta,z,W|\alpha,\beta)=p(\theta|\alpha) \prod_{n=1}^{N}p(z_n|\theta)p(w_n|z_n,\beta)">    ......(2)

其中<img src="http://www.forkosh.com/mathtex.cgi?p(z_n|\theta)">可以简单认为：θi对于唯一的i，有<img src="http://www.forkosh.com/mathtex.cgi?{z_{n}}^{i}=1">。在θ上积分(θ上连续)，并在z上进行求和（z上离散），我们可以得到一个**文档的边缘分布（marginal distribution）**：

<img src="http://www.forkosh.com/mathtex.cgi?p(W|\alpha,\beta)=\int p(\theta|\alpha)(\prod_{n=1}^{N}\sum_{z_n}p(z_n|\theta)p(w_n|z_n,\beta))d\theta">    ......(3)

最后，将所有单个文档（document）的边缘分布进行连乘运算，我得到**整个语料（corpus）的概率**：

<img src="http://www.forkosh.com/mathtex.cgi?p(D|\alpha,\beta)=\prod_{d=1}^{M}\int p(\theta_{d}|\alpha)(\prod_{n=1}^{N}\sum_{z_{dn}}p(z_n|\theta)p(w_n|z_n,\beta))d\theta_{d}">

LDA可以表示成图1所示的概率图模型。有三个级别的LDA表述。

- **语料级参数**：α 和 β是语料级参数（corpus-level parameters），假设在生成一个语料的过程中只抽样一次。
- **文档级变量**：θd是文档级变量（document-level variable），每个文档抽样一次。
- **词级别变量**：<img src="http://www.forkosh.com/mathtex.cgi?z_{dn}">和<img src="http://www.forkosh.com/mathtex.cgi?w_{dn}">是词级别变量（word-level variables），对于每个文档中的每个词抽样一次。

将LDA与一个简单的Dirichlet-multinomial clustering模型相区分很重要。一个经典的clustering模型将涉及到一个两层模型(two-level)，对于一个语料只抽样一次Dirichlet；对于语料中的每个文档，只选一次multinomial clustering变量；对于在基于该cluster变量条件的文档上只选择一个词集合。有了许多clustering models后，这样的模型会限制一个文档与单个主题相关联。而LDA涉及三层(three-level)，尤其是主题节点(topic node)在单个文档中被重复抽样。在该模型下，文档可以与多个主题相关联。

<img src="http://pic.yupoo.com/wangdren23/GqFMQRBe/medish.jpg">

图一：LDA的图形化模型表示。"plates"表示可重复。outer plate表示文档，inner plate表示在单个文档中关于主题和词汇的可重复的选择。

通常在贝叶斯统计建模（Bayesian statistical modeling）中学到与图1相似的结构。它们被称为层级模型（hierarchical models），或更精确地称为：条件独立层级模型（con-ditionally independent hierarchical models）(Kass and Steffey, 1989).这样的模型经常作为参数期望贝叶斯模型（parametric empirical Bayes models）被提到，它也被用于参数估计中。在第5部分，我们将采用经验贝叶斯方法（empirical Bayes approach），来估计在LDA的简单实现中的α 和 β, 我们也会考虑更完整的Bayesian方法。

## 2.1 几何学解释

LDA与其它隐主题模型间不同，可以通过隐空间(latent space)的几何表示来解释。来看下在每种模型下，一个文档在几何上是如何表示的。

上面所描述的所有4种模型（unigram, mixture of unigrams, pLSI, LDA），都是在词的空间分布上操作。每个这样的分布，可以看成是在(V-1)-simplex上的一个点，我们称之为word simplex。

unigram模型是在word simplex上找到单个点，假定语料中的所有词都来自该分布。隐变量模型（letent variable models）会考虑在word simplex上的k个点，并基于这些点形成一个子simplex(sub-simplex)，我们称之为topic simplex。注意，在topic simplex上的任意点，在word simplex也是一个点。使用topic simplex的不同隐变量模型，以不同的方式生成一个文档。

- 1-gram混合模型（mixture of unigram model）：假设对于每个文档，在word simplex上的k个点（也就是说：topic simplex的其中一个角落）被随机选中，该文档的所有词汇都从这些点的分布上抽取得到。
- pLSI模型：假定一个训练文档的每个词都来自一个随机选中的topic。这些主题(topics)本身从一个在这些主题上的指定文档分布上抽取到，例如，在topic simplex上的一个点。对于每个文档都有一个这样的分布；训练文档的集合定义了在topic simplex上的一个经验分布。
- LDA: 假定，已观察到和未观察到的文档上的每个词，都由一个随机选中的topic生成，这个topic从一个随机选中的参数的分布上抽取。该参数从来自topic simplex的一个平滑分布上的每个文档中抽样得到。

<img src="http://pic.yupoo.com/wangdren23/GqHodVkl/medish.jpg">

图4: 嵌在包含三个词的word simplex上的三个主题的topic simplex。word simplex的角(corners)对应于三个分布，每个词各自都具有一个概率分布。topic simplex的三个顶点对应于在词上的三个不同分布。The mixture of unigrams模型会将每个文档放到topic simplex上其中一个角(corners)上。pLSI模型会引入根据x的topic simplex的一个经验分布。LDA则在由等高线表示的topic simplex上放置一个平滑分布。

# 3.推断与参与估计

## 3.1 inference

为了使用LDA，我们需要解决的**核心推断问题**是，**对于一个给定文档，计算这些隐变量的后验分布(主题分布)**：

<img src="http://www.forkosh.com/mathtex.cgi?p(\theta,z|W,\alpha,\beta)=\frac{p(\theta,z,W|\alpha,\beta)}{p(W|\alpha,\beta)}">

不幸的是，该分布很难计算。为了归一化该分布，我们对隐变量边缘化（marginalize），并将等式(3)表示成模型参数的形式：

<img src="http://www.forkosh.com/mathtex.cgi?p(W|\alpha,\beta)=\frac{\Gamma(\sum_i\alpha_i)}{\prod_{i}\Gamma(\alpha_i)} \int (\prod_{i=1}^{k}\theta_{i}^{\alpha_i-1}) (\prod_{n=1}^{N}\sum_{i=1}^{k}\prod_{j=1}^{V}(\theta_i\beta_{ij})^{w_n^j}) d\theta">

该函数很难解，因为θ 和 β在隐主题的求和上相耦合(Dickey, 1983)。Dickey展示了该函数是一个在Dirichlet分布(可以表示成一个超几何函数)的特定扩展下的期望。它被用在一个Beyesian上下文中...

尽管后验分布很难精准推断（inference），有许多近似推断算法可以用于LDA，包括Laplace近似，变分近似（variational approximation），马尔可夫链蒙特卡罗法MCMC（jordan,1999）。本节我们描述了一种在LDA中简单的**凸变分推断算法**（convexity-based variational algorithm for inference）。其它方法在paper 第8部分讨论。

## 3.2 变分推断

凸变分推断算法的基本思想是，充分利用Jensen不等式来获取一个在log likelihood上的可调下界（jordan et al.,1999）。本质上，可以考虑一个关于下界的家族(a family of lower bounds)，由一个变分参数（variational parameters）集合进行索引。变分参数由一个优化过程进行选择，它会尝试找到最紧可能的下界（tightest possible lower bound）。

获取一个可控下界家族的一个简单方法是，考虑将原始的图示模型进行简单修改，移去一些边（edge）和节点（node）。将图1所示的LDA模型进行修改， θ 和 β之间的耦合，由于边θ, z, 和 W之间存在边(edge)。通过抛弃这些边以及W节点，生成一个更简单的图示模型，它有自由的变分参数，我们可以在隐变量上获取一个分布族。该分布族由以下的变分分布组成：

<img src="http://www.forkosh.com/mathtex.cgi?q(\theta,z|\gamma,\phi)=q(\theta|\gamma)\prod_{n=1}^{N}q(z_n|\phi_n)"> ......(4)

其中，**Dirichlet分布参数γ**和**multinomial分布参数(φ1 , . . . , φN)**，是自由变分参数。

<img src="http://pic.yupoo.com/wangdren23/GqObibc7/medish.jpg">

图4: (左)LDA的图示模型 (右)采用变分分布来近似LDA后验的图示模型

在指定了一个简化版本的概率分布族后，下一步是建立一个优化问题，来确定变分参数γ 和 φ的值。正如在附录A中展示的，找到一个在log likelihood上的紧下限，可以直接翻译成下面的优化问题：

<img src="http://www.forkosh.com/mathtex.cgi?(\gamma^{*},\phi^{*})=arg min_{\gamma,\phi}^{} D(q(\theta,z|\gamma,\phi)||p(\theta,z|W,\alpha,\beta))"> ......(5)

变分参数的最优值，可以通过对变分分布<img src="http://www.forkosh.com/mathtex.cgi?q(\theta,z|\gamma,\phi)">和真实后验概率<img src="http://www.forkosh.com/mathtex.cgi?p(\theta,z|W,\alpha,\beta)">间的KL散度进行最小化得到。该最小化可以通过一个迭代型的定点方法(fixed-point method)完成。特别的，我们在附录A.3中展示了：通过计算KL散度的导数，将它们等于0, 可以得到以下更新等式的pair：

<img src="http://www.forkosh.com/mathtex.cgi?\phi_{ni} \propto \beta_{iw_n}exp\{E_q[log(\theta_i)|\gamma]\}"> ......(6)

<img src="http://www.forkosh.com/mathtex.cgi?\gamma_{i}=\alpha_{i}+\sum_{n=1}^N\phi_{ni}"> ......(7)

在附录A.1中，多项分布的期望更新可以以如下方式计算：

<img src="http://www.forkosh.com/mathtex.cgi?E_q[log(\theta_i)|\gamma]=\Phi(\gamma_i)-\Phi(\sum_{j=1}^{k}\gamma_{j})"> ......(8)

其中，Ψ 是logΓ函数通过Taylor展开式的首个导数项。

等式(6)和(7)具有一个吸引人的直觉解释。Dirichlet分布更新(Dirichlet update)是一个在给定变分分布(<img src="http://www.forkosh.com/mathtex.cgi?E[z_n|\phi_n]">)下给定的期望观察的后验Dirichlet。多项分布更新（multinomial update）类似于使用贝叶斯理论，<img src="http://www.forkosh.com/mathtex.cgi?p(z_n|w_n) \propto p(w_n|z_n)">，其中p(zn)是在变分分布下的期望值的log的指数近似。

注意，变分分布实际上是一个条件分布，由一个关于W的函数区分。因为在等式(5)中的优化问题，由确定的W所管理，因而，最优解(<img src="http://www.forkosh.com/mathtex.cgi?\gamma^{*},\phi^{*}">)是一个关于W的函数。我们可以将产生的变分分布写成：<img src="http://www.forkosh.com/mathtex.cgi?q(\theta,z|\gamma^{*}(W),\phi^{*}(W))">，其中，我们已经在W上显式地作出了独立性。这样，变分分布可以看成是一个对后验分布<img src="http://www.forkosh.com/mathtex.cgi?p(\theta,z|W,\alpha,\beta)">的近似。

在文本语言中，最优参数（<img src="http://www.forkosh.com/mathtex.cgi?\gamma^{*}(W), \phi^{*}(W)">）是由文档决定的(document-specific)。特别的，我们可以将Dirichlet参数<img src="http://www.forkosh.com/mathtex.cgi?\gamma^{*}(W)">看成是在topic simplex中提供一个文档表示。

<img src="http://pic.yupoo.com/wangdren23/GqIxrY1I/medish.jpg">

图6: LDA的variational inference算法

我们将变分推断过程归结到图6上，对于 γ 和 φn具有合适的起始点。从该伪代码可以知道，LDA的变分推断算法，每次迭代需要O((N+1)k)次操作。经验上，我们可以找到对于单个文档的迭代数，与文档中词的数目相近似。因而总的操作数与<img src="http://www.forkosh.com/mathtex.cgi?N^2k">相近。

## 3.3 参数估计

本节描述了一种在LDA模型中用于参数估计的经验贝叶斯方法(empirical Bayes method)。特别的，给定一个文档集的语料库：D={W1,W2,...WM}。我们希望找到α 和 β，来最大化数据的(marginal)log likelihood: 

<img src="http://www.forkosh.com/mathtex.cgi?l(\alpha,\beta)=\sum_{d=1}^{M}logp(W_d|\alpha,\beta)">

如上所述，p(W|α,β)的计算很麻烦。然后，变分推断提供给我们一个在log likelihood上的下界，我们可以分别根据α和β进行最大化。我们找到了近似经验贝叶斯估计，通过一种变分EM过程，来分别对变分参数γ 和 φ，最大化下界。接着，变分参数确定下来后，再分别根据模型参数α 和 β，最大化下界。

我们在附录A.4中提供了一个用于LDA的变分EM算法的导数的详细推导。导数以下面的迭代形式描述：

- 1.(E-step): 对于每个文档，找到变分参数<img src="http://www.forkosh.com/mathtex.cgi?\gamma_d^{*},\phi_d^{*}: d \in D">。
- 2.(M-step): 分别根据模型参数α 和 β，最大化在log likelihood上产生的下界。这相当于，在E-step中计算出的合适的后验概率下，为每个文档使用期望的足够统计量，找到最大化likelihood估计。

这两个step会一直循环，直到log likelihood的下界收敛为止。

在附录A.4,我们展示了对于条件多项分布参数β进行M-step更新（M-step update），可以写成：

<img src="http://www.forkosh.com/mathtex.cgi?\beta_{ij} \propto \sum_{d=1}^{M}\sum_{n=1}^{N_d}\phi_{dni}^{*}w_{dn}^j"> ......(9)

我们会进一步展示Dirichlet参数α的M-step更新，它可以使用一种有效在线性时间上增长的Newton-Raphson方法，其中的Hessian是可逆的。

## 3.4 Smoothing

对于许多文档的大语料，具有很大的词汇表size，通常会有严重的sparsity问题。对于一个新文档，它包含的词汇很可能没有出现在训练语料中。对于这些词来说，多项分布参数（multinomial parameters）的最大似然估计会分配概率为0，因而，这些新文档的概率为0。应付该问题的方法是，对多项分布参数进行"平滑（smooth）"，对于所有词汇表item都分配一个正的概率，不管它们是否出现在训练集当中。最常用的方法是Laplace smoothing；这本质上会产生在多项分布参数上的均匀Dirichlet先验分布下，对后验分布求平均。

不幸的是，在混合模型设置中，简单的Laplace smoothing不再适合最大化后验方法。事实上，在多项参数上放置一个Dirichlet先验，我们可以在混合模型设置上得到一个难解的后验，这与在基本的LDA模型上得到一个难解的后验的原因基本类似。我们提出解决该问题的方法是，简单地应用变分推断方法到扩展模型上，它包括了在多项分布参数上的Dirichlet smoothing。

在LDA设置中，我们可以获得如图7所示的扩展模型。我们将β看成是一个k x V的随机矩阵（每行表示一个mixture component），其中我们假设每行都是从一个可交换的Dirichlet分布上独立抽取的。我们现在扩展我们的inference过程，将βi看成是随机变量，它天生具有一个后验分布，条件基于数据。此时考虑一个更完整的LDA Bayesian方法。

将一个变分过程看成是一个Bayesian inference，它在随机变量β, θ 和 z上放置一个独立分布(Attias,2000):

<img src="http://www.forkosh.com/mathtex.cgi?q(\beta_{1:k},z_{1:M}|\lambda,\phi,\gamma)=\prod_{i=1}^{k}Dir(\beta_i|\lambda_i) \prod_{d=1}^M q_d(\theta_d,z_d|\phi_d,\gamma_d)">

其中 <img src="http://www.forkosh.com/mathtex.cgi?q_d(\theta,z|\phi,\gamma)">是在等式(4)中定义的变分分布。很容易验证，产生的变分推断过程会产生等式(6)和(7)来分别作为对变分参数φ 和 γ更新等式，同时也会对一个新的变分参数λ做一个额外更新：

<img src="http://www.forkosh.com/mathtex.cgi?\lambda_{ij}=\eta + \sum_{d=1}^{M} \sum_{n=1}^{N_d} \phi_{dni}^{*} w_{dn}^j">

迭代这些等式直到收敛，会产生一个在β, θ 和 z上的一个合适后验分布。

现在我们将超参数η，以及超参数α，留给在可交换的Dirichlet。设置这些超参数的方法是：我们使用变分EM来寻找基于边缘似然上的这些参数的最大似然估计。这些过程在附录A.4描述。


# Batch算法

# Online算法


[Latent Dirichlet Allocation](http://www.seas.upenn.edu/~cis520/lectures/LDA.pdf)