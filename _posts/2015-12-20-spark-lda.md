---
layout: post
title: mllib中lda源码分析 
description: 
modified: 2015-12-20
tags: [lda]
---

# 介绍

首先，先做下约定。

- 一个**词(word)**是离散数据是基本单位，被定义成一个来自词汇表的item，由{1,...,V}进行索引。我们瘵词表示成one-hot格式。这样，使用上标来表示components，在词汇表中第v个词被表示成一个V维的向量w，其中<img src="http://www.forkosh.com/mathtex.cgi?w^v=1, w^u=0(u\neqv)">。
- 一个**文档(document)**是一个N个词组成的序列，表示成**W=(w1,w2,...,wN)**，其中wn是序列上的第n个词。
- 一个**语料(corpus)**是一个关于M个文档的集合，被表示成**D={W1,W2,...,Wm}**.

(blei论文中用黑体的w表示文档，为便于区分，此处用大写W)。

我们希望找到一个关于该语料的概率模型，它不仅可以将高概率分配给该语料的组成文档，还可以将高概率分配给其它“相似”文档。

# LDA

LDA是一个关于语料的**生成概率模型**。基本思想是，文档被表示成在隐主题(latent topics)上的随机混合，其中每个主题(topic)都以一个在词(words)上的分布进行表示。

对于在语料D中的每个文档W，LDA假设如下的生成过程：

- 1.选择 N ~ Poisson(ξ)
- 2.选择 θ ~ Dir(α)
- 3.对于每个文档（N个词汇wn）: 
	- (a) 选择一个主题zn ~ Multinomial(θ)
	- (b) 再在主题zn的基础上，从一个多项分布概率（multinomial probability）：<img src="http://www.forkosh.com/mathtex.cgi?P(w_n| z_n, \beta)">上选中一个词wn

在该基础模型上，做了一些简化假设。

- 首先，Dirichlet分布的维度k(主题变量z的维度)，假设是已知并且确定的。
- 第二，词概率由一个k x V的矩阵β进行参数化，其中<img src="http://www.forkosh.com/mathtex.cgi?\beta_{ij}=p(w^j=1|z^i=1)">, 目前当成是一个待估计的确定量。
- 最后，Poisson猜想是不严格的，可以使用更多的实际文档长度分布。注意N对于所有其它数据生成变量(θ和z)是独立的。它是个辅助变量，我们通常忽略它的随机性。

一个k维的Dirichlet随机变量θ，它的取值为(k-1)-simplex，在该simplex上具有以下的概率密度：

<img src="http://www.forkosh.com/mathtex.cgi?p(\theta|\alpha)=\frac{\Gamma(\sum_{i=1}^{k}\alpha_{i})}{\prod_{i=1}^{k}\Gamma(\alpha_{i})}{\theta_{1}}^{\alpha_{1}-1}...{\theta_{k}}^{\alpha_{k}-1}">    ......(1)

其中，参数α是一个k维的vector，相应的components: αi > 0, 其中 Γ(x)为Gamma函数。Dirichlet是在simplex上的一个便利分布——它在指数族内，具有有限维的充分统计量（finite dimensional sufficient statistics），与multinomial分布相结合。在paper第5部分，这些属性将有助于LDA的inference和parameter estimation算法。

给定参数 α 和 β，我们可以给出**关于一个主题混合θ，一个关于N个主题的z集合，一个N个词W的的联合分布**：

<img src="http://www.forkosh.com/mathtex.cgi?p(\theta,z,W|\alpha,\beta)=p(\theta|\alpha)=\prod_{n=1}^{N}p(z_n|\theta)p(w_n|z_n,\beta)">    ......(2)

其中<img src="http://www.forkosh.com/mathtex.cgi?p(z_n|\theta">可以简单认为：θi对于唯一的i，有<img src="http://www.forkosh.com/mathtex.cgi?{z_{n}}^{i}=1">。在θ上积分，并在z上进行求和，我们可以得到一个**文档的边缘分布（marginal distribution）**：

<img src="http://www.forkosh.com/mathtex.cgi?p(W|\alpha,\beta)=\int p(\theta|\alpha)(\prod_{n=1}^{N}\sum_{z_n}p(z_n|\theta)p(w_n|z_n,\beta))d\theta">    ......(3)

最后，将所有单个文档（document）的边缘分布进行连乘运算，我得到**整个语料（corpus）的概率**：

<img src="http://www.forkosh.com/mathtex.cgi?p(D|\alpha,\beta)=\prod_{d=1}^{M}\int p(\theta_{d}|\alpha)(\prod_{n=1}^{N}\sum_{z_{dn}}p(z_n|\theta)p(w_n|z_n,\beta))d\theta_{d}">

LDA可以表示成图1所示的概率图模型。有三个级别的LDA表述。

- 语料级参数：α 和 β是语料级参数（corpus-level parameters），假设在生成一个语料的过程中只抽样一次。
- 文档级变量：θd是文档级变量（document-level variable），每个文档抽样一次。
- 词级别变量：<img src="http://www.forkosh.com/mathtex.cgi?z_{dn}">和<img src="http://www.forkosh.com/mathtex.cgi?w_{dn}">是词级别变量（word-level variables），对于每个文档中的每个词抽样一次。

将LDA与一个简单的Dirichlet-multinomial clustering模型相区分很重要。一个经典的clustering模型将涉及到一个两层模型(two-level)，对于一个语料只抽样一次Dirichlet；对于语料中的每个文档，只选一次multinomial clustering变量；对于在基于该cluster变量条件的文档上只选择一个词集合。有了许多clustering models后，这样的模型会限制一个文档与单个主题相关系。而LDA涉及三层(three-level)，尤其是主题节点(topic node)在单个文档中被重复抽样。在该模型下，文档可以与多个主题相关联。

<img src="http://pic.yupoo.com/wangdren23/GqFMQRBe/medish.jpg">

图一：LDA的图形化模型表示。"plates"表示可重复。outer plate表示文档，inner plate表示在单个文档中关于主题和词汇的可重复的选择。

通常在贝叶斯统计建模（Bayesian statistical modeling）中学到与图1相似的结构。它们被称为层级模型（hierarchical models），或更精确地称为：条件独立层级模型（con-ditionally independent hierarchical models）(Kass and Steffey, 1989).这样的模型经常作为参数期望贝叶斯模型（parametric empirical Bayes models）被提到，它也被用于参数估计中。在第5部分，我们将采用经验贝叶斯方法（empirical Bayes approach），来估计在LDA的简单实现中的α 和 β, 我们也会考虑更完整的Bayesian方法。

# 推断与参与估计

# Batch算法

# Online算法

