---
layout: post
title: DeepFM介绍
description: 
modified: 2017-11-12
tags: [deepfm]
---

在google提出的deep-wide模型之后，华为实验室的人提出了一个DeepFM模型：

# 1.介绍

现有的模型基本都是偏向于这么几类：低阶特征交叉、高阶特征交叉、或者依赖特征工程。DeepFM可以以一种端到端（end-to-end）的方式来学习特征交叉，无需在原始特征之上做特征工程。deepfm可以归结为：

- 提出了一种新的NN模型：DeepFM（图1），它**集成了FM和DNN的架构**。它即可以像FM那样建模低阶特征交叉，也可以像DNN那样建模高阶特征交叉。不同于Wide&Deep模型，DeepFM可以进行端到端训练，无需任何特征工程。
- DeepFM的wide part和deep part，与Wide&Deep模型不同的是，它可以很有效地进行训练，**共享相同的输入以及embedding vector**。在Wide&Deep方法中，输入向量（input vector）的size可能很大，因为需要为wide部分人工设计pairwise型特征交叉，复杂度增长很快。
- DeepFM在benchmark数据上和商业数据上都进行了评测，对现有模型可以有效进行提升。

# 2.方法

假设训练数据包含了n个样本(x, y)，其中x是一个m-fields的数据记录，通常涉及到一个(user,item)的pair，其中$$y \in \{ 0, 1\}$$表示用户点击行为的label。x也包含了类别型字段（比如：性别，位置）和连续型字段（比如：年龄）。每个类别型字段被表示成一个one-hot编码的向量，每个连续型字段用它原有的值进行表示，或者进行离散化one-hot编码表示。这样，每个实例可以被转换成(x,y)，其中 $$ x = [x_{field_1}, x_{field_2}, .., x_{field_j}, ..., x_{field_m}] $$是一个多维向量，其中$$x_{field_j}$$是向量中的第j个字段。通常，x是高维且十分稀疏的。CTR预测的任务就是构建一个预测模型$$ \hat{y} = CTR_{model}(x) $$来估计用户点击的概率。

## 2.1 DeepFM

<img src="http://pic.yupoo.com/wangdren23/HlAMnssW/medish.jpg">

图1

我们的目标是同时学到低阶和高阶特征交叉。为此，我们提出了基于NN的FM（DeepFM）。如图1所示，DeepFM包含了两个组件：FM组件和Deep组件，它们共享相同的输入。**对于feature i，使用$$w_i$$作为权重来衡量1阶的重要性，隐向量$$V_i$$用于衡量feature i与其它特征交叉的影响**。所有的参数，包括$$w_i, V_i$$，以及网络参数$$（W^{(l)}, b^{(l)}）$$会进行joint training：

$$
\hat{y} = sigmoid(y_{FM} + y_{DNN})
$$

...(1)

其中$$ \hat{y} \in (0, 1) $$是预测的CTR，$$y_{FM}$$是FM组件的输出，$$y_{DNN}$$是deep组件的输出。

## 2.1.1 FM组件

<img src="http://pic.yupoo.com/wangdren23/HlBBGGUV/medish.jpg">

图2：FM组件架构

FM组件是一个因子分解机，它可以学到推荐系统的交叉特征。除了特征间的线性交叉（1阶）外，FM还可以将pairwise（2阶）特征交叉建模成各自特征隐向量的内积。

当数据集是稀疏时，对比起过去的方法，它可以更有效地捕获2阶特征交叉。在之前的方法中，**特征i和特征j的交叉的参数只有当两者都出现在同一数据记录时才能被训练。而在FM中，它可以通过隐向量$$V_i$$和$$V_j$$的内积来衡量**。由于这种灵活的设计，当只有i（或j）出来在数据记录中，FM也可以训练隐向量Vi(Vj)。因而，对于从未出来或很少出现在训练数据中的特征交叉，可以通过FM很好地学到。

如图2所示，FM的输出是一个求和单元（Addition
unit），以及多个内积单元：

$$
y_{FM} = \langle w,x \rangle + \sum_{j_1=1}^{d} \sum_{j_2=j_1+1}^{d} \langle V_i,V_j \rangle x_{j_1} \cdot x_{j_2}
$$

...(2)

其中 $$ w \in R^d $$, $$V_i \in R^k $$(k给定)。求和单元反映了1阶特征的重要性，而内积单元则表示二阶特征交叉的影响。

## 2.1.2 Deep组件

<img src="http://pic.yupoo.com/wangdren23/HlBLDkoh/medish.jpg">

图3：deep组件的架构

**Deep组件是一个前馈神经网络，它用于学习高阶特征交叉**。如图3所示，一个数据记录（即一个向量）被feed给NN。对比起图像和音频的神经网络输入数据（它们几乎都是continuous和dense的），CTR预测所需的输入相当不同（需要一个新的网络结构设计）。尤其是，ctr预测的原始特征输入向量通常是高度稀疏的，相当高维，类别型和连续型混杂，以fields进行分组（例如：性别、地域、年龄）。这暗示了：在进一步feed给第一个隐层(the first hidden layer)之前，需要一个embedding layer来将输入向量压缩成一个低维的、dense的实数值向量，否则该网络将很难训练。

<img src="http://pic.yupoo.com/wangdren23/HlI1syYS/medish.jpg">

图4:embedding layer的结构

图4高亮出从input-layer到embedding layer的子网络结构。我们指出了该网络结果的**两个有趣的特征**：

- 1) 当不同的field输入向量（input field vectors）的长度可以不同，他们的embeddings则具有相同的size(k)
- 2) **在FM中的隐特征向量(V)现在作为网络的权重(weight)使用**，它可以将input field vectors压缩到embedding vectors中。在[Zhang et al.2016]中，V由FM预训练得到，用于初始化。**在DeepFM中，并不会这样做，FM模型是整个学习架构的一部分。这样，我们不需要由FM进行预训练，而是直接以end-to-end的方式进行joint train**。

embedding layer的output定义如下：

$$
a^{(0)} = [e_1, e_2, ..., e_m]
$$ 

...(3)

其中$$e_i$$是第i个field的embedding，m是field总数。接着，$$a^{(0)}$$被feed给DNN，forward处理如下：

$$
a^{(l+1)} = \sigma(W^{(l)} a^{(l)} + b^{(l)})
$$

...(4)

其中l是layer的depth，$$\sigma$$是一个activation function。$$a^{(l)}, W^{(l)}, b^{(l)}$$分别是第l层的output，模型weight，bias。之后，会生成一个dense型实数值特征向量（dense real-value feature vector），最后它会被feed给CTR预测用的sigmoid function：

$$ 
y_{DNN} = \sigma (w^{|H|+1} \cdot a^H + b^{|H|+1}) 
$$

其中\$\|H\|\$是hidden layer的数目。

需要指出的是，**FM组件和Deep组件会共享相同的feature embedding**，这会带来两个好处：

- 1) 可以学到从原始特征的低阶交叉和高阶交叉
- 2) 没必要像Wide&Deep模型那样对输入进行专门的特征工程


## 2.2 与其它NN关系


<img src="http://pic.yupoo.com/wangdren23/HlGieLqx/Axig4.png">

图5



这部分比较了CTR预测中，DeepFM与其它存在的deep模型。

### FNN

如图5（左）所示，FNN是一个由FM初始化的前馈神经网络。FM预训练策略会产生两个限制：1) embedding参数完全受FM的影响 2) 由于预训练阶段引入的开销，效率会降低。另外，FNN只能捕获高阶特征交叉。

### PNN

目标是捕获高阶特征交叉，PNN在embedding layer和第一个hidden layer间引入了一个product layer。根据不同类型的product操作，有三种变种：IPNN，OPNN，PNN*，其中IPNN是基于向量内积，OPNN基于外积，PNN*同时基于内积和外积。

为了让计算更有效率，作者提出了内积和外积的近似计算：1) 内积通过消除一些神经元来近似计算 2) 外积通过将m个k维feature vector压缩到一个k维vector来近似。然而，我们发现外积比内积的可靠性更差，因为外积的近似计算会丢失很多信息，让结果不稳定。尽管内积更可靠，它仍具有高的计算复杂度，因为product layer的输出连接到第一个hidden layer上的所有神经元（neuron）上。不同于PNN，DeepFM的product layer的output只连接到最后的output layer上（一个neuron）。与FNN类似，所有的PNN都会忽视低阶特征交叉。

### Wide & Deep

Wide & Deep由Google提出，用于同时建模低阶和高阶特征交叉。它需要专家对wide部分的输入端进行特征工程（例如：用户安装的app和app推荐曝光的app间的交叉），相反地，DeepFM不需要专家知识，直接从输入的原始特征进行学习。

该模型的一个简单扩展是，通过FM替代LR。该扩展与DeepFM相似，但DeepFM会在FM组件和Deep组件间共享feature embedding。这种共享策略会影响低阶和高阶特征交叉的特征表示，可以更准备地进行建模。

### 总结

<img src="http://pic.yupoo.com/wangdren23/HlGj2N6R/medish.jpg">

# 3.试验 

数据集：

1) Criteo Dataset: 4500w用户点击记录。13个连续特征，26个类别型特征。
2) ) Company∗（华为） Dataset：收集了该公司App Store的游戏中心连续7天的用户点击记录数据进行训练，下一天数据进行预测。整体数据集有10亿记录。在该数据集中，有app特征（id，类别等），user特征（下载过的app等），context特征（操作时间）

## Evaluation Metrics

AUC （(Area Under ROC)）和 LogLoss（cross entropy）

具体见paper，不详述。

# 参考

[https://arxiv.org/pdf/1703.04247.pdf](https://arxiv.org/pdf/1703.04247.pdf)