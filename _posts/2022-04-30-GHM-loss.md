---
layout: post
title: GHM loss介绍
description: 
modified: 2022-04-30
tags: 
---

对focal loss又有一些loss方法，在《Gradient Harmonized Single-stage Detector》提出了GHM loss。

# 3.梯度协调机制（Gradient Harmonizing Mechanism）

## 3.1 问题描述

与(Lin et 2017b)相似，这里主要关注one-stage object detection分类：它的样本(forgeground/background)的分类是相当不均衡的（imbalanced）。对于一个候选方框（candidate box）, 假设：

- $$p \in [0, 1]$$是由模型预估的概率
- $$p^* \in \lbrace 0, 1 \rbrace$$是对于一个指定class的ground truth label

考虑二元cross entropy loss：

$$
L_{CE}(p, p^*) = \begin{cases}
-log(p),  & \text{if $p^*=1$} \\
-log(p-1), & \text{if $p^*=0$}
\end{cases}
$$

假设x是模型的直接输出，比如：sigmoid(x)，我们有随x的梯度：

$$
\frac{\partial{L_{CE}}}{\partial{x}} = \begin{cases}
p - 1,  & \text{if $p^*=1$} \\
p, & \text{if $p^*=0$}
\end{cases} \\
= p - p^*
$$

...(2)

我们如下定义g：

$$
g = | p - p^* | = \begin{cases}
1 - p,  & \text{if $p^*=1$} \\
p, & \text{if $p^*=0$}
\end{cases}
$$

...(3)

g等于梯度w.r.t x的范数（norm）。**g的值表示一个样本的属性（例如：easy或hard），并隐含表示了样本在全局梯度上的影响**。尽管梯度的严格定义是在整个参数空间上的，它意味着g是一个关于样本梯度的相对范数，出于便利，我们称g为gradient norm.

图2展示了来自一个收敛的one-stage detection model的g分布。由于easy negatives占据绝大多数，我们使用log axis来展示样本比例，来演示具有不同属性的样本变种的详情。它可以被看成是：very easy examples的数目是相当大的，对全局梯度具有一个很大影响。再者，我们可以看到，一个收敛模型仍不能处理一些very hard examples：它们的数目要比中等困难的样本还要更大。这些very hard examples可以被看成是异类（outliers），因为它们的梯度方向趋势与大量其它样本的梯度方向非常不同。也就是说，如果收敛模型会被强制学习对这些异类更好的分类，大量其它样本的分类趋向于更少的精准度(accurate)。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/58c4798737a93887fbbf9fb3c2f161b35c65637101d5e59c5f5113bd1208c66602e07bb85fb6b6a0b5d4ecdd72c99953?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 来自一个收敛的one-stage detection模型的gradient norm g分布。注意，y轴使用log scale，因为具有不同gradient norm的样本数，可以通过幅值的阶数进行区分

## 梯度密度（Gradient Density）

为了处理梯度范数分布的不一致问题，我们介绍了一种会考虑Gradient Density的协调方案。训练样本的Gradient Density function被公式化成等式（4）：

$$
GD(g) = \frac{1}{l_{epsilon}(g)} \sum\limits_{k=1}^N \delta_{epsilon} (g_k, g)
$$

...(4)

其中，$$g_k$$是第k个样本的gradient norm。并且：

$$
\delta_{\epsilon}(x, y) =  \begin{cases}
1,  & \text{if $y - \frac{\epsilon}{2} <= x < y + \frac{\epsilon}{2} $} \\
p, & \text{otherwise}
\end{cases}
$$
...(5)

$$
l_{epsilon}(g) = min(g + \frac{\epsilon}{2}, 1) - max(g - \frac{\epsilon}{2}, 0)
$$

...(6)

g的gradient density表示了位于以g区域中心、长度为$$\epsilon$$、通过区域合法长度进行归一化的样本数目。

现在，我们定义了梯度密度协调参数（gradient density harmonizing parameter）为：

$$
\beta_i = \frac{N}{GD(g_i)}
$$

...(7)

其中，N是样本总数。为了更好理解梯度密度协调参数，我们可以将它重写成：$$\beta_i = \frac{1}{GD(g_i)/N}$$。其中：

- $${GD(g_i)/N}$$：是一个normalizer，它表示与第i个样本有邻近梯度的样本比例。如果样本随梯度被均匀分布，则对于任意$$g_i$$，有$$GD(g_i) = N$$，每个样本具有相同的$$\beta_i = 1$$，它意味着无需任何改变。否则，具有大密度的样本会通过normalizer被相对地进行down-weighted。

## GHM-C Loss

通过将$$\beta_i$$看成是第i个样本的loss weight，我们将GHM嵌入到分类loss，loss function的gradient density harmonized形式如下：

$$
L_{GHM\-C}=\frac{1}{N} \sum\limits_{i=1}^N \beta_i L_{CE} (p_i, p_i^*) \\
= \sum\limits_{i=1}^N \frac{L_{CE}(p_i, p_i^*)}{GD(g_i)}
$$

...(8)

图3展示了不同loss的重新公式化后的gradient norm。这里我们采用CE的原始gradient norm（例如：$$g = \| p - p^* \|$$）作为convenient view的x轴，因为该density会根据g进行计算。我们可以看到，Focal Loss和GHM-C loss的曲线具有相似的趋势，它暗示着具有最好参数的Focal Loss与均匀梯度协调（uniform gradient harmonizing）是相似的。更进一步，GHM-C具有Focal loss所没有的多个优点：对异常点的梯度贡献做down-weighting。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/14ac31d494409bbb0058bd79d155c842b80c81950dc464eb42e828544b3cbacb52f36815a15ea618f9391b117fb37ad5?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3 不同loss function的 reformulated gradient norm w.r.t 原始gradient norm g。y轴使用log scale来更好展示FL与GHM-C的细节


有了GHM-C loss，大量very easy examples可以被down-weighted，异常点也会被轻微down-weighted，这可以同时解决属性不平衡（attribute imbalance）问题以及异常点问题（outliers problem）。从图1的右图所示，我们可以更好看到：GHM-C可以使得不同group的examples的总梯度贡献很好协调。由于gradient density会在每轮迭代被计算，examples的weights会像focal loss那边随g(或x)不固定，但会适配模型的当前状态和mini-batch。GHM-C loss的动态特性会让训练更高效和健壮。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/6776372cf580b52232be70a3e60253a7b2080092c5daa0e99f8b43d7c0bdb68d4c2e5996d3eaf68ac4e3a5bf7001e610?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1

# 

- 1.[https://arxiv.org/pdf/1811.05181.pdf](https://arxiv.org/pdf/1811.05181.pdf)