---
layout: post
title: Label Partitioning介绍
description: 
modified: 2015-10-21
tags: [ctr]
---

在google 发表的paper: 《Label Partitioning For Sublinear Ranking》中，有过介绍：

# 介绍

许多任务的目标是，对一个巨大的items, documents 或者labels进行排序，返回给少量top k个给用户。例如，推荐系统任务，比如：通过协同过滤，需要对产品（比如：电影或音乐）的一个大集合，根据给定的user profile进行排序。对于注解任务（annotation），比如：对图片进行关键词注解，需要通过给定的图片像素，给出的可能注解的一个大集合进行排序。最后，在信息检索中，文档的大集合（文本、图片or视频）会基于用户提供的query进行排序。该paper会涉及到实体(items, documents, 等)，被当作labels进行排序，所有上述的问题都看成是**标签排序问题（label ranking problem）**。在机器学习社区中，提出了许多强大的算法，应用于该领域。这些方法通常会通过对每个label依次进行打分(scoring)后根据可能性进行排序，可以使用比如SVM, 神经网络，决策树，其它流行方法等。我们将这些方法称为**标签打分器（label scorers）**。由于对标签打分是独立进行的，许多这些方法的开销与label的数量上是成线性关系的。因而，不幸的是，当标签数目为上百万或者更多时变得不实际，在serving time时会很慢。

本paper的目标是，让这些方法变得实用，在现实世界中的问题就具有海量的labels。这里并没有提出一种新方法来替换你喜欢的方法，**我们提出了一个"wrapper"方法**，当想继续维持(maintaining)或者提升(improve) accuracy时，这种算法能让这些方法更容易驾驭。(**注意，我们的方法会改善测试时间，而非训练时间，作为一个wrapper方法，在训练时实际不会更快**)

该算法**首先会将输入空间进行划分**，因而，任意给定的样本可以被映射到一个分区（partition）或者某分区集合(set of partitions)中。在每个分区中，只有标签的一个子集可以由给定的label scorer进行打分。我们提出该算法，用于优化输入分区，以及标签如何分配给分区。两种算法会考虑选择label scorer，来优化整体的precision @ k。我们展示了如何不需考虑这些因素，比如，label scorer的分区独立性，会导致更差的性能表现。这是因为当标签分区时（label partitioning），对于给定输入，最可能被纠正（根据ground truth）的是labels的子集，原始label scorer实际上表现也不错。我们的算法提供了一个优雅的方式来捕获这些期望。

本paper主要：

- 引入通过label partitioning，在一个base scorer上进行加速的概念
- 对于输入划分(input partitioning)，我们提供了一个算法来优化期望的预测（precision@K）
- 对于标签分配（label assignment），我们提供了一个算法来优化期望的预测（precision@K）
- 应用在现实世界中的海量数据集，来展示该方法

# 前置工作

有许多算法可以用于对标签进行打分和排序，它们与label set的size成线性时间关系。因为它们的打分操作会依次对每个label进行。例如，one-vs-rest方法，可以用于为每个label训练一个模型。这种模型本身可以是任何方法：线性SVM，kernel SVM，神经网络，决策树，或者其它方法。对于图片标注任务，也可以以这种方法进行。对于协同过滤，一个items的集合可以被排序，好多人提出了许多方法应用于该任务，也是通常依次为每个item进行打分，例如：item-based CF，latent ranking模型(Weimer et al.2007)，或SVD-based系统。最终，在IR领域，会对一个文档集合进行排序，SVM和神经网络，以及LambdaRank和RankNet是流行的选择。在这种情况下，不同于注解任务通常只会训练单个模型，它对输入特征和要排序的文档有一个联合表示，这样可以区别于one-vs-test训练法。然而，文档仍然会在线性时间上独立打分。本paper的目标是，提供一个wrapper方法来加速这些系统。

有许多算法用来加速，这些算法取决于对输入空间进行**hashing**，比如通过**局部敏感哈希**(LSH:  locality-sensitive hashing)，或者通过**构建一棵树**来完成。本文则使用分区的方法来加速label scorer。对于该原因，该方法可以相当不同，因为我们不需要将样本存储在分区上（来找到最近邻），我们也不需要对样本进行划分，而是对label进行划分，这样，分区的数目会更小。

在sublinear classification schemes上，近期有许多方法。我们的方法主要关注点在ranking上，而非classification上。例如：label embedding trees（bengio et al.,2010）可以将label划分用来正确分类样本，(Deng et al.,2011)提出一种相似的改进版算法。其它方法如DAGs，filter tree, fast ECOC，也主要关注在快速分类上。尽管如此，我们的算法也可以运行图片标注任务。

# 3.Label Partitioning

给定一个数据集: pairs \$(x_i, y_i), i=1, ..., m \$. 在每个pair中，xi是输入，yi是labels的集合（通常是可能的labels D的一个子集）。我们的目标是，给定一个新的样本 \$ x^{*} \$, 为整个labels集合D进行排序，并输出top k给用户，它们包含了最可能相关的结果。注意，我们提到的集合D是一个"labels"的集合，但我们可以很容易地将它们看成是一个关于文档的集合（例如：我们对文本文档进行ranking），或者是一个items的集合（比如：协同过滤里要推荐的items）。在这些case中，我们感兴趣的问题是，D非常大，算法随label集合的size规模线性扩展，在预测阶段不合适。

假设用户已经训练了一个label scorer: f(x,y)， 对于一个给定的输入和单个label，它可以返回一个real-valued型的分值(score)。在D中对这些labels进行ranking，可以对所有\$ y \in D\$，通过简单计算f(x,y)进行排序来执行。这对于D很大的情况是不实际的。再者，在计算完所有的f(x,y)后，你仍会另外做sorting计算，或者做topK的计算（比如：使用一个heap）。

我们的目标是，给定一个线性时间(或更差)的label scorer: f(x,y)，为了让它在预测时更快（并保持或提升accuracy）。我们提出的方法：label partitioning，有两部分构成：

- (i)输入划分器（input partititoner）: 对于一个给定的样本，将它映射到输入空间的一或多个partitions上
- (ii)标签分配（label assignment）: 它会为每个partition将labels的一个子集

对于一个给定的样本，label scorer只会使用在相对应partitions上的labels子集，因此它的计算更快。

在预测时，对这些labels进行ranking的过程如下：

- 1.给定一个测试输入x，input partitioner会将x映射到partitions的某一个集合中： \$ p=g(x) \$
- 2.我们检索每个被分配到分区\$p_j\$上的标签集合(label sets)：\$ L= \bigcup_{j=1}^{\|p\|} \mathscr{L}_{p_j} \$，其中 \$ \mathscr{L} \subseteq D \$是分配给分区\$p_j\$的标签子集。
- 3.使用label scorer函数f(x,y)对labels \$ y \in L \$进行打分，并对它们进行排序来产生我们最终的结果

在预测阶段ranking的开销，已经被附加在将输入分配到对应分区（通过计算p=g(x)）上的开销；以及在相对应的分区上计算每个label（计算: \$ f(x,y), y \in L \$）。通过使用快速的input partitioner，就不用再取决于label set的size大小了（比如：使用hashing或者tree-based lookup）。提供给scorer的labels set的大小是确定的，相对小很多（例如：\$ \|L\| << \|D\| \$），我们可以确保整个预测过程在\$ \|D\| \$上是**亚线性(sublinear)**的。

## 3.1 输入分区器（Input Partitioner）

我们将选择一个input partitioner的问题看成是：\$ g(x)-> p \subseteq \mathcal{P} \$，它将一个输入点x映射到一个分区p的集合中，其中P是可能的分区：\$ \mathcal{P} = {1,...,P} \$。g总是映射到单个整数上，因而，每个输入只会映射到单个分区，但这不是必须的。

有许多文献适合我们的input partitioning任务。例如：可以使用最近邻算法作为input partitioner，比如，对输入x做hashing((Indyk & Motwani, 1998)，或者tree-based clustering和assignment (e.g. hierarchical k-means (Duda
et al., 1995)，或者KD-treess (Bentley, 1975)，这些选择都可行，我们只需要担心label assignment即可。然而，这些选择的注意点是，当它们可以有效地对我们的数据执行fully unsupervised partitioning，它们不会对我们的任务的唯一需求做出解释。特别的，当我们希望在加速的同时要保持accuracy时。为了对我们的目标进行归纳，我们将输入空间进行分区，以便具有高度相似labels的相应样本在同一个分区内，让label scorer进行排序。

我们提出了一种**层次化分区器（hierarchical partitioner）**，对于一个label scorer:f(x,y), 一个训练集：\$(xi,yi), i=1,...,m \$，以及之前定义的label集合D，它尝试优化目标：precision@k。对于一个给定的训练样本(xi,yi)以及label scorer，我们定义了accuracy的衡量(比如：precision@k)为：\$ \hat{l}(f(x_i),y_i)\$，

以及最小化loss为：
\$ l(f(x_i),y_i)=1-\hat{l}(f(x_i),y_i) \$

这里f(x)是对所有labels的得分向量：

$$
f(x)=f_{D}(x)=(f(x,D_1),...,f(x,D_{|D|})))
$$

其中\$ D_i \$是整个label set上的第i个label。然而，为了衡量label partitioner的loss，而非label scorer，我们需要考虑\$l(f_{g(x_i)}(x_i), yi)\$，它是当ranking时只有xi对应的分区label set的loss。比如：\$ f_{g(x)}(x)=(f(x,L_1),...,f(x,L_{|L|)}))\$

对于一个给定的分区，我们定义它的整个loss为：

$$
\sum_{i=1}^{m}l(f_{g(x_i}(x_i),y_i)
$$

不幸的是，当训练input partitioner时，label assignments: L是未知的，它会让上述的目标函数不可解(infeasible)。然而，该模型发生的error可以分解成一些组件。对于任意给定的样本，它的precision@k会收到一个低值或是0:

- 在一个分区里，相关的标签不在该集合中
- 原始的label scorer在第一个位置效果很差

当我们不知道label assignment时，我们将会把每个分区上labels的数目限制在一个相对小的数（\$ |L_j|<<|D| \$）。实际上，我们会将考虑两点来定义label partitioner：

- 对于共享高度相关的标签的样本，应映射到相同的分区上
- 对于label scorer可发很好执行的样本，当学习一个partitioner时应被优化(prioritized)

基于此，我们提出了方法来进行input partitioning。考虑到一个partitioner的情况，假如定义了分区中心(partition centroids) \$c_i, i=1,...,P\$, 使用最接近的已分配的分区：

$$
g(x)=argmin_{i=1,...,P} || x-c_i ||
$$

这可以很容易地一般化到hierarchical的case，通过递归选择子中心(child centroids)来完成，通常在hierarchical k-means和其它方法中使用。

** Weighted Hierarchical Partitioner ** ，一个简单方法来确保input partitioner可以优化样本，这些使用给定的label scorer可以很好的执行，使用label scorer结果对每个训练样本进行加权：

$$
\sum_{i=1}^{m}\sum_{j=1}^{P} \hat{l}(f(x_i),y_i)||x_i-c_j||^{2}
$$

实际上，一个基于该目标函数的hierarchical partitioner，可以通过一个“加权(weighted)”版本的 hierarchical k-means来完成。在我们的实验中，我们简单地执行一个"hard"版本： 我们只在训练样本集合 \$ {(x_i,y_i): \hat{l}(f(x_i),y_i) \geq \rho )} \$上运行k-means，取ρ = 1.

注意，我们不使用 \$ l(f_g(x_i)(x_i), y_i) \$, 而使用\$ l(f(x_i),y_i) \$，但它是未知的。然而，\$ l(f_g(x_i)(x_i), y_i) \leq l(f_D(x_i),y_i) \$，如果\$ y_i \in L_{g(x_i)}\$，以及\$ l(f_g(x_i)(x_i), y_i)=1\$，otherwise。也就是说，我们使用的proxy loss，上界逼近真实值，因为比起完整的集合，我们只有很少的label，因而precision不能降低——除非真实label不在分区中。为了阻止后面的情况，我们必须确保具有相似label的样本在同一个分区中，我们可以通过学习一个合适的metrics来完成。

** Weighted Embedded Partitioners **, 在上述构建weighted hierarchical partitioner时，我们可以更进一步，引入constraint：样本共享着高度相关的labels，映射到同一个partitioner上。编码这些constrints可以通过一种metric learning阶段来完成(Weinberger et al., 2006).。

接着，你可以学习一个input partitioner，通过使用上面的weighted hierarchical partitioner目标函数，在要学的"embedding"空间上处理：

$$
\sum_{i=1}^{m} \sum_{j=1}{P} \hat{l}(f(x_i),y_i)||Mx_i-c_j||^2
$$

然而，一些label scorer已经学到了一个latent "embedding" space。例如，SVD和LSI等模型，以及一些神经网络模型（Bai et al., 2009). 在这样的case中，你可以在隐空间(latent space)上直接执行input partitioning，而非在输入空间上；例如：如果label scorer模型的形式是：\$\f(x,y)= \Phi_{x}(x)^T \Phi_{y}(y) \$，那么partitioning可以在空间 \$ \Phi_x(x) \$上执行。这同样可以节省计算两个embeddings（一个用于label partitioning，一个用于label scorer）的时间，在特征空间中的进一步分区则为label scorer调整。

## 3.2 Label Assignment

本节将选反一个label assignment L。

- 训练集\$ (x_i,y_i), i=1,...,m \$，label set为：D
- input partitioner: g(x)，使用之前的方式构建
- 线性时间label scorer: f(x,y)

我们希望学到label assignment: \$ L_j \subseteq D \$，第j个分区对应的label set。我们提出的label assignment方法会应用到每个分区中。首先，我来考虑优化precision@1,接着这种简化版的case中，每个样本只有一个相关的label。这里我们使用索引t来索引训练样本，相关的label为\$ y_t \$。我们定义：\$ \alpha \in {0,1}^{|D|}\$，其中\$ \alpha_{i} \$决定着一个label \$ D_i \$是否会被分配到该分区上（\$ \alpha_{i}=1 \$），或不分配（\$ \alpha_{i}=0 \$）。这里的\$ \alpha_{i} \$就是我们希望优化的变量。接下去，我们通过给定的label scorer对rankings进行编码：

- \$ R_{t,i} \$是label i对于样本t的rank分值：

$$
R_{t,i}= 1 + \sum_{j \neq i}\delta(f(x_t,D_j)>f(x_t,D_i))
$$

- \$ R_{t,y_t} \$是样本t的true label的rank分值

我们接着将需要优化的目标函数写出来：

$$
max_{\alpha} \sum_{t} \alpha_{y_t}(1 - max_{R_{t,i}<R_{t,y_t}} \alpha_i)
$$

......(1)

服从：

\$ \alpha_{i} \in {0,1} \$ ....(2)

\$ | \alpha | = C \$ ....(3)

其中，C是分配给该分区的label数。对于一个给定的样本t，为了最大化precision@1,需满足两个条件：(1) true label必须被分配给该分区 (2) true label必须是所有被分配labels上排序分值最高的。我们可以看到，等式1可以精确计算precision@1,因为项\$ \alpha_{y_t} \$和\$ (1-max_{R_{t,i}<R_{t,y_t}} \alpha_{i}) \$ 会对这两个条件各自进行measure。我们的目标函数会统计训练样本数precision@1。

有意转的是，注意，label partitioning的本意是：

- (i) 训练样本t在原始的label scorer上标记不正确，但由label partitioner标注正确的，因为高度不相关的label不会被分配到该分区上
- (ii) 原始的label scorer可以正确标注样本，但现在不正确，由于相关的label没有被分配到该分区上

该优化问题，尽可能地将多个相关的label放在同一分区中，并且尽可能消除尽可能混淆的labels（高排序值但不正确），如果通过移除它们，更多的样本会被正确标注。如图1所示：

[图1]

图1: 从D中选择2个labels，考虑precision@1的label assignment问题。\$ R_i \$是这些样本的ranked labels（粗体为：true labels）。当选中sky时，会正确序测样本1和2, 上面展示的sky，样本3-5的true labels. 最优的选择是car和house，它们在样本3-5中可以被正确预测，因为所有更高排序的不相关labels（higher-ranked irrelevant labels）会被抛弃掉。该选择问题展示了我们在要label assignment任务中要面临的挑战。

不幸的是，等式2的二元限制（binary constraint）


# 4.实验

## 4.1 图像注解

首先使用ImageNet数据集来测试图片注解任务。ImageNet是一个很大的图片数据集，它将人口验证通过的图片与WordNet的概念相绑定。我们使用Spring 2010的版本，它具有9M的images，我们使用：10%用于validation, 10%用于test，80%用于training。该任务会对15589个可能的labels做rank，它们的范围从animals(“white
admiral butterfly”)到objects(“refracting telescope”).

我们使用与(Weston et al.,2011)相似的一个特征表示，它会将多种特征表示相组合。它们可以是多种空间的串联，

... 



# 1.介绍

在线广告是一个好几十亿美金的生意，机器学习在该领域获得巨大成功。赞助搜索广告，上下文广告，展示广告，以及实时竞价拍卖，都十分依赖于机器学习模型的能力来精准地／快速地／可靠地预测点击率。该问题如果在几十年前几乎不可想像。一个典型的工业模型，可以每天提供十亿级的事件预测，使用相当大的特征空间，并能从产生的大量数据中进行学习。

在本paper中，我们发表了一系列从最近试验中总结的学习案例，它们在google部署的系统中用于预测赞助搜索广告的广告点击率。因为该问题目前已经能很好地进行学习，我们选择集中在一系列很少注意的话题上，它们在实际工作系统中相当重要。接着，我们探索了内存节省技术，性能分析，预测的置信度，校准，特征管理。该paper的目标是，在大规模工业领域中的技巧和见解上，给读者一个感观认识。

# 2.详细系统总览

当一个用户做出一个搜索q时，会基于广告主选择的关键词，与query q相匹配得到候选广告的初始集。竞拍机制接着决定哪个广告展示给用户，以何种顺序展示，如果广告被点击后广告主支付多少价格。除了广告主竞价，对于竞拍，一个重要的输入是，对于每个广告a，一个P(click|q,a)的估计，该概率表示广告是否会被点击。

在我们系统中使用的特征，会从多个数据源进行抽取，包括：query，广告创意文本，多种与广告相关的元数据(metadata)。数据趋于相当稀疏，通常每个样本只有很小一部分是非零特征。

正则化logistic回归这样的方法天然适用于这类问题。它很适合做出预测，每天几十亿次，随着新观察到的点击和非点击事件，模型可以快速更新。当然，这些数据比例意味着训练数据集相当庞大。数据通过一个基于Photon系统的streaming服务提供。

因为大规模学习系统在最近这些年中很好地进行学习，我们在该paper中不再大篇幅描述我们的系统架构。然而，我们注意到，这些训练方法与Google Brain团队的Downpour SGD具有相似性，区别是，我们训练了一个单层模型，而非一个多层的深度神经网络。这允许我们处理更巨大的数据集，更大的模型，十亿级的系数。因为训练模型被复制到许多数据中心作为服务，我们更关系在线服务时间的稀疏性，而非训练时。

# 3.在线学习和稀疏性

对于大规模学习，泛化线性模型的在线学习算法（比如logistic回归）具有许多优点。尽管特征向量x具有十亿级的维度，通常每个实例都具有上百个非零值。通过磁盘上／网络上的streaming，这可以使得大数据集上的训练更有效，因为每个训练样本只需要被考虑一次。

为了精确表示算法，我们需要确定一些注意点。我们表示向量：<e gt ∈ R
d>，其中t表示当前训练实例；该gt向量中的第i个条目表示成gt,i.我们也使用压缩版的表示：g1:t=∑gs.

如果我们希望使用logistic回归建模，我们可以使用以下的在线框架。在第t轮，我们由特征向量xt来预测一个实例；给定模型参数wt，我们预测pt=sigma(wt*xt), 其中sigma(a)=1/(1+exp(-a)是sigmoid函数。接着，我们观察到label yt, 给定p和yt的负log似然yt, 可产生的LogLoss结果输出：

$$
l_t(w_t)=-y_{t}log p_{t} - (1-y_t)log(1-p_t)
$$

简单展示为\$ O`t(w) = (σ(w · xt) − yt)xt = (pt − yt)xt \$，该梯度是我们需要优化的目标。

在线梯度下降(OGD)，在许多问题上证明非常有效，它使用最少的计算资源，带来很精准的预测准确率。




和\$ \hat{C} \$各自表示平滑后的impressions和clicks。平滑后的CTR就等于\$ \frac{\hat{C}}{\hat{I}} \$。

## 参考

[Label Partitioning For Sublinear Ranking](http://www.thespermwhale.com/jaseweston/papers/label_partitioner.pdf)


