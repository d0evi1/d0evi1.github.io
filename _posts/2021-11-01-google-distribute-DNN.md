---
layout: post
title: 大规模分布式DNN介绍
description: 
modified: 2021-11-01
tags: 
---

# 介绍

google的Jeffrey Dean在2012《Large Scale Distributed Deep Networks》这篇经典paper中提出了大规模分布式DNN的设计。我们回顾一下：

# 摘要

最近在无监督特征学习和深度学习领域的工作表明，能够训练大型模型可以显著提高性能。在本文中，我们考虑了使用**数万个CPU核心训练具有数十亿参数的深度网络的问题**。我们开发了一个名为DistBelief的软件框架，可以利用数千台机器的计算集群来训练大型模型。在此框架内，我们开发了两种大规模分布式训练算法：

- （i）Downpour SGD，一种支持大量模型副本的异步随机梯度下降过程；
- （ii）Sandblaster，一个支持多种分布式批量优化过程的框架，包括L-BFGS的分布式实现。

Downpour SGD和Sandblaster L-BFGS都增加了深度网络训练的规模和速度。我们已经成功地使用我们的系统训练了一个比文献中先前报道的**深度网络大30倍的网络，并在ImageNet上实现了最先进的性能**，ImageNet是一个包含1600万张图像和21k类别的视觉对象识别任务。我们展示了这些相同的技术如何显著加速了一个更适度规模的深度网络的训练，用于商业语音识别服务。尽管我们专注于这些方法在训练大型神经网络方面的性能，但底层算法适用于任何基于梯度的机器学习算法。

# 1 引言

深度学习和无监督特征学习在许多实际应用中显示出巨大的潜力。在几个领域已经报告了最先进的性能，从语音识别[1, 2]、视觉对象识别[3, 4]到文本处理[5, 6]。还观察到，增加深度学习的规模，无论是训练样本的数量、模型参数的数量，还是两者都增加，都可以大幅提高最终分类精度[3, 4, 7]。这些结果导致了对扩大这些模型的训练和推理算法的兴趣激增[8]，并改进适用的优化程序[7, 9]。GPU的使用[1, 2, 3, 8]是近年来的一个重大进步，使得训练适度规模的深度网络变得实际。**GPU方法的一个已知限制是，当模型不适合GPU内存时（通常小于6GB），训练加速很小**。为了有效地使用GPU，研究人员经常减小数据或参数的大小，以便CPU到GPU的传输不是一个显著的瓶颈。虽然数据和参数的减小对于小问题（例如语音识别中的声学建模）效果很好，但对于具有大量样本和维度的问题（例如高分辨率图像）则不太吸引人。

在本文中，我们描述了一种替代方法：**使用大型机器集群来分布式训练和推理深度网络**。我们开发了一个名为DistBelief的软件框架，它**支持在机器内部（通过多线程）和机器之间（通过消息传递）的模型并行性，框架管理并行性、同步和通信的细节**。除了支持模型并行性(s model parallelism)，DistBelief框架还支持数据并行性(data parallelism)，其中使用多个模型副本来优化单个目标。在此框架内，我们设计并实现了两种新的大规模分布式训练方法：

- （i）Downpour SGD：一种**异步SGD过程**，利用自适应学习率并支持大量模型副本；
- （ii）Sandblaster L-BFGS：一种使用**数据和模型并行性**的L-BFGS的分布式实现。

Downpour SGD和Sandblaster L-BFGS与更传统的SGD和L-BFGS实现相比，都获得了显著的速度提升。我们的实验揭示了关于大规模非凸优化的几个令人惊讶的结果。

- 首先，异步SGD很少应用于非凸问题，但对于训练深度网络非常有效，特别是当与Adagrad[10]自适应学习率结合时。
- 其次，我们展示了：如果有足够的资源，L-BFGS与许多SGD变体相比具有竞争力或更快。

关于深度学习中的具体应用，我们报告了两个主要发现：我们的分布式优化方法既**可以大大加速适度规模模型的训练，也可以训练比以往更大的模型**。

- 为了说明第一点，我们展示了我们可以使用机器集群以不到GPU所需时间的1/10来训练一个适度规模的语音模型，达到相同的分类精度。
- 为了说明第二点，我们训练了一个超过10亿参数的大型神经网络，并使用这个网络在ImageNet数据集上大幅提高了最先进的性能，这是计算机视觉中最大的数据集之一。

# 2 先前的工作

近年来，商业和学术机器学习数据集以前所未有的速度增长。作为回应，许多作者已经探索了扩大机器学习算法以应对这些数据的洪流[11, 12, 13, 14, 15, 16, 17]。这些研究中的大部分集中在线性、凸模型[11, 12, 17]上。在凸情况下，分布式梯度计算[18]是自然的第一步，但有时由于同步问题而遭受减速。有一些有希望的努力来解决这个问题，例如在异步随机梯度下降中的无锁参数更新，例如Hogwild[19]。

不幸的是，将这些方法扩展到密集的非凸问题，例如在训练深度架构时遇到的问题，基本上是未知的领域。特别是，不知道是否可能在多个局部最小值存在的情况下平均参数或执行密集的异步参数更新。在深度学习的背景下，大部分工作集中在在单台机器上训练相对较小的模型（例如，Theano[20]）。扩大深度学习的一个有趣的建议是使用GPU农场来训练许多小型模型，然后平均它们的预测[21]，或者修改标准深度网络使其更易于并行化[22]。

与以前的工作相比，我们的重点是在不限制模型形式的情况下，扩大具有数十亿参数的非常大的模型的深度学习技术。在这种情况下，**模型并行性**，类似于[23]的精神，是一个基本成分，但必须与巧妙的分布式优化技术相结合，这些技术利用**数据并行性**。

我们考虑了许多现有的大规模计算工具，将其应用于我们的问题，MapReduce[24]和GraphLab[25]是显著的例子。我们得出结论：

- MapReduce，旨在并行数据处理，不适合深度网络训练中固有的迭代计算；
- 而GraphLab，旨在通用（非结构化）图计算，不会利用在深度网络中通常发现的结构化图中的计算效率。

# 3 模型并行性

为了促进非常大的深度网络的训练，我们开发了一个名为DistBelief的软件框架，它支持在神经网络和分层图形模型中的分布式计算。用户定义在模型的每个层的每个节点处进行的计算，以及在计算的向上和向下阶段应该传递的消息。对于大型模型，用户可以将模型分割到几台机器上（图1），以便将不同节点的计算责任分配给不同的机器。框架自动使用所有可用核心在每台机器上并行化计算，并在训练和推理期间管理机器之间的通信、同步和数据传输。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/1cab0b115e8fdc5a072fa123d573d94bf3447c65939d3c9223b11518556ab6560f40b960c3686bb38ef302138cac9e36?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 DistBelief中**模型并行性**的例子。这里展示了一个具有局部连接性的五层深度神经网络，它被分割在四台机器（蓝色矩形）上。只有那些边**跨越分区边界（粗线）的节点**才需要在机器之间传输它们的状态。即使在节点有多个边跨越分区边界的情况下，其状态也**只发送给边界另一侧的机器一次**。在每个分区内部，单个节点的计算将在所有可用的CPU核心上并行化。

在多台机器上分布深度网络的性能优势取决于模型的连接结构和计算需求。具有大量参数或高计算需求的模型通常从访问更多的CPU和内存中受益，直到通信成本占主导地位。我们已经成功地在DistBelief框架中运行了多达144个分区的大型模型，并取得了显著的速度提升，而更适度规模的模型在多达8或16个分区上显示出不错的速度提升。（见第5节，在“模型并行性基准测试”标题下查看实验结果。）显然，**具有局部连接结构的模型比全连接结构更容易进行广泛的分布，因为它们的通信需求较低**。提速不理想的典型原因是：不同机器之间的处理时间有差异，导致许多机器需要等待那台最慢的机器，以便完成给定阶段的计算。尽管如此，对于我们最大的模型，我们可以有效地使用32台机器，每台机器平均使用16个核心，总共512个CPU核心训练一个大型神经网络。当与下一节描述的分布式优化算法结合使用时，这些算法利用整个神经网络的多个副本，可以利用数万个CPU核心来训练单个模型，从而显著减少整体训练时间。

# 4 分布式优化算法

在DistBelief框架内并行化计算使我们能够实例化和运行比以前报告的更大的神经网络。但是，**为了在合理的时间内训练如此大的模型，我们不仅需要在单个模型实例内进行并行计算，也需要跨多个模型实例进行分布式训练**。在本节中，我们描述了这第二级别的并行性，我们使用一组DistBelief模型实例或副本来同时解决单个优化问题。

我们比较了两种大规模分布式优化程序：

- Downpour SGD，一种**在线方法**
- Sandblaster L-BFGS，一种**批量方法**

两种方法都利用了**集中式分片参数服务器**的概念，模型副本使用它来共享它们的参数。两种方法都利用了DistBelief在每个单独副本内允许的分布式计算。但最重要的是，两种方法都被设计为容忍不同模型副本的处理速度差异，甚至是模型副本的大规模故障，这些副本可能被离线或随机重启。

从某种意义上说，这两种优化算法实现了数据并行性的智能版本。两种方法都允许我们同时在许多模型副本中的每一个中处理不同的训练样本，并定期组合它们的结果来优化我们的目标函数。

## 4.1 Downpour SGD

随机梯度下降（SGD）或许是训练深度神经网络最常用的优化过程[26, 27, 3]。不幸的是，SGD的传统公式本质上是顺序的，使得它不适用于非常大的数据集，因为在完全顺序化的方式下完成全部数据计算所需的时间是令人望而却步的。

为了将SGD应用于大型数据集，我们引入了Downpour SGD，这是一种使用单个DistBelief模型的多个副本的异步随机梯度下降变体。基本方法如下：我们将训练数据分成多个子集，并在这些子集上运行模型的副本。**模型通过一个集中的参数服务器通信更新，该服务器保存模型的所有参数的当前状态，这些参数分布在许多机器上（例如，如果我们有10个参数服务器分片，每个分片负责存储和应用模型参数的1/10的更新）**（图2）。这种方法在两个不同的方面是异步的：模型副本独立运行，参数服务器分片也独立运行。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/c88ccdea4ad5ac4e1b978b2f09389741592833b84e3add4ae96f06fa68d01ee023970e6ade271901533c666fa762380b?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 左边：Downpour SGD。模型副本异步地从参数服务器获取参数 $ \mathbf{w} $ 并推送梯度 $ \Delta \mathbf{w} $。右边：Sandblaster L-BFGS。一个单独的“协调器”向副本和参数服务器发送小消息，以协调批量优化。

在最简单的实现中，在处理每个小批量之前，模型副本会向参数服务器服务请求其模型参数的更新副本。由于DistBelief模型本身分布在多台机器上，因此**每台机器只需要与持有与其分区相关的模型参数的参数服务器分片通信**。在接收到其参数的更新副本后，DistBelief模型副本处理一小批数据以计算参数梯度，并将梯度发送到参数服务器，然后参数服务器将梯度应用于模型参数的当前值。

可以减少Downpour SGD的通信开销，通过限制每个模型副本仅在每$ n_{\text{fetch}}$ 步请求更新参数，并且**仅在每$ n_{\text{push}}$步发送更新的梯度值**（其中：$ n_{\text{fetch}} \text{可能不等于} n_{\text{push}}$。

实际上，**获取参数（fetch）、推送梯度（push）和处理训练数据的过程可以在三个仅弱同步的线程中进行**（见附录中的伪代码）。在下面报告的实验中，我们为了简单和便于与传统SGD比较，将 $ n_{\text{fetch}} = n_{\text{push}} = 1 $ 固定。

Downpour SGD比标准（同步）SGD更能够抵抗机器故障。对于同步SGD，如果一台机器失败，整个训练过程将被延迟；而**对于异步SGD，如果模型副本中的一台机器失败，其他模型副本继续处理它们的训练数据并通过参数服务器更新模型参数**。另一方面，Downpour SGD中的多种异步处理引入了优化过程中的大量额外随机性。最明显的是，**模型副本几乎肯定是基于一组稍微过时的参数计算其梯度，因为其他模型副本可能在此期间已经更新了参数服务器上的参数**。但是，除此之外还有几个其他的随机性来源：由于参数服务器分片独立运行，不能保证在任何给定时刻，每个参数服务器分片上的参数经历了相同数量的更新，或者更新以相同的顺序应用。此外，由于模型副本被允许在单独的线程中获取参数和推送梯度，可能还存在参数的时间戳的额外微妙不一致性。对于非凸问题，这些操作的安全性几乎没有理论基础，但在实践中我们发现**放宽一致性要求非常有效**。

我们发现一种可以大大增加Downpour SGD鲁棒性的技术是使用Adagrad[10]自适应学习率过程。Adagrad不是在参数服务器上使用单一固定学习率（图2中的η），而是为每个参数使用单独的自适应学习率。设：

- $ \eta_{i,K} $ 为第 $ i $ 个参数在第 $ K $ 次迭代的学习率
- $ \Delta w_{i,K} $ 为其梯度

则我们设置：

$$ 
\eta_{i,K} = \frac{\gamma}{\sqrt{\sum_{j=1}^{K} (\Delta w_{i,j})^2}} 
$$

由于这些学习率仅从每个参数的累积平方梯度计算，**Adagrad可以很容易地在每个参数服务器分片内局部实现**。γ的值，所有学习率的常数缩放因子，通常比没有使用Adagrad时使用的最佳固定学习率大（可能大一个数量级）。Adagrad的使用扩展了可以同时有效工作的模型副本的最大数量，并且结合了仅使用单个模型副本“预热”模型训练的做法，然后释放其他副本，它几乎消除了使用Downpour SGD训练深度网络时的稳定性问题（见第5节的结果）。

## 4.2 Sandblaster L-BFGS

批量方法已被证明在训练小型深度网络方面表现良好[7]。为了将这些方法应用于大型模型和大型数据集，我们引入了Sandblaster批量优化框架，并讨论了在此框架中使用L-BFGS的实现。

Sandblaster的一个关键思想是分布式参数存储和操作。优化算法（例如L-BFGS）的核心位于协调器进程（图2），它没有直接访问模型参数。相反，协调器发出来自一小组操作（例如，点积、缩放、系数加法、乘法）的命令，每个参数服务器分片可以独立执行这些操作，并将结果存储在同一分片上。附加信息，例如L-BFGS的历史缓存，也存储在计算它的参数服务器分片上。这允许运行大型模型（数十亿参数）而不会因将所有参数和梯度发送到单个中央服务器而产生开销（见附录中的伪代码）。

在典型的L-BFGS并行化实现中，数据被分发到多台机器上，每台机器负责计算特定子集数据示例上的梯度。梯度被发送回中央服务器（或通过树形结构聚合[16]）。许多这样的方法等待最慢的机器，因此不适用于大型共享集群。为了解决这个问题，我们采用了以下负载平衡方案：协调器为N个模型副本中的每一个分配一小部分工作，远小于批次总大小的1/N，并且每当副本空闲时就分配新的部分。通过这种方法，较快的模型副本比慢的副本做更多的工作。为了进一步管理批次末尾的慢模型副本，协调器调度多份未完成的部分，并使用最先完成的模型副本的结果。这种方案类似于MapReduce框架中“备份任务”的使用[24]。数据预取，以及通过将数据的连续部分分配给同一工作器来支持数据亲和性，使得数据访问不成问题。与Downpour SGD相比，后者需要相对高频率、高带宽的参数同步与参数服务器，Sandblaster工作器仅在每个批次开始时（当它们被协调器更新时）获取参数，并且仅在完成几个部分后发送梯度（以防止副本故障和重启）。


# 实验

略

# 参考

- 1.[https://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf](https://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf)