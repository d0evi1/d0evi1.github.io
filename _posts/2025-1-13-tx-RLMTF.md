---
layout: post
title: IntegratedRL-MTF介绍
description: 
modified: 2025-2-23
tags: 
---

**摘要**  

作为推荐系统（RS）的关键最终环节，多任务融合（MTF）负责将多任务学习（MTL）生成的多个评分整合为最终评分，以最大化用户满意度并决定最终推荐结果。近年来，业界开始采用强化学习（RL）进行MTF，以优化推荐会话（session）中的长期（long-term）用户满意度。然而，**当前用于MTF的离线RL算法存在三个严重缺陷**：  

1) 为避免分布外（OOD：out-of-distribution）问题，其约束条件过于严格，严重损害模型性能；  
2) 算法无法感知训练数据生成所用的探索策略，且未与真实环境交互，导致仅能学习次优策略；  
3) 传统探索策略效率低下且损害用户体验。  

针对这些问题，我们提出面向大规模推荐系统MTF的创新方法**IntegratedRL-MTF**，其核心创新包括：  

- **离线/在线策略融合**：通过将离线RL模型与在线探索策略相整合，放宽过严的约束条件，显著提升性能；  
- **高效探索策略**：剔除低价值探索空间（low-value exploration space），聚焦潜在高价值状态-动作对的探索；  
- **渐进式训练**：借助探索策略进一步优化模型表现。

在腾讯新闻短视频频道的离线和在线实验表明，该方法显著优于基线模型。目前**IntegratedRL-MTF**已在腾讯推荐系统及其他大型推荐场景中全面部署，取得显著效果提升。

# 1 引言

推荐系统（Recommender Systems, RSs）[1, 2]通过分析用户偏好提供个性化推荐服务，目前已广泛应用于短视频平台[3, 7, 14]、视频平台[4, 5]、电子商务平台[6, 8-11]及社交网络[12, 13]等场景，每日服务数十亿用户。工业级推荐系统通常包含三阶段流程：候选生成（candidate generation）、排序（ranking）和多任务融合（Multi-Task Fusion, MTF）[4, 15]。在候选生成阶段，系统需从数百万乃至数十亿候选项中筛选出数千个候选项目；排序阶段则采用多任务学习模型（Multi-Task Learning, MTL）[4, 8, 16-18]预测用户点击、观看时长、快速滑动、点赞、分享等多种行为的预估分数；最终通过MTF模型将MTL输出的多任务分数融合为单一分数，生成候选项目的最终排序[15]，从而决定推荐结果。然而目前针对MTF的研究仍缺乏实质性突破。

MTF的核心目标是最大化用户满意度。用户满意度通常通过加权计算单次推荐或推荐会话中的多种反馈指标来评估，包括观看时长、有效点击、点赞、分享等行为。其中，推荐会话定义为用户从开始访问推荐系统到离开的完整过程，可能包含一次或多次连续请求。

如图1

在腾讯新闻、抖音和快手等推荐系统中，当前推荐结果会对后续推荐产生显著影响，特别是在同一推荐会话内。因此，我们需要同时考虑当前推荐的即时收益和整个会话内的长期累积收益。最近，部分研究[15,24,25]开始采用离线强化学习（RL）[26]来寻找最优融合权重，以最大化长期收益。与前述方法相比，RL不仅考虑会话内的累积奖励，还能推荐既满足当前用户需求又能带来长期正向交互的内容。此外，RL相比进化策略（ES）具有更强的模型性能和更高的样本效率[23]。目前，RL已在腾讯[15]等多家公司的推荐系统中应用于MTF任务。

然而，现有RL-MTF方法存在以下严重问题[15,26-31]：

- 1）为避免分布外（OOD）问题，现有离线RL算法采用了过于严格复杂的约束条件，严重损害了模型性能；
- 2）在线探索与离线训练相互割裂，离线RL算法无法感知训练数据背后的探索策略，也不再与真实环境交互，因此只能学习到次优策略；
- 3）现有探索策略效率低下且损害用户体验。

针对这些问题，我们提出了一种专门为推荐系统MTF任务设计的新方法IntegratedRL-MTF。首先，该方法将离线RL模型与我们的在线探索策略相结合。在离线训练时，可以直接获取探索策略生成的训练数据分布，从而放宽为避免OOD问题而设置的过度约束，显著提升RL模型性能。其次，我们设计了一种简单但极其高效的探索策略，不仅加快了模型迭代速度，还减少了对用户体验的负面影响，这对商业公司具有重要价值。最后，我们提出渐进式训练模式，借助高效探索策略通过多轮在线探索和离线训练的迭代，使目标策略快速收敛至最优策略。

我们使用自设计的新评估指标（该指标更简单且更适用于RL-MTF评估）在相同数据集上进行了离线实验对比。此外，在大规模推荐系统中进行的在线实验表明，我们的RL模型显著优于其他模型。IntegratedRL-MTF已在我们的推荐系统中稳定运行近一年，并推广至腾讯其他大型推荐系统，取得了显著效果提升。本文将重点阐述IntegratedRL-MTF的核心思想，不深入讨论实现细节。

本研究的主要贡献包括：

- 系统分析了现有RL-MTF方法，指出其存在约束条件过严影响性能、在线探索与离线训练割裂导致策略次优、传统探索策略低效损害用户体验等核心问题
- 提出面向大规模推荐系统MTF的定制化RL算法，通过离线RL与探索策略的融合放宽约束条件提升性能，并采用渐进式训练模式实现策略快速收敛
- 在腾讯新闻短视频频道进行实验验证：离线实验采用新设计的评估指标，在线A/B测试显示模型显著优于基线（用户有效消费时长提升+4.64%，用户停留时长提升+1.74%）

## 2 问题定义

本节给出腾讯新闻短视频频道（与抖音类似）中RL-MTF的问题定义。如前所述，在当前推荐会话中，推荐结果会对后续推荐产生显著影响。在每个时间步$t$，推荐系统（RS）接收到用户请求后：
1. 首先从数百万内容中筛选出数千候选项目
2. 多任务学习（MTL）模型预测每个候选的多种用户行为得分
3. 多任务融合（MTF）模型使用公式(1)生成融合权重，将MTL模型输出的多个得分组合为最终得分
4. 最后将推荐列表发送给用户，并将用户反馈上报至平台数据系统

我们将上述融合问题建模为推荐会话内的马尔可夫决策过程（MDP）。在这个MDP中，推荐系统作为智能体与用户（环境）交互，进行序列化推荐，目标是最大化会话内的累积奖励。该MDP框架包含以下关键组件[26]：

- **状态空间$\mathcal{S}$**：是状态$s$的集合，包括用户画像特征（如年龄、性别、top K兴趣、刷新次数等）和用户历史行为序列（如观看、有效点击、点赞等）

- **动作空间$\mathcal{A}$**：是RL模型生成的动作$a$的集合。在我们的问题中，动作$a$是一个融合权重向量$(w_1,...,w_k)$，其中每个元素对应公式(1)中的不同幂次项或偏置项

- **奖励$\mathcal{R}$**：当推荐系统在状态$s_t$采取动作$a_t$并向用户发送推荐列表后，用户对这些内容的各种行为将上报至RS，基于这些行为计算即时奖励$r(s_t,a_t)$

- **状态转移概率$\mathcal{P}$**：转移概率$p(s_{t+1}|s_t,a_t)$表示采取动作$a_t$后从状态$s_t$转移到$s_{t+1}$的概率。在我们的问题中，状态包含用户画像特征和用户历史行为序列，因此下一状态$s_{t+1}$取决于用户反馈且是确定性的

- **折扣因子$\gamma$**：决定智能体对未来奖励相对于即时奖励的重视程度，$\gamma \in [0,1]$

基于以上定义，在推荐系统中应用RL进行MTF的目标可以定义为：给定推荐会话内RS与用户以MDP形式交互的历史，如何学习最优策略以最大化累积奖励。

## 3 提出的解决方案
### 3.1 奖励函数
在推荐会话中，RS在状态$s_t$采取动作$a_t$计算每个候选的最终得分，并向用户发送推荐列表，随后用户的多种反馈会上报至RS，如图1所示。为了评估即时奖励，我们定义如公式(2)所示的即时奖励函数：

$$
r(s_t,a_t) = \sum_{i=1}^k \alpha_i \cdot b_i
$$

其中$\alpha_i$是行为$b_i$的权重。在我们的推荐场景中，用户行为$b_1,...,b_k$包括观看时长、有效消费（观看视频超过10秒）以及点赞、分享、收藏等交互行为。通过分析不同用户行为与用户停留时长的相关性，我们为这些行为设置了不同的权重。

### 3.2 在线探索
在训练RL模型之前，首先需要收集大量探索数据，这对模型性能有关键影响。然而，传统探索策略面临两个挑战[15,32]：

- **低效率**：在实践中，使用传统探索策略在大规模RS中收集足够的探索数据通常需要很长时间。例如，在我们的平台上使用动作噪声探索策略收集一次探索数据通常需要五天或更长时间。这影响了模型迭代速度并意味着收入损失

- **对用户体验的负面影响**：传统探索策略生成的过多探索动作（包括异常动作）会对用户体验产生显著负面影响，甚至导致用户流失，这是不可接受的


为解决上述问题，我们首先在推荐场景数据集上，对新学习的RL策略与基线RL策略在相同状态下生成动作的绝对差值分布进行了分析。为简化分析，我们将动作各维度的取值范围归一化至$[-1,1]$区间，并选取最重要的4维动作（包括有效消费、观看时长、播放完成率和正向行为率）进行说明，如图2所示。我们观察到，对于相同状态，新学习RL策略生成的动作通常不会与基线RL策略生成的动作产生显著偏离，这一现象也与我们的直觉相符。

基于此发现，我们提出了一种简单但极其高效的探索策略，如公式(3)所示，该策略根据基线策略为每个用户定义个性化的探索上下界：

$$
a_{explore} = a_{baseline} + \delta,\quad \delta \sim \mathcal{U}(lower_b, upper_b)
$$

探索动作由基线策略输出的动作加上由$lower_b$和$upper_b$定义的均匀分布随机扰动生成。我们通过统计分析精心选择了$lower_b$和$upper_b$的取值。该探索策略的核心思想是消除低价值探索空间，仅聚焦于探索潜在高价值的状态-动作对，如图3所示。相较于传统探索策略（本文以常用于生成探索数据的动作噪声探索策略为例，如图3珊瑚色曲线所示），我们的策略展现出极高的效率。在相同探索密度要求下，我们推荐场景中的探索策略效率约为动作噪声探索策略的210倍（具体分析见第4节）。此外，相比动作噪声探索策略，我们的策略能减少数据分布对RL-MTF模型训练的干扰。第3节详述的渐进式训练模式进一步扩展了探索策略的探索空间，因此可设置更小的个性化探索空间上下界。

```markdown
### 3.3 IntegratedRL-MTF：面向大规模推荐系统MTF定制的强化学习算法

为解决前文所述问题，我们提出名为**IntegratedRL-MTF**的新方法。下面将分别介绍其执行器网络（Actor Network）、评价器网络（Critic Network）和渐进式训练模式。

#### 3.3.1 执行器网络
执行器网络的目标是为特定状态输出最优动作。遵循常规设置，我们在学习过程中构建两个执行器网络：

- 当前执行器网络 $\pi(s)$  
- 目标执行器网络 $\pi'(s)$  

$\pi(s)$ 通过将执行器网络与我们的探索策略相融合，实现了以下创新设计（如公式4-5所示）：  

1. **约束松弛机制**：通过整合在线探索策略的数据分布知识，放宽传统RL-MTF的严格约束条件  
2. **多评价器一致性惩罚项**：基于多个评价器输出的一致性引入额外惩罚项，有效缓解外推误差  

数学表达为：  
$$
\pi(s) = \arg\min_a \left[ \mathcal{L}_{actor} + \lambda \cdot \mathbb{E}_{\xi\sim\mathcal{U}}[\max_{j}Q_j(s,a+\xi) - \min_{j}Q_j(s,a+\xi)] \right]
$$  
其中$\lambda$为调节系数，$\xi$为探索噪声，$Q_j$表示第$j$个评价器网络。


在训练$\pi(s)$期间，如第3.2节所述，可以直接获取每个用户探索数据分布的上界和下界。因此，我们可以利用这一特性来简化过于严格的约束条件，并充分发挥$\pi(s)$的能力。如果$\pi(s)$在状态$s_t$生成的动作处于用户的上界和下界范围内，则公式4中第二项的值为零，即不施加惩罚以避免影响模型能力。否则，将根据超出用户上界或下界的偏差施加惩罚。通过这种方式，当前actor网络的性能相比现有方法得到显著提升，这一点在第4节的实验中得到验证。

此外，我们还引入了一个惩罚机制，该机制定义为多个独立critics[33]输出估计值的标准差，以减轻外推误差，这是公式4中的第三项。由于我们的探索策略具有极高的效率，在用户上下界范围内收集的探索动作与传统动作噪声探索策略相比具有显著更高的平均密度，这对模型优化极具价值。此外，与高斯扰动相比，个性化上下界内的随机扰动减轻了数据分布对模型训练的干扰。如果$\pi(s)$输出的动作处于用户的探索空间内，公式4中第三项的值会很小甚至可以忽略。否则，将施加相应的惩罚来减轻外推误差。

目标actor网络$\pi'(s)$是一个辅助网络，负责基于下一状态生成下一最优动作，以缓解由bootstrapping引起的过估计问题。其参数会使用当前actor网络进行周期性的软更新。

### 3.3.2 Critic网络

Critic网络$Q(s,a)$负责估计推荐会话中状态-动作对$(s,a)$的累积奖励。$Q(s,a)$还将critic网络与我们的探索策略相结合以避免外推误差。在我们的解决方案中，创建了多个独立的critic网络，这些网络被随机初始化并独立训练。每个critic网络的目标是最小化TD-error，如公式6所示。如果$\pi'(s)$在下一状态$s_{t+1}$生成的下一动作处于用户的上下界范围内，公式6中第二项的值为零。否则，将根据超出用户规定上下界的偏差施加惩罚。实践中，我们通常将critic网络数量设为24，这足以在我们的推荐场景中取得良好效果。为了获得更好的性能，我们为每个critic定义了一个目标网络，其参数会使用相应的critic网络进行周期性软更新。

### 3.3.3 渐进式训练模式

离线RL的一个严重缺点是当模型离线训练时，它仅依赖于之前收集的数据而不再与真实环境交互。离线训练期间缺乏实时交互会导致学习策略与实际环境之间的差异，这对离线RL算法的性能产生显著负面影响[15,26-31]。

为了在大规模RS中缓解这个问题，我们的解决方案采用渐进式训练模式，通过高效的探索策略进行多轮在线探索和离线模型训练来学习最优策略，使目标策略能够快速收敛到最优策略。由于我们的探索策略效率很高，我们将之前的单次数据探索和离线模型训练划分为五轮在线数据探索和离线模型训练。最新学习到的策略将作为下一轮在线探索的基线策略。通过迭代高效地探索环境，学习到的策略将不断改进，从而进一步提升我们RL模型的性能。

## 3.4 基于RL-MTF的推荐系统

我们在腾讯新闻短视频频道实现了IntegratedRL-MTF，如图4所示。我们的RL-MTF框架由两个组件组成：离线模型训练和在线模型服务。离线模型训练组件负责预处理探索数据和训练RL-MTF模型。在线模型服务组件主要负责在接收到用户请求时生成个性化最优动作，计算每个候选的最终得分。此外，在线模型服务组件还负责在线探索以收集训练数据。

# 4.实验

略

[https://arxiv.org/pdf/2404.17589](https://arxiv.org/pdf/2404.17589)