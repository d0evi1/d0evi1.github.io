---
layout: post
title: kuaishou TPM介绍
description: 
modified: 2023-07-15
tags: 
---

kuaishou在《Tree based Progressive Regression Model for 观看时长预估 in Short-video Recommendation》提出了TPM的模型。

# 摘要

关于观看时长（watch time）的精准预估，对于视频推荐系统中增强user engagement来说非常重要。为了达到它，在一个观看时长预估框架中需要满足4种属性：

- 首先，**尽管观看时长是个连续值，它也是一个序数变量（ordinal variable），它的值的相对序会影响着用户体验上的差异**。因此，这种序数关系（ordinal relations）会在观看时长预估中被影响。
- 第二，视频观看行为间的条件依赖，可以在模型中被捕获。例如，**在用户完成观看整个视频之前，它必须已经看了视频的一半**。
- 第三，使用点估计（point estimation）来建模观看时长会忽略掉以下事实：模型会给出具有高度不确定的结果，这会造成badcase
- 第四，真实推荐系统会有偏差放大（bias amplifications），因而需要进行无偏估计

如何设计一个框架来解决上述4个问题仍是未被探索充分的。因此，我们提出TPM（Tree-based Progressive regression Model）来进行观看时长预估。特别的，观看时长的序数排名（ordinal ranks）会被引入到TPM中，该问题被解耦成一系列的条件依赖的分类任务，它们会以树结构进行组织。期望观看时长可以通过遍历该tree来生成，观看时长预估的变种会被显式引入到目标函数中作为不确定性（uncertianty）的一种measurement。再者，我们演示了后门调整（backdoor adjustment）可以被无缝混合在TPM中，它可以缓解偏差放大（bias amplifications）问题。

# 1.介绍

获得推荐内容的用户具有更高观看时长，趋向于在平台上留存更久，带来DAU的增长。

尽管它很重要，但观看时长预估在之前的研究中并没有被广泛研究。我们会讨论在观看时长建模上一些特别重要的方面：

首先，观看时长预估本质上是一个regression问题，但在观看时长预估间的序数关系（ordinal relation）是在推荐中很重要。

- 一方面，观看时长是一个连续随机变量，推荐系统需要获得一个关于观看时长的精准预估，才能在下游阶段进一步使用；
- 另一方面，观看时长是视频对比的一个指标，预估的顺序关系也非常重要。

例如，对于一个视频给定关于观看时长的两个预估：$$T_1 = 3.5s, T_2 = 4.5s$$，而ground truth是：$$T=4s$$。在MAE上两个预估具有相同的regression error：0.5s。然而，这两个预估会在推荐系统上导致非常不同的结果。**由于系统通常趋向于推荐具有更高预估观看时长（predicted watch time）的视频，具有预估值4.5s的视频，要比预估值为3.5s的视频更可能被推荐**。因此，使用直接回归来估计观看时长在建模观看时长的顺序关系时会失败。**ranking losses只关注于顺序关系，可能会导致预估值本身离ground truth偏离非常远**。因此，一个关于观看时长预估的好公式，可以同时满足两者。

第二，在视频观看行为中，存在着较强的条件依赖。例如，用户在看完整个视频前，肯定会看过一半的视频。这与电商平台上的点击（click）和post-click行为（购买）的情况相似。该条件依赖需要在观看时长预估上考虑。

第三，为了确保观看时长的一个健壮预估，我们期望：prediction模型对于它的预估值来说是uncertainty-aware的。对于大多数回归模型，目标是：通过最小化一个$L_1$或$L_2$ loss，来获得一个精准的点估计。因而，该模型必须生成具有较高不确定的预估。对于真实推荐系统来说，这会导致base cases：**模型会分配那些具有高排序的次优视频，造成不满意的用户体验**。然而，如何在观看时长建模上建模不确定性（uncertainty）仍是欠研究的。

第四，大多数真实推荐系统存在bias amplifications问题（例如：sample selection bias, popularity bias）。由于模型的训练数据通常从平台的日志中收集，这会造成非常严重的bias amplification。**根据之前的研究，视频推荐系统会偏向于具有更长duration的视频**，这验证了bias amplification的存在。

给定上述4个问题，我们会review在观看时长预估上的已存在研究。尽管这些方法解决了一些重要限制，并能达到一些较好效果，但这些方法均没考虑上述4个问题。我们会研究两个在观看时长预估上的SOTA方法：(WLR[3]和D2Q[19])，如表1所示。


表1

在WLR中，训练样本有正例（被点击的视频曝光）或负例（未点击的曝光）。观看时长预估会被看成是一个二分类问题（bianry classification），其中：正样本会在cross-entropy loss中加权上观看时长。因此，由分类学到的可能性近似等同于期望观看时长（expected watch-time）。尽管它简单有效，WLR仍有一些限制：**这会阻止使它在全屏视频推荐系统（full-screen：其中所有的视频曝光均会被观看）中的直接使用**。因此，WLR必须使用人工设计的正例和负例样本和权重（weights），这会造成在watch-time上的一个较差近似。同时，bias amplification效应在WLR会被放大，因为**更多的weights会被分配给更长duration的视频**。D2Q[19]通过将videos根据它们的durations进行划分成不同的groups来缓解duration bias，并在每个group使用传统的regression models进行建模观看时长。因此，在观看时长值上的普通关系和条件依赖会被忽略；再者WLR和D2Q会将观看时长预估看成是一个点估计问题（point estimation problem），因而预估的不确定性（uncertainty of predictions）会被忽略。

考虑到上面的4个问题，我们提出了一个新框架：TPM用于观看时长预估来解决它们。特别的，观看时长会被split成多个序数间隔（ordinal intervals），并且**观看时长预估等价于一个搜索问题，决定预估观看时长属于哪个interval**。该搜索过程会被建模成一个以树结构组织的一系列决策问题。**在树中的每个中间节点都代表了一个决策问题，哪会使用一个相应的分类器进行分配**。同时，每个叶子节点表示ordinal intervals的其中之一，它会使用一个观看时长的期望值来进行分配。接着，从upper level产生的结果会变成在当前节点决策的条件。因此，从root node到一个特定leaf node的path，对应于一个包含了一串决策的搜索路径（searching trajectory）。我们在图1中的一个示例会用于演示该框架。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/e657896d8451705c1713ffdbc794d0dd0abd7668fc7ef56e080aaeb6b718e65e6ea7e82b69fd81dfc3281c22654cf1f2?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1

我们解释了，TPM是如何解决这4个问题的：

- 首先，我们将ordinal ranks引入到观看时长的逼近中。**回归任务被解耦成多个二分类任务**，它们的labels与 ordinal ranks相关联。这种近似（approximation）会使用观看时长的连续性、以及在ranks间的顺序关系。
- 第二，我们将条件依赖引入到TPM中。一个子节点的每个任务依赖于来自父节点的任务之上。这种方式下，在多个解耦分类任务间的条件依赖是puch显式建模的。注意，有许多关于该预估任务的解耦模式，我们可以将任务的条件依赖编码到模型中。
- 第三，为了确保关于观看时长预估的框架足够健壮，我们会将模型不确定性引入到目标函数中。由于将观看时长划分成多个ranks，预估的观看时长可以被看成是一个来自multinomial分布的随机变量，我们可以显式计算预估观看时长的方差（variance），它可以被看成是关于模型不确定性（model uncertainty）的一个metric。由于它被引入目标函数中，因此，TPM会获得一个关于观看时长的高置信的精准预估。
- 第四，我们会在bias的混杂效应上进行因果分析，并展示了backdoor调整等价于在多分类任务上进行一个特定解耦。我们展示了该方法应用到不同的biases上，并且D2Q可以看成是 TPM的一个特例。

注意，TPM的结论与决策树相似，我们会讨论：它具有与回归的常规树模型间的显著差异：

- 1） TPM使有树结构来将纯回归问题解耦成一系列分类问题，因而，在label space（例如：观看时长 intervals的划分）上进行解耦；而树模型会将feature space划分到预估的子空间上。
- 2）在TPM中的tree-alike解耦，会将每个node分配一个相应的分类器（类似于neural networks）进行决策；而常规树模型则直接学习一个用于labeling的feature partition规则。

# 2.相关工作

## 2.1 观看时长预估

观看时间预测是工业推荐系统（特别是短视频和电影推荐系统）中最关注的问题之一。然而，据我们所知，这个领域只有少数几篇论文[3, 19]。第一项工作[3]专注于YouTube的视频推荐，并提出了加权逻辑回归（即WLR）方法来进行观看时间预测。WLR已成为相关应用中的最先进的方法。然而，这种方法不能直接应用于全屏视频推荐系统，并且由于其加权机制，WLR可能会遭受严重的偏差问题。D2Q[19]通过进行后门调整来减轻时长偏差，并使用直接的观看时间分位数回归来建模观看时间。然而，**这种方法忽略了分位数之间的序数关系和依赖性**。此外，这两种方法都是通过点估计来建模观看时间，没有考虑预测的不确定性。

## 2.2 序数回归（Ordinal regression）

序数回归是一种预测序数标签的技术，即标签的相对顺序很重要。它的应用可以在年龄估计[12]、单目深度估计[6]、头部姿态估计[7]中找到。尽管序数回归有广泛的应用，但它尚未被应用于观看时间预测任务。

大多数序数回归算法都是从经典分类算法修改而来。例如，SVM已经与多个阈值结合并应用于视觉分类[14]；另一个例子是与在线感知算法[4]的结合，用于评分预测。此外，类别属性中的排序信息被利用来将序数回归转化为多个分类问题[5]。值得注意的是，决策树在这项工作[5]中被使用。然而，序数回归中的二元分类问题不像TPM中的那些是条件依赖的，这是一个根本的区别。

## 2.3 基于树的神经网络推荐

基于树的模型和神经网络是各种机器学习应用中的强大模型，特别是在推荐系统中。基于树的方法如LambdaMART[2]在排名任务中相当有竞争力。同时，神经网络在利用稀疏和复杂特征方面实现了最先进的性能。

然而，很少有努力将两种方法的优势结合起来。早期的研究[10]尝试将决策树与神经网络结合起来进行搜索。两个模型通过集成学习技术（例如线性组合和堆叠）结合，组合模型比单一模型实现了更优越的性能。此外，树模型已被用于增强嵌入模型以实现可解释推荐[16]。

TDM[20, 21]是结合基于树的模型和神经网络进行推荐的示例。TDM的思想是将候选检索过程组织为沿树的搜索过程，以便以对数复杂度检索最受欢迎的候选。TPM与TDM在几个重要方面不同：首先，TPM旨在预测给定用户和相应视频的观看时间，而TDM旨在从庞大的素材库中检索相关候选；其次，TPM使用树结构进行问题分解，而TDM利用树结构进行素材划分；第三，TPM遍历树以预测预期观看时间，而TDM使用束搜索来搜索目标叶节点。

## 2.4 去偏推荐

为了解决推荐系统中的偏差问题，已经做了许多研究。关于此主题的先前研究大致可以分为以下三类：

- **逆倾向得分**：首先基于某些假设计算样本的倾向得分，然后样本在目标函数中以逆倾向得分进行加权。例如，曝光倾向已被用于解决随机缺失问题[13]；然而，这种方法的性能有时不稳定，因为估计的倾向得分方差较高。通过倾向得分裁剪或双重鲁棒学习[17]，可以通过数据填充来缓解这个问题。

- **因果嵌入**：这种方法[1]将相关嵌入分解为无偏成分和有偏成分。两个成分都在训练阶段使用，而在推理阶段丢弃有偏成分以获得无偏预测。

- **因果干预**：将偏差的原因引入方法中，并进行干预以消除它们对推荐的影响。随机化和后门调整是因果干预的两种代表性方法。然而，在实际推荐系统上进行随机实验成本很高，**后门调整[15, 19]在实际场景中更受青睐**。因果干预方法已被用于观看时间预测中，以消除时长偏差。我们展示了后门调整可以无缝地整合到TPM中，这种方法也适用于其他混杂因素。

# 3.TPM

## 3.1 TPM的公式化

给定一个训练实例（𝑋,𝑇），其中：

- 𝑋代表用户和视频的特征，
- 𝑇代表观看时间

观看时间预测的目的是要找到一个模型M，使得在某些度量下，预测M(𝑋)接近𝑇。

我们不是将问题视为直接回归，而是首先将尺度量化为序数等级：$\lbrace 𝛾_0 \leq 𝛾_1, \cdots, 𝛾_𝑘, \cdots, \leq 𝛾_𝑚 \rbrace$，然后将观看时间预测视为预期序数等级的估计。从序数等级的估计类似于具有迭代比较的搜索过程。

例如，如果我们沿着等级进行线性搜索，搜索过程如下：

我们首先判断：$𝑡(𝑥) ≤ 𝛾_0$ 还是大于$𝛾_0$。 如果$𝑡(𝑥) ≤ 𝛾_0$，则预测的序数等级是$𝛾_0$；否则，我们继续判断：$𝑡(𝑥) ≤ 𝛾_1$，还是大于$𝛾_1$。如果为true，那么等级是$𝛾_1$，否则这个过程继续。这个过程一直进行，直到最终找到区间。

如果我们进行**二分搜索**，那么搜索过程就变成了：我们首先决定$𝑡(𝑥) \leq 𝛾_𝑚/2$：如果为true，那么𝑡(𝑥)属于集合$\lbrace 𝛾_0, 𝛾_1, \cdots, 𝛾_𝑚/2 \rbrace$；否则它属于$\lbrace 𝛾_𝑚/2+1, \cdots, 𝛾_𝑚 \rbrace$的等级。这个过程继续，搜索空间缩小到某个等级。

注意，每个搜索过程由一系列决策组成，我们提出将观看时间预测模型拟合成从根到叶节点沿树的搜索过程。（见图2）：对于线性搜索情况，树是一个不平衡的二叉树；对于二分搜索情况，它是一棵平衡的二叉树。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/907ace52c6e542b6633a6770c667fd356e324a0b7532f00e1b9ef29a4eae1d066295679934aa47b121c9b185549bd2ab?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 TPM中两个分解树的示例

因此，我们提出了一个基于树的渐进模型（TPM）用于观看时间预测。模型由问题分解的树T和相应的分类模型$\lbrace M_𝑖，𝑖 \in \lbrace 0, 1, ..., \mid 𝑁_T \mid − \mid 𝐿_T \mid \rbrace \rbrace$组成，其中：

- $𝑁_T$是T中的节点集合，
- $𝐿_T$是T中的叶节点集合（见图2为概览）

TPM中的树T由一组节点组成：$T(X) = \lbrace 𝑁_T ∪ 𝐿_T \rbrace$。

- 每个**非根节点$n_i$**：代表一个由连续序数等级组成的区间，即$𝑛_𝑖：[𝛾_{𝑠_𝑖},𝛾_{𝑒_𝑖}]，𝑒_𝑖−𝑠_𝑖 > 1$。不失一般性，根节点假定为全空间：$𝑇 ∈ [𝛾_0,𝛾_𝑚]$。
- 每个**叶节点$l_i$**：被分配一个特定的序数等级，即$𝑙_𝑖：[𝛾_𝑖, 𝛾_{𝑖+1}]$。并且父节点的子空间是其子节点的子空间的并集。
- **从根到叶$𝑙_𝑘$的路径为$𝜙_{𝑙_𝑘}$**，表示为有序节点集：

$$ 
𝜙_{𝑙_𝑘} = \lbrace \hat{𝑛}_{𝜙_{𝑙_𝑘}(0)}, \cdots, \hat{𝑛}_{𝜙_{𝑙_𝑘}(𝑑(𝑙_𝑘))} \rbrace
$$

其中：

- $ \hat{n}_{\phi_{l_k}(𝑖)} $：是路径 $ \phi_{l_k}$上的第𝑖层的节点
- $ 𝑑(𝑙_𝑘)$：是叶节点$𝑙_𝑘$的深度

以下等式始终成立：

$$
\hat{𝑛}_{𝜙_{𝑙_𝑘}(𝑖)} ⊆ \hat{𝑛}_{𝜙_{𝑙_𝑘}(𝑗)}, ∀ 𝑖 ≥ 𝑗
$$

**在TPM中，每个非叶节点都被分配一个分类器，其输出指示给定父节点输出概率下，观看时间属于子节点对应序数等级的条件概率**。因此，给定一个实例𝑋和树T，其预测观看时间𝑇遵循如下多项分布：

$$
𝑝(𝑇 ∈ 𝑙_𝑘|𝑋, T) 
= 𝑝(𝑇 \in \hat{𝑛}_{𝜙_{𝑙_k}(𝑖)}, ∀ 𝑖 ≤ 𝑑(𝑙_𝑘)|𝑋, T) \\
= 𝑝(𝑇 \in \hat{𝑛}_{𝜙_{𝑙_𝑘}(𝑑(𝑙_𝑘))} | 𝑋, T, 𝑇 \in \hat{𝑛}_{𝜙_{𝑙_𝑘}(𝑖)}, ∀𝑖 < 𝑑(𝑙_𝑘)) · 𝑝(𝑇 ∈ \hat{𝑛}_{𝜙_{𝑙_𝑘}(𝑖)}, ∀𝑖 < 𝑑(𝑙_𝑘)|𝑋, T) \\
= \prod_{1≤𝑖≤𝑑}(𝑙_𝑘) 𝑝(𝑇 ∈ \hat{𝑛}_{𝜙_{𝑙_𝑘}(𝑖)}|𝑋, T,𝑇 ∈ \hat{𝑛}_{𝜙_{𝑙_𝑘}(𝑖 − 1)}) 
$$

...(1)

其中：

- $ p(𝑇 \in \hat{𝑛}_{𝜙_{𝑙_𝑘}(𝑖)} \mid 𝑋, T,𝑇 \in \hat{𝑛}_{𝜙_{𝑙_𝑘(𝑖 − 1)}})$ ：由分配给节点$ \hat{n}_{\phi_{l_k}(𝑖 − 1)} $的分类器$ M \hat{𝑛}_{𝜙_{𝑙_𝑘}}(𝑖−1)$参数化。

基于推导，给定树T的观看时间的期望可以如下计算：

$$
𝐸(𝑇 |𝑋, T) = \sum\limits_{ 𝑙_𝑘 \in 𝐿_T} 𝐸(𝑇 |𝑇 ∈ 𝑙_𝑘, 𝑋, T)𝑝(𝑇 ∈ 𝑙_𝑘|𝑋, T)
$$

... (2)

注意，期望涉及项$𝐸(𝑇 |𝑇 ∈ 𝑙_𝑘, 𝑋, T)$，这可以通过任何预测模型估计。在本文中，我们采用简单的方法进行估计：

$$
𝐸(𝑇 |𝑇 \in 𝑙_𝑘, 𝑋, T) = (𝛾_{𝑙_𝑘} + 𝛾_{𝑙_{𝑘+1}})/2
$$

通过构建多个树进行观看时间估计，可以通过将树的分布作为先验来计算观看时间的期望：

$$
𝐸(𝑇 |𝑋) = \sum\limits_T 𝐸(𝑇 |𝑋, T)𝑝(T |𝑋) = ∑T 𝐸(𝑇 |𝑋, T)𝑝(T) 
$$

...（3）

尽管TPM允许如方程3所示的预测打包方案，我们为了简单起见，将树的数量限制为一个。


## 3.2 树的构建

请注意，在TPM中对树的类型没有限制，树的结构可以根据任务和数据集进行设计。在TPM中，每棵树对应于序数等级的一个分解，树内的每个非叶节点对应于一个分类器。正如先前的研究[8, 9]所揭示的，标签不平衡为预测建模增加了难度，我们尝试构建每棵树以实现每个节点的平衡标签分布。

因此，我们计算观看时间的分位数并将它们设置为序数等级的依据。然后，我们将序数等级分成两半，并在完整二叉树的叶节点中设置它们。树是通过反复将两个子节点合并为一个父节点来构建的。因此，每个节点的分类器是一个二元分类器，并且每个分类器的标签分布是平衡的。例如，当观看时间的尺度被均匀分成4个区间时，构建的树对应于图2中的平衡二叉树。

## 3.3 不确定性建模

先前的方法[3, 19]专注于通过点估计来建模观看时间，然而，我们不知道应该对预测结果有多大的信心。我们展示TPM不仅对预期观看时间的误差进行建模，还尝试最小化其预测的不确定性。

请注意，给定一个问题分解的树，TPM预测观看时间属于序数等级的概率。因此，观看时间变成了一个随机变量，遵循多项分布𝑝(𝑇 ∈ 𝑙𝑘|𝑋, T)，∀𝑙𝑘 ∈ 𝐿T。这种用分布预测观看时间的特性非常有用，因为它使我们能够近似估计观看时间的方差：

$$ 
\text{Var}(𝑇 |𝑋, T) = \mathbb{E}(𝑇^2 |𝑋, T) - \mathbb{E}(𝑇 |𝑋, T)^2 \quad
$$
... (4)

请注意，$𝑝(𝑇 \in 𝑙_𝑘 \mid 𝑋, T)$可以通过方程1计算，方差可以在假设$𝑇 ∼ 𝑝(𝑇 \in 𝑙_𝑘 \mid 𝑋, T)，∀ 𝑙_𝑘 \in 𝐿_T$的情况下轻松计算。

图3中有一个简单的例子来说明这个想法：假设观看时间的尺度被分成八个序数等级，两个模型M𝑎和M𝑏的预测具有相同的观看时间期望值：𝐸(𝑇) = 4.5。然而，很容易验证这两个预测具有不同的方差：$ \text{Var}_{M_a}(𝑇) > \text{Var}_{M_b}(𝑇) $。这表明M𝑎对其预测结果更不确定。我们期望一个模型能够以高确定性正确估计观看时间。因此，我们明确地将预测观看时间的方差加入到TPM的目标函数中。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/0563c66021c81ae1be52ee9720f06c3576fc376738609eb8cc388ca008ecbac9e21e0a7be66ecd6e367ad81f698fa577?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3

## 3.4 使用TPM进行训练

现在我们介绍TPM的训练过程。给定一个训练样本（𝑋,𝑇）和TPM中的树T，我们首先确定样本的序数等级和对应的叶节点𝑙𝑘(𝑇) ∈ 𝐿T。然后确定从根节点到叶节点的路径，并将样本与路径上的分类器关联起来。每个分类器以𝑋为输入，标签与路径上的子节点相对应。在本文中，T是一个平衡的二叉树，每个非叶节点都被分配了一个二元分类任务。对于每个分类器，属于右手子节点的样本被视为正样本。

考虑图2中的例子，给定一个样本（𝑋,𝑇）和𝑇 = 0.8，这个样本与分类器M0和M2关联，对于这两个分类器来说，它都是一个正样本。

TPM的目标函数由三个部分组成：

- 路径上的分类器的分类误差：TPM试图最大化关于𝑝(𝑇ˆ ∈ 𝑙𝑘(𝑇)|𝑋, T)的似然，其中𝑇ˆ是预测的观看时间。
- 预测方差：𝑉 𝑎𝑟(𝑇ˆ|𝑋, T)。为了更容易优化，我们在损失函数中使用标准差：𝑉 𝑎𝑟(𝑇ˆ|𝑋, T)^0.5。
- 回归误差：评估最终预测的观看时间与真实值之间的差异：|𝑇 − 𝐸(𝑇ˆ)|。

最终的目标函数是三个部分的加权和：

$$ 
\max.L = \alpha_1 \log(𝑝( \hat{𝑇} ∈ 𝑙𝑘(𝑇)|𝑋, T)) - \alpha_2 \text{Var}( \hat{𝑇} |𝑋, T)^{0.5} - \alpha_3 \|\mathbb{E}(\hat{𝑇}) - 𝑇\|^2 \quad (5) 
$$

训练过程在算法1中有说明。

## 3.5 结合后门调整

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/6c8dd75a4134135d44f31a481284c31079484a61f0aa0ddf9a5eec5afeb60b299dae052f5cfe85c534b506b0d723c142?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=4.jpg&amp;size=750">

图4

现在我们介绍后门调整如何无缝适应TPM以实现去偏推荐。首先，我们展示图4中的因果图来说明偏差对观看时间预测的影响。将混杂因素表示为𝐷，特征表示为𝑋，观看时间为𝑇，变量间的作用关系体现在边缘上：

- 𝐷 → 𝑇：混杂因素直接影响观看时间。这应该被模型捕获以进行准确估计[15, 19]。
- 𝐷 → 𝑋：混杂因素隐式影响特征表示。这应该被消除，以避免偏差放大。
- 𝑋 → 𝑇：特征表示直接影响观看时间，包括用户偏好和视频内容等。

不失一般性，我们假设𝐷遵循多项分布：𝐷 ∼ 𝑃 (𝐷 = 𝑑)，∀𝑑。通过阻断边缘𝐷 → 𝑋可以得到去混杂的观看时间估计[15, 19]：

$$
 \mathbb{E}(𝑇 |𝑑𝑜 (𝑋)) = \sum_{𝑑} 𝑃 (𝐷 = 𝑑|𝑑𝑜 (𝑋))\mathbb{E}(𝑇 |𝑑𝑜 (𝑋), 𝐷 = 𝑑) \quad (6) 
$$

$$ = \sum_{𝑑} 𝑃 (𝐷 = 𝑑|𝑋)\mathbb{E}(𝑇 |𝑋, 𝐷 = 𝑑) \quad (7) 
$$

$$
= \sum_{𝑑} 𝑃 (𝐷 = 𝑑)\mathbb{E}(𝑇 |𝑋, 𝐷 = 𝑑) \quad (8) 
$$

注意：

$$ 
\mathbb{E}(𝑇 |𝑋) = \sum_{T} \mathbb{E}(𝑇 |𝑋, T )𝑝(T |𝑋) 
$$
$$ 
\mathbb{E}(𝑇 |𝑋, 𝐷 = 𝑑) = \sum_{T} \mathbb{E}(𝑇 |𝑋, T, 𝐷 = 𝑑)𝑝(T |𝑋, 𝐷 = 𝑑) 
$$

我们有：

$$ 
\mathbb{E}(𝑇 |𝑑𝑜 (𝑋)) (9) = \sum_{𝑑} 𝑃 (𝐷 = 𝑑)\mathbb{E}(𝑇 |𝑋, 𝐷 = 𝑑) (10) 
$$

$$ 
= \sum_{𝑑} 𝑃 (𝐷 = 𝑑) \sum_{T} \mathbb{E}(𝑇 |𝑋, T, 𝐷 = 𝑑)𝑝(T |𝑋, 𝐷 = 𝑑) (11) 
$$

$$ 
= \sum_{T} \sum_{𝑑} 𝑃 (𝐷 = 𝑑)\mathbb{E}(𝑇 |𝑋, T, 𝐷 = 𝑑)𝑝(T |𝑋, 𝐷 = 𝑑) (12) 
$$

具体来说，这表明我们可以通过根据混杂因素的分布构建树，并通过将样本分割到相应的树来进行后门调整。这可以通过将混杂因素的尺度分成组并相应地构建树来实现。同时，训练数据应根据组进行分割，每个组中的分类器分别用分割后的数据进行训练（见图5为例）。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/8e8e6d761d7584394a5621544c2da4bc1c956539552167a5d757d6abac676da2530dae84a03a591a25c8b77f9ec7b6ff?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=5.jpg&amp;size=750">

图5

具体来说，我们可以将𝐷注入TPM如下：

$$ 
L1 = 𝑝(𝑇 ∈ 𝑙𝑘 |𝑋, 𝐷, T) = \prod_{1≤𝑖≤𝑑(𝑙𝑘)} 𝑝(𝑇 ∈ 𝑛ˆ𝜙𝑙𝑘(𝑖)|𝑋, 𝐷, T,𝑇 ∈ 𝑛ˆ𝜙𝑙𝑘(𝑖 − 1)) \quad (13) 
$$

$$ 
\mathbb{E}(𝑇 |𝑋, 𝐷, T) = \sum_{𝑙𝑘 ∈ 𝐿T} \mathbb{E}(𝑇 |𝑇 ∈ 𝑙𝑘, 𝑋, 𝐷, T)𝑝(𝑇 ∈ 𝑙𝑘 |𝑋, 𝐷, T) \quad (14) 
$$

$$ 
L2 = \text{Var}(𝑇 |𝑋, 𝐷, T) = \mathbb{E}(𝑇^2 |𝑋, 𝐷, T) − \mathbb{E}(𝑇 |𝑋, 𝐷, T)^2 \quad (15) 
$$

$$ 
L (𝑇 , 𝑋, 𝐷, T) = \alpha_1L1 − \alpha_2L2 − \alpha_3 \|\mathbb{E}(𝑇ˆ) −𝑇 \|^2 \quad (16) 
$$

训练过程在算法2中有说明。

## 3.6 模型架构

请注意，TPM不限制分类器的架构，任何二元分类器的架构都适用于TPM。因此，我们采用多层感知器作为分类器的主干结构。架构在图6中有说明。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/0230c540be2b9856cfa70c78c3a6add652b38a4d7b7ef838fc1927963cd2804c03492a195a7f8df14fb57da8fc054927?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=6.jpg&amp;size=750">

图6

由于树中的每个非叶节点对应于一个二元分类任务，一个简单的设计是为每个节点构建一个分类器，其中分类器独立训练。然而，这将导致相当大的模型尺寸，因此不适用于实际环境。因此，我们通过在任务之间共享隐藏层的参数来为所有分类任务设计一个单一模型。同时，引入任务特定的输出层以产生每个节点的输出。

# 4.实验

略

# 

- 1.[https://arxiv.org/pdf/2306.01720.pdf](https://arxiv.org/pdf/2306.01720.pdf)