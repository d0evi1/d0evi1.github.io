---
layout: post
title: ali的XDL介绍
description: 
modified: 2021-11-01
tags: 
---

# 介绍

ali在2020《XDL: An Industrial Deep Learning Framework for
High-dimensional Sparse Data》提出了它们的XDL平台。我们看下它的实现：


## 摘要
随着数据和计算能力的快速增长，基于深度学习的方法已成为许多人工智能问题的主流解决方案，如图像分类、语音识别和计算机视觉。包括Tensorflow、MxNet和PyTorch在内的几个优秀的深度学习（DL）框架已经开源，进一步加速了社区的进步。然而，**现有的DL框架并不针对涉及高维稀疏数据的应用程序设计**，这种数据在许多成功的在线业务中广泛存在，如搜索引擎、推荐系统和在线广告。在这些工业场景中，深度模型通常在大规模数据集上进行训练，这些数据集包含高达数十亿的稀疏特征和数千亿的样本，给DL框架带来了巨大挑战。

在本文中，我们介绍了一个高性能、大规模且分布式的DL框架XDL，它提供了一个优雅的解决方案，填补了现有DL框架的通用设计与高维稀疏数据引起的工业需求之间的差距。自2016年以来，XDL已在阿里巴巴成功部署，服务于许多生产环境，如在线广告和推荐系统。在数百个GPU卡上并行运行，XDL可以在仅几个小时内训练具有数千亿参数的深度模型。除了其卓越的性能和灵活性，XDL对开发人员也非常友好。阿里巴巴的算法科学家可以用几行简单的代码开发和部署新的深度模型。XDL API和参考实现作为开源包在2018年12月发布，遵循Apache 2.0许可，并可在[https://github.com/alibaba/xdeeplearning](https://github.com/alibaba/xdeeplearning)找到。

## 1 引言
在过去的十年中，作为人工智能最令人兴奋和强大的分支之一，深度学习在语音识别、计算机视觉、自然语言处理和医学诊断等领域取得了重要的突破。得益于许多现实世界应用中产生的天文数量级的数据，以及图形处理单元（GPU）等基础设施带来的前所未有的计算能力，以及TensorFlow[1]、MxNet[6]、Caffe[18]等复杂的开源深度学习框架，深度学习技术已被广泛用于解决现实世界问题。

尽管现有的深度学习框架在许多领域取得了巨大成功，但它们并不针对涉及高维稀疏数据的应用程序设计友好。高维稀疏数据在许多互联网规模的应用中广泛存在，如搜索引擎、推荐系统和在线广告。例如，在我们的展示广告系统中，我们每天生成的用户行为日志数据量达到PB。**从这些数据中提取的训练样本包含数十亿个特征，而每个样本只有少数几个维度是非零的**。

与语音识别、计算机视觉和自然语言处理等数据密集型应用不同，这些互联网规模的在线业务，数据稀疏，对深度学习框架提出了独特的挑战。

- 首先，问题的规模如此之大，以至于**无法在单个节点上解决**。需要并行化来解决这种规模的问题。
- 其次，**数据极其稀疏**。如果处理不当，稀疏性可能导致效率低下。
- 第三，这些数据的表述并不规范。**样本之间可能有重复的特征**，给带宽带来高压。

此外，在快速模型演化中，需要添加或删除特征以测试其与目标的相关性。现有的深度学习框架并不设计友好地解决这些挑战。这种差距使得这些业务难以充分从蓬勃发展的深度学习技术中受益。

通过抽象我们在构建在线广告系统的深度学习解决方案中的实践经验，涉及高维稀疏数据的深度模型通常包括两部分：sparse特征学习和dense模型学习。稀疏特征学习是将大量高维稀疏数据转换为密集特征的过程，而密集模型学习则负责基于密集特征学习最优模型结构。这两部分还需要智能连接，以便这两个学习任务可以相互交互以实现最佳结果。

在本文中，我们介绍了XDL，这是一个为涉及高维稀疏数据的学习任务设计的高性能、大规模且分布式的深度学习框架。在稀疏特征学习部分，XDL提供了一个精心设计的分布式系统，对I/O、数据管道、通信和GPU进行了深入优化，提供了极高的效率和可扩展性。用户可以使用embedding字典或像CNN/RNN这样的深度模型将稀疏项目特征或图像/文本特征映射到密集表示。在密集模型学习部分，XDL采用任何开源深度学习框架作为其后端，并采用一种名为桥接技术的新技术支持。有了这种设计，XDL能够加速互联网规模学习问题的培训，并在端到端生产系统中提供服务。

在对XDL进行真实世界数据集评估时，我们发现它至少可以比Tensorflow/MxNet的原生分布式版本快5倍。

自2016年以来，XDL已在我们公司的核心业务中部署，如电子商务推荐和在线广告。在数百台服务器上运行，XDL在仅几个小时内训练了数十亿参数的多个模型。除了其性能和灵活性，XDL致力于隐藏复杂的工程细节。使用XDL，我们的算法科学家可以用几行简单的代码开发和部署新模型。这种系统设计带来了灵活性，并解锁了许多算法创新。

## 2 相关工作

随着问题规模和复杂性的增加，我们面临着深度学习框架性能和可扩展性的更严重挑战。传统的解决方案如TensorFlow[1]、MxNet[6]和Caffe[18]通常在单机上运行，并为分布式训练提供有限支持。**即使有多个GPU，这些单机解决方案也难以处理具有$10^{11}$样本和$10^{10}$参数的模型训练**。因此，分布式训练平台似乎是一个有希望的解决方案，许多学者在这方面做出了巨大贡献。

MPI[14]定义了一组接口，用于机器之间的通信和协调。由于其兼容性和可扩展性，MPI在分布式内存应用的并行计算中被广泛使用。许多机器学习框架都建立在MPI之上。

- Chen等人提出了RABIT[5]，这是一个AllReduce库，通过增加容错属性改进了OpenMPI。
- Adam Coates等人提出了COTS HPC技术[3, 9]。他们在InfiniBand互连上用MPI构建了一个GPU服务器集群。仅用16台机器，这个系统就可以有效地训练具有110亿参数的模型。

然而，这些框架的数据处理部分过于简化。因此，它们既不能充分利用数据特性，也不能处理大规模稀疏数据的工作。此外，**基于MPI的系统在容错方面支持不足**，因此在一些工业场景中导致了可用性问题。

另一个广泛使用的范式是**参数服务器（PS）**，它可以利用键值存储适当处理稀疏数据。

- Dean等人[11]首次引入了PS，并使用downpour SGD在谷歌训练大规模深度网络。
- Petuum[30]引入了有界延迟模型到PS架构[23]，实现了异步计算。

参数服务器在工业中也广泛使用。

- DSSTNE[2]是亚马逊设计的DL框架，特别适用于大型稀疏数据集。它提供了极其高效的自动模型并行多GPU支持和100%确定性执行。模型并行和数据并行同时支持。
- Kunpeng[34]是阿里巴巴基于PS的分布式学习系统，支持像稀疏逻辑回归和多项式加性回归树这样的模型。

这些基于PS的框架通常设计用来支持传统模型，如逻辑回归或卷积神经网络，它们只包含稀疏部分或密集部分。**这些框架很难处理涉及大规模稀疏特征和定制深度网络的模型**，如[12, 31, 32]中所述。

另一方面，机器翻译[29]和语音识别[3]等自然语言处理系统使用类似的SparseNet + DenseNet模型结构。然而，它们通常只有多达百万个单词，而我们通常有数十亿个项目ID或图像作为特征。

另一个活跃的研究领域是**自动寻找分布式模型训练的最佳运行时配置**。

- [27]使用强化学习来寻找最优设备放置。
- [19, 20]基于Legion[4]搜索混合并行策略的最佳配置，Legion是一种用于异构和并行机器的高性能编程系统。

然而，**这些算法都没有考虑到稀疏性**，也没有为如此规模的问题提供解决方案。

这些相关工作为构建高效的模型训练系统提供了许多见解。通过学习它们的优缺点，我们开发了XDL以支持涉及高维稀疏数据的工业问题。

## 3 XDL架构

在本节中，我们介绍XDL的架构和设计哲学。在讨论之前，有必要描述我们设计的动力。

### 3.1 涉及高维稀疏数据的深度模型的网络特性

近年来，基于深度学习的方法成功地革新了在线业务中应用的算法，并取得了最先进的性能。作为先驱工作，DSSM[17]提出在网络搜索引擎的场景中用深度网络建模查询和文档的相关性。DSSM模型的核心架构遵循embedding&多层感知器（MLP）范式：首先将高维sparse输入映射到低维embedding空间，然后使用多层感知器（MLP）拟合标签。这种embedding&MLP架构激发了大多数后续工作设计具有高维sparse输入的深度模型，这些模型来自视频推荐、网络搜索和电子商务网站广告等各种应用。代表性网络包括Wide & Deep Learning[8]、Youtube深度模型[10]、DeepFM[15]、深度兴趣网络（DIN）[32, 33]和CrossMedia[12]等。关于推荐系统深度模型的详细调查可以在[31]中找到。

从算法的角度来看，基于embedding&MLP的网络抽象了这些工业模型家族，这些模型通常将从稀疏数据中学习分为两个步骤：

- i）**表示学习**（representation learning），从高维sparse输入中捕获信息并将它们embedding到低维空间，
- ii）**函数拟合**（function fitting），模型denseembedding表示和监督label之间的关系。

为了简单起见，我们称第一步的网络为SparseNet，第二步的网络为DenseNet。图1说明了这种抽象。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/f7fba88f80bcd0e5b8daf2525abf3463dcc924361289bf9507e36343e1068c7d29d44089b1c2bdb58d7cb947a67d9397?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 embedding和多层感知器（MLP）网络架构的抽象，主要包含SparseNet和DenseNet。SparseNet将原始的sparse输入（维度为D，其中D可扩展至数十亿）映射到低维表示（维度为d，传统上扩展至数千）。DenseNet学习拟合数据的函数

在工业应用场景中，新的深度模型应该快速发展以提高业务利润。因此，算法科学家希望训练系统易于理解和使用：新模型应该在脚本代码中以独立视角设计，并并行运行，将复杂的分布式训练细节隐藏在后台。基于这些考虑，我们设计了XDL，这是一个工业深度学习框架，旨在构建一个高性能系统来训练涉及高维稀疏数据的深度模型。我们将展示我们如何应对高维稀疏数据带来的挑战，并提出一个遵循桥接策略的优雅设计。

### 3.2 XDL的设计哲学和桥接方法论

从上述内容中，我们可以看到涉及高维稀疏数据的模型中的SparseNet和DenseNet需要不同的系统功能。DenseNet由几个密集层组成，需要在本地机器上进行高计算密度。包括Tensorflow、MxNet和Pytorch在内的几个DL框架已经开源，这些框架可以很好地处理这类DenseNet。然而，SparseNet包含数千亿个特征，这些特征来自原始样本。因此，能够处理这种情况的成功DL框架必须具有出色的分布式特性和足够的稀疏数据计算能力。上述提到的现有DL框架并不针对涉及高维稀疏数据的网络设计良好。

为了更好地理解训练涉及高维稀疏数据的深度模型的挑战，让我们详细看看SparseNet和DenseNet的规模。首先展示我们数据集的规模。表1显示了我们在展示广告系统中一天使用的典型生产数据量。

表1

表2显示了我们日常任务中使用的一些模型的网络参数统计。显然，SparseNet是这些模型中贡献最大困难的部分。DenseNet遵循现有深度学习框架所持有的传统设置。此外，SparseNet需要处理输入数据的实际I/O问题，其体积为数TB，以及复杂和繁重的并行问题。这些挑战使得SparseNet的训练与DenseNet相比截然不同且至关重要。

| 模型       | 输入维度   | 网络参数   | 输出维度 |
|------------|------------|------------|------------|
| MLP        | 10亿       | 18亿       | 1440       |
| DIN        | 10亿       | 18亿       | 33438      |
| CrossMedia | 2.5亿      | 5亿        | 1464       |

| 模型       | 输入维度 | 网络参数   | 输出维度 |
|------------|----------|------------|------------|
| MLP        | 1440     | 1.2万      | 1          |
| DIN        | 33438    | 1.7万      | 1          |
| CrossMedia | 1464     | 1万        | 1          |

表2: 多层感知器、深度兴趣网络[32]和Crossmedia[12]模型的网络统计。

综上所述，我们设计了XDL，其架构如图2所示。遵循**分而治之的策略**，我们提出了一种全新的桥接架构，其中训练系统由两个主要子系统构成：

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/3314d9a4665acf88cfc0357f5f2fa1117ebea7bc1f16b7c8295387e966a24b7c6f2cfca2ce28163740902dab33568518?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 XDL架构。服务器和worker不需要具有相同的等级。它们可以部署在同一物理节点上，以提高通信效率。

- **高级模型服务器（AMS）**。AMS提供了一个精心设计的分布式系统，为训练SparseNet提供极高的效率和可扩展性。AMS处理快速演变的表示学习算法，这些算法具有大规模sparse输入。支持embedding字典和像CNN/RNN这样的模型，将大型sparse输入映射到密集向量。

- **后端worker（BW）**。BW遵循常见的深度学习设置，从低维输入中学习，并允许采用任何开源DL框架作为其后端。有了这种灵活性，我们的算法科学家可以轻松尝试涉及DNN[17]、注意力机制[32]或门控循环单元[33]的新模型结构，以模拟复杂且不断演变的用户行为。

在前向传递中，后端worker首先从I/O模块读取数据，然后向AMS发送特征ID请求。AMS将根据ID计算embedding向量并将其发送回后端worker。worker还将接收到上一次迭代的更新后的DenseNet模型参数。在收集完所有dense输入向量和模型参数后，worker将对输入向量执行sum pooling，并将结果输入到Dense中。在反向传递中，数据流被反转，worker计算梯度。AMS将收集DenseNet和SparseNet的梯度。参数更新将在服务器端使用像Adam[22]或Momentum SGD这样的求解器进行。关于AMS模块的实现和优化的更多细节将在下一节中描述。

XDL具有高性能和良好的可扩展性，因此能够支持工业生产。从2016年开始，XDL被用于训练目标广告部门的CTR预测模型。**在数百台机器上运行，XDL训练了一系列具有数十亿参数的DNN模型，支持数百种业务场景**。

为了便于在各种计算平台上部署，XDL可以被多种资源管理平台调度，如Yarn，并为各种数据存储系统提供数据I/O接口，如HDFS和Kafka。XDL的良好兼容性使得业务应用能够快速升级。

### 3.3 高级模型服务器

从参数服务器的架构设计中学习，我们设计了高级模型服务器（AMS），这是一个管理SparseNet和DenseNet训练的分布式系统。**AMS负责存储和更新所有参数，包括SparseNet和DenseNet**。因此，参数放置（Parameter Placement）成为高性能设计的关键时刻。考虑到在线业务的稳定性和可用性，容错也应得到充分考虑。

#### 参数放置（Parameter Placement）

由于sparse参数和dense参数都存储在AMS上，参数放置（Parameter Placement）算法应处理这两种参数的不同特性：

- 一方面，**sparse参数占用大量内存**，因为稀疏特征的维度可能高达数千亿。然而，每个mini-batch中，只有少数特征ID被worker请求，这表明sparse参数的I/O压力很低。此外，随着训练过程的进行，sparse参数的内存使用量会发生变化，因为样本会不断将新的ID带入sparse参数中。
- 另一方面，**dense参数占用的内存较少，但I/O压力较大**，因为worker在每个mini-batch中都请求整个dense参数。

根据上述分析，**我们的kv存储使用hash mapping作为AMS中sparse参数的低级数据结构**，这也存在于传统的参数服务器中。因此，我们可以在每个服务器上平均分配哈希表桶，减轻服务器上的内存存储压力。不幸的是，这个过程会导致worker请求数量大幅增加。为了解决这个问题，**我们在embedding字典查找之前，在worker端合并worker请求**。这种优化在实践中为我们提供了巨大的加速。至于dense参数，每个参数的I/O压力将在内存限制内平均分配在每个服务器上。

#### 容错

随着样本数量和模型复杂性的快速增长，特定深度模型的离线训练需要越来越多的时间。当DL系统中的某些角色失败时，从一开始就重新训练在时间和资源方面都非常昂贵。此外，在一些在线学习场景中，稳定性和可用性非常重要。因此，XDL中精心设计了容错机制。AMS由调度器和多个服务器组成。服务器使用定期心跳与调度器保持同步。在训练过程中，整个模型的快照将存储在某个位置。如果任何服务器由于某种原因失败，调度器将注意到异常状态，并设置整个AMS系统为不可用状态，直到服务器被Yarn或其他调度系统重启。如果AMS准备好了，调度器将通知服务器从最新的快照恢复并继续训练过程。

同时，由于worker负责读取样本，每个worker的读取状态也作为特殊参数存储在AMS中。当worker失败并重新启动时，它们可以通过从AMS拉取参数来恢复其读取状态。通过这种努力，worker在训练过程中不会丢失或重复读取任何样本。

#### 异步更新
XDL支持同步模式和异步模式。

- 在同步模式下，worker在每次迭代中拉取参数和推送更新各一次。**每次迭代直到所有AMS从所有worker接收更新并对参数应用平均梯度后才完成**。
- 在异步模式下，没有这样的迭代概念。worker独立操作。因此，每个worker可以在将更新推送到AMS后立即进入下一步，而不需要等待其他worker完成他们的更新。同时，在服务器端，来自worker的更新以无锁风格应用。我们使用这种方法，**因为embedding表的更新非常稀疏，冲突很少发生**。对于密集DNN，尽管每个更新可能不会成功应用于整个模型，但总是只有一个全局模型版本。因此，**无锁风格更新可以被视为在模型上添加一个掩码以随机丢弃部分梯度**。

在实践中，我们发现异步模式可以实现更大的系统吞吐量，同时几乎不会损失模型性能。

## 4 系统实现和优化

### 4.1 I/O
在生产场景中，我们训练系统的输入，如样本和初始模型，通常存储在不同的位置。例如，**一些样本从在线队列系统中流式传输，而其他样本则从离线分布式文件系统中检索**。I/O模块设计的目标是：**最大化来自数据源（如HDFS、Kafka等）的输入带宽使用**。

众所周知，分布式ML系统中的I/O带宽已成为最大的瓶颈之一。由于我们使用现有的DL框架作为计算后端，如何更快地吞吐数据成为一个更关键的问题。因此，我们实现了各种优化以最大化I/O吞吐量。

首先，不同样本之间存在大量重复特征。**这些重复来自我们数据源的最开始：用户行为**。一个用户点击两个不同的链接将产生两个包含相同用户重复特征的不同样本。图3说明了重复的结构。**压缩可以优化存储、通信和计算效率**。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/2741f948ed61d268440f5a71eee41cbbc64f88fdcbb794ca4fe7d47b918de1216e1dcdafbaed19d3db4f2e0194039e5d?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3 重复

分层样本压缩：

XDL充分利用样本之间存在ID重复的事实。在预处理阶段，原始样本被组织成**多前缀树（multi-prefix trees）**，这大大减少了存储空间和通信成本。在训练阶段，许多前缀树被组合成用户定义的batch大小的mini-batch。这种压缩也为计算带来好处，因为它减少了重复的embedding向量计算。如果前缀树的总样本大小超过batch size，最后一个前缀树将被分割。

图4展示了分层样本压缩的一个例子。原始样本被组织成两个不同的3层前缀树。**树的第一层、第二层和第三层分别代表用户特征、广告特征和创意特征**。在训练阶段，最后一个前缀树根据batch size=6进行分割。构建了两个辅助张量（指示器），以指示相邻层之间的关系。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/f7251ef6b4bca7a0c5695000eb70e523f8488a6a6b4c1f3ffc47fb856a31655bdc4c3ffc4f7e86c48a56726d177485df?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=4.jpg&amp;size=750">

图4 I/O数据和计算压缩。在数据预处理阶段，生成多前缀树以用于训练阶段的mini-batch生成。

### 4.2 workflow流水线

workflow流水线是加速程序运行过程的常用技术。通过多线程技术使程序中的阶段在时间线上重叠，可以节省相当的运行时间。XDL中也实现了几种workflow流水线。

为了最大化I/O吞吐量，我们用线程流水线处理过程。一个完整的训练迭代被分为三个阶段：

- 1）读取样本并将它们组合成mini-batch；
- 2）从键值存储或输入数据中预提取mini-batch的参数索引，以在服务器上进行计算；
- 3）拉取模型参数并进行前向/反向传播。

流水线利用线程并行性通过重叠执行这三个阶段。

如图5所示，这三个阶段的工作被调度到三个不同的线程池。这三个线程池通过一个无锁队列进行协调。**如果使用GPU，除了I/O实现中的流水线外，还设计了SparseNet和DenseNet之间的workflow流水线，以进一步提高XDL的性能**。在这种情况下，由于DenseNet的输入是SparseNet的输出，SparseNet的结果可以在DenseNet的前一个训练过程结束之前预计算。假设SparseNet和DenseNet的训练时间分别为$T_{sparse}$和$T_{dense}$，**总运行时间将从$T_{sparse} + T_{dense}$减少到$max{T_spar se, Tdense}$**。在这种情况下，训练过程的效率显著提高，**模型性能损失很小，因为当SparseNet的输出被预提取时，可能存在一些未更新的稀疏ID**。因此，当相邻mini-batch之间有很少共同稀疏ID时，这种流水线通常被适应。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/9c7a6319eb5604bc377cca2bc1d9a38f187d09bf1f990a376a779ef0d57da3d1c022263e1fb895425b750078513bbcd0?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=5.jpg&amp;size=750">

图5 训练迭代由三个阶段组成，使用流水线来最大化并行性。

XDL还允许算法科学家用几行代码构建连接训练过程中任何阶段的流水线。**深度模型可以被划分为任意数量的阶段(stage)**。算法科学家在考虑性能和准确性时有多种选择，以在训练过程中构建流水线。

### 4.3 高级模型服务器的优化

为了提高AMS的效率，在各个方面都采用了优化。

为了加速embedding过程中的K-V查询，XDL利用了GPU的能力。GPU有两个优势：

- 首先，GPU的内存带宽更高（Nvidia P100为700GBps，Intel E5-2699 v4为100GBps）。
- 其次，它在数千个核心上并行运行（Nvidia P100有3584个核心）。

因此，我们可以在GPU上实现更快的embedding字典查找。**GPU的缺点是GRAM大小有限**。为了解决这个问题，只有embedding字典的索引将被预提取到GRAM中。

由于AMS在大多数情况下扮演参数服务器的角色，因此AMS和后端worker之间存在大量的网络通信。当并行度很高时，网络通信量可能非常大。在这种情况下，网络通信消耗的运行时间成为总运行时间的关键部分。XDL采用Seastar[28]作为其通信库，并在Seastar框架的基础上进行了许多有效的优化。涉及零拷贝和CPU绑定等技术，以确保网络通信不会成为整个训练过程的瓶颈。

在深度学习中，batch size是一个非常重要的超参数。一些模型在使用mini-batch size时表现出良好的性能，而一些模型更喜欢大batch。这要求DL框架能够处理不同的样本batch  size。正如我们之前提到的，**CPU绑定（cpu-binding）是AMS中的一项重要技术**。AMS中的每个线程都绑定到一个CPU核心，因此节省了不同核心之间的线程切换时间。

- 当样本batch size较小时，这种CPU绑定机制使AMS上的计算性能高。
- 然而，如果样本batch size很大，将有太多的ID需要由单个线程处理。

在这种情况下，AMS有一个自适应机制，其中创建了一个额外的线程池，用于处理大量的ID。有了这种优化，AMS在mini-batch和大batch样本上都表现出色。

### 4.4 使用XDL进行在线学习

在线学习方法最近在工业中被广泛使用，因为它能够实时捕捉客户行为的变化。一个有效的在线学习模型在许多业务场景中非常有价值。XDL为在线学习提供了一系列机制。

**特征入口过滤器（Feature Entry Filter）**

正如我们之前提到的，由于样本将新的ID带入sparse参数，sparse参数的内存使用量随着训练过程而变化。因此，模型存储在AMS上不断增加。因此，必须将ID的规模控制在一定水平。为了解决这个问题，XDL提供了几种特征入口过滤器机制，如**基于概率的过滤器和计数布隆过滤器**。在这种情况下，**低频ID将在训练过程之前被丢弃**，因为它们对模型的贡献较小。同时，在线模型的规模和内存存储得到很好的控制，这保证了在线系统的稳定性。

**增量模型导出**

随着持续的训练过程，当模型存储达到数百GB甚至更高时，全模型导出在时间和计算资源方面将变得极其昂贵。因此，XDL为在线训练和推理系统提供了增量模型导出。**每个增量模型只需要少量的存储空间**，并且非常容易部署在在线系统上。

**特征过期**

与特征入口过滤器具有相同目的，特征过期机制也控制模型的大小。在在线训练过程中，**长时间未更新的特征将变得无用**，可以被删除。XDL允许算法专家**编写自己的特征过期函数**，并定制特征过期计划。

**用户界面**

XDL为算法科学家提供了Python API，以便开发新的深度学习模型。这些API让用户专注于模型本身，而不必担心I/O效率、分布式策略、系统一致性和低级通信。在并行化模型上的编码和调试几乎与在串行模型上相同。通过隐藏并行化和优化的细节，用户可以非常容易地将新开发的模型部署到沙箱和生产环境中的大量GPU集群上。大多数情况下，科学家只需要修改DenseNet结构来测试新想法。用户可以通过运行用户定义的代码完全控制后端worker。修改数据流和SparseNet组件以尝试更激进的算法创新也非常方便。

# 5 XDL生态系统

除了XDL，阿里巴巴还开源了**XDL算法解决方案和Blaze推理引擎**，它们与XDL框架一起构成了一个生态系统。所有这些基于XDL的工具都可以在[https://github.com/alibaba/x-deeplearning](https://github.com/alibaba/x-deeplearning)找到。

XDL算法解决方案包括几个在阿里巴巴广告业务中成功采用的有效模型，如深度兴趣网络（DIN）[32]、深度兴趣演化网络（DIEN）[33]、基于树的深度匹配（TDM）[35]等。

Blaze推理引擎是一个高性能的推理引擎，它利用内核融合和低精度推理等技术，为在线广告业务启用了大规模实时计算。

XDL生态系统为广告行业树立了示例和指南。越来越多的公司采用XDL作为他们的离线或在线深度学习工具。

## 6 评估

在本节中，我们提供了前一节描述的优化策略的详细评估。实验在通过25Gbps以太网互联的节点上进行。每个节点配备了2个Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz（96核）和512GB RAM。在编译时，我们使用GCC 4.8.5，带有-O2 -fopenmp -mavx2标志。

在我们的实践中，当选择后端时，MxNet或TensorFlow的性能几乎相同，因为它们在DenseNet计算方面具有相同的性能。在以下评估中，我们选择TensorFlow作为XDL的计算后端。

### 6.1 数据集

在我们的实验中，我们使用[25]发布的开放数据集，训练了一个用于点击率（CTR）预测问题的深度神经网络（DNN）模型。该数据集是从淘宝的电子商务推荐系统中以1%的比例抽样的。该数据集的统计信息显示在表3中。

开放数据集包括三个方面：用户兴趣、广告特征和上下文。它们中的每一个都由高维稀疏ID表示。使用XDL的Python接口，**我们基于SparseNet + DenseNet抽象构建了模型**。

- 在SparseNet中，我们使用embedding字典将每个ID转换为维度为18的密集向量。
- 随后是一个分组sum pooling，将这些向量减少到23个向量。
- 在DenseNet中，我们使用一个3层全连接DNN，包含约100K参数。（在这个DNN中，输入层有23*18个节点，第一层有200个节点，第二层有80个节点，输出层有1个节点。）

模型使用点击label来估计CTR。

### 6.2 子系统评估

为了了解XDL的性能，我们将逐一评估优化策略，看看我们能从每一个策略中获得多少好处。以下实验在上述开放数据集上使用相同的环境运行。所有实验在200个worker（除了可扩展性实验）上运行，使用80个CPU模式的AMS。我们为每个worker分配8个CPU核心，每个AMS将占用一台机器中的所有96个CPU核心。XDL在异步模式下运行，更新是无锁的，如第4.3节所述。

#### 分层样本压缩：
I/O模块中的分层样本压缩对通信和计算都带来了巨大的加速。平均来说，每个用户ID在开放数据集中可以重复57次，这意味着那些用户共享的公共特征的压缩比可以达到57。为了评估使用样本压缩可以获得多少性能提升，我们重新抽样数据集以构建具有不同平均压缩比的新数据集。然后我们比较XDL在这些数据集上运行时的吞吐量。结果可以在图6中看到。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/4334d892095af1abb094250b9137b32b6ed492e9c7c7c4678fc952332326b9202da8b4aae561c019b1c8166e997b61c7?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=6.jpg&amp;size=750">

图6  压缩比与吞吐量。运行在200个工作进程上，batch size为5000。

**大batch size**

使用大batch size将减少迭代次数，因此可以大幅降低数据复制和通信的开销。尽管Keskar等人[21]发现使用大batch训练可能导致尖锐的最小值并损害泛化。**已有几种技术[13, 16]被提出来缓解这个问题**。因此，我们倾向于使用更大的batch size来提高吞吐量。不同batch size下AMS的吞吐量显示在图7中。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/2204f5daaf836edbc39552ddc83cbccfd549ba4809d5ea4c936685b9f444dbdedbbbb60beb763d8638588300c71c720f?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=7.jpg&amp;size=750">

图7 批量大小与吞吐量。压缩比为57，使用200个工作进程。

**可扩展性**

我们还测试了XDL的可扩展性，并将其与不同数量的worker上的原生TensorFlow进行了比较，使用了80个AMS。同样，每个worker使用8个CPU核心，每个AMS使用96个CPU核心。XDL和TensorFlow都使用无锁风格的异步更新。结果显示在图8中。由于其对通信和重复特征的优化，XDL始终表现更好。当我们固定AMS的数量时，在异步模式下运行时，我们几乎实现了线性可扩展性。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/5378d8cc51853a9c88a59683593d9cd91d7ac1017463dda61ee414bbcbea095df1d310e0168c8790c4ba50cc3cca9e6e?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=8.jpg&amp;size=750">

图8

## 7 结论
我们介绍了一个名为XDL的深度学习框架，它对于具有SparseNet + DenseNet抽象的高维稀疏数据集非常强大。凭借面向高维度的设计和系统优化，XDL在真实世界的数据集上可以比原生TensorFlow快得多。我们的算法科学家可以轻松地使用XDL灵活的API开发新模型，以支持阿里巴巴业务的快速发展。

随着XDL生态系统的开源进程，XDL可以在未来进一步发展。首先，像TVM[7]、深度梯度压缩[24]和混合精度训练[26]这样的高级加速技术正在测试中，并将很快部署。其次，作为参数存储和更新的开放解决方案，AMS将需要更灵活的数据流和模型结构，以满足最新模型如记忆网络和动态计算图的要求。

# 参考

- 1.[https://dlp-kdd.github.io/dlp-kdd2019/assets/pdf/a6-jiang.pdf](https://dlp-kdd.github.io/dlp-kdd2019/assets/pdf/a6-jiang.pdf)