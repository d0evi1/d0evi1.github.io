---
layout: post
title: IntegratedRL-MTF介绍
description: 
modified: 2025-2-23
tags: 
---

**摘要**  

作为推荐系统（RS）的关键最终环节，多任务融合（MTF）负责将多任务学习（MTL）生成的多个评分整合为最终评分，以最大化用户满意度并决定最终推荐结果。近年来，业界开始采用强化学习（RL）进行MTF，以优化推荐会话（session）中的长期（long-term）用户满意度。然而，**当前用于MTF的离线RL算法存在三个严重缺陷**：  

1) 为避免分布外（OOD：out-of-distribution）问题，其约束条件过于严格，严重损害模型性能；  
2) 算法无法感知训练数据生成所用的探索策略，且未与真实环境交互，导致仅能学习次优策略；  
3) 传统探索策略效率低下且损害用户体验。  

针对这些问题，我们提出面向大规模推荐系统MTF的创新方法**IntegratedRL-MTF**，其核心创新包括：  

- **离线/在线策略融合**：通过将离线RL模型与在线探索策略相整合，放宽过严的约束条件，显著提升性能；  
- **高效探索策略**：剔除低价值探索空间（low-value exploration space），聚焦潜在高价值状态-动作对的探索；  
- **渐进式训练**：借助探索策略进一步优化模型表现。

在腾讯新闻短视频频道的离线和在线实验表明，该方法显著优于基线模型。目前**IntegratedRL-MTF**已在腾讯推荐系统及其他大型推荐场景中全面部署，取得显著效果提升。

# 1 引言

推荐系统（Recommender Systems, RSs）[1, 2]通过分析用户偏好提供个性化推荐服务，目前已广泛应用于短视频平台[3, 7, 14]、视频平台[4, 5]、电子商务平台[6, 8-11]及社交网络[12, 13]等场景，每日服务数十亿用户。工业级推荐系统通常包含三阶段流程：候选生成（candidate generation）、排序（ranking）和多任务融合（Multi-Task Fusion, MTF）[4, 15]。在候选生成阶段，系统需从数百万乃至数十亿候选项中筛选出数千个候选项目；排序阶段则采用多任务学习模型（Multi-Task Learning, MTL）[4, 8, 16-18]预测用户点击、观看时长、快速滑动、点赞、分享等多种行为的预估分数；**最终通过MTF模型将MTL输出的多任务分数融合为单一分数**，生成候选项目的最终排序[15]，从而决定推荐结果。然而目前针对MTF的研究仍缺乏实质性突破。

MTF的核心目标是：最大化用户满意度。**用户满意度通常通过对单次推荐或推荐会话中的多种反馈指标进行加权计算进行评估，包括观看时长、有效点击、点赞、分享等行为**。其中：推荐会话定义为用户从开始访问推荐系统到离开的完整过程，可能包含一次或多次连续请求。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/0658d1fbeb9078c9db0dfcab825f9b148138e7f524207a676c2ccca48d3cea970acf16222b2d3152e816b794c99765d4?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 推荐会话中用户与推荐系统（RS）之间的交互

在腾讯新闻、抖音和快手等推荐系统中，**当前推荐结果会对后续推荐产生显著影响，特别是在同一推荐会话内**。因此，我们需要同时考虑**当前推荐的即时收益**和**整个会话内的长期累积收益**。最近，部分研究[15,24,25]开始采用离线强化学习（RL）[26]来寻找最优融合权重，以最大化长期收益。与前述方法相比，RL不仅考虑会话内的累积奖励，还能推荐既满足当前用户需求又能带来长期正向交互的内容。此外，RL相比进化策略（ES）具有更强的模型性能和更高的样本效率[23]。目前，RL已在腾讯[15]等多家公司的推荐系统中应用于MTF任务。

然而，现有RL-MTF方法存在以下严重问题[15,26-31]：

- 1）为避免分布外（OOD）问题，现有离线RL算法采用了过于严格复杂的约束条件，严重损害了模型性能；
- 2）在线探索与离线训练相互割裂，离线RL算法无法感知训练数据背后的探索策略，也不再与真实环境交互，因此只能学习到次优策略；
- 3）现有探索策略效率低下且损害用户体验。

针对这些问题，我们提出了一种专门为推荐系统MTF任务设计的新方法IntegratedRL-MTF。

- 首先，该方法将**离线RL模型与我们的在线探索策略相结合**。在离线训练时，可以直接获取探索策略生成的训练数据分布，从而放宽为避免OOD问题而设置的过度约束，显著提升RL模型性能。
- 其次，我们设计了**一种简单但极其高效的探索策略**，不仅加快了模型迭代速度，还减少了对用户体验的负面影响，这对商业公司具有重要价值。
- 最后，我们提出**渐进式训练模式**，借助高效探索策略通过多轮在线探索和离线训练的迭代，使目标策略快速收敛至最优策略。

我们使用自设计的**新评估指标**（该指标更简单且更适用于RL-MTF评估）在相同数据集上进行了离线实验对比。此外，在大规模推荐系统中进行的在线实验表明，我们的RL模型显著优于其他模型。IntegratedRL-MTF已在我们的推荐系统中稳定运行近一年，并推广至腾讯其他大型推荐系统，取得了显著效果提升。本文将重点阐述IntegratedRL-MTF的核心思想，不深入讨论实现细节。

本研究的主要贡献包括：

- 系统分析了现有RL-MTF方法，指出其存在约束条件过严影响性能、在线探索与离线训练割裂导致策略次优、传统探索策略低效损害用户体验等核心问题
- 提出面向大规模推荐系统MTF的定制化RL算法，通过离线RL与探索策略的融合放宽约束条件提升性能，并采用渐进式训练模式实现策略快速收敛
- 在腾讯新闻短视频频道进行实验验证：离线实验采用新设计的评估指标，在线A/B测试显示模型显著优于基线（用户有效消费时长提升+4.64%，用户停留时长提升+1.74%）

## 2 问题定义

本节给出腾讯新闻短视频频道（与抖音类似）中RL-MTF的问题定义。如前所述，在当前推荐会话中，推荐结果会对后续推荐产生显著影响。在每个时间步$t$，推荐系统（RS）接收到用户请求后：

- 1.首先从数百万内容中筛选出数千候选项目
- 2.多任务学习（MTL）模型预测每个候选的多种用户行为得分
- 3.多任务融合（MTF）模型使用公式(1)生成融合权重，将MTL模型输出的多个得分组合为最终得分
- 4.最后将推荐列表发送给用户，并将用户反馈上报至平台数据系统

$$
final\_score=\prod_{i=1}^m (pred\_score_i + bias_i)^{power_i}
$$ 

...(1)

我们将上述融合问题建模为推荐会话内的马尔可夫决策过程（MDP）。在这个MDP中，推荐系统作为智能体与用户（环境）交互，进行序列化推荐，目标是：最大化会话内的累积奖励。该MDP框架包含以下关键组件[26]：

- **状态空间 State Space(S)**：是状态$s$的集合，包括用户画像特征（如年龄、性别、top K兴趣、刷新次数等）和用户历史行为序列（如观看、有效点击、点赞等）

- **动作空间 Action Space (A)**：是RL模型生成的动作$a$的集合。在我们的问题中，动作$a$是一个融合权重向量$(w_1,...,w_k)$，其中每个元素对应公式(1)中的不同幂次项或偏置项

- **奖励Reward (R)**：当推荐系统在状态$s_t$采取动作$a_t$并向用户发送推荐列表后，用户对这些内容的各种行为将上报至RS，基于这些行为计算即时奖励$r(s_t,a_t)$

- **状态转移概率Transition Probability (P)**：转移概率$p(s_{t+1} \mid s_t,a_t)$表示采取动作$a_t$后从状态$s_t$转移到$s_{t+1}$的概率。在我们的问题中，状态包含用户画像特征和用户历史行为序列，因此下一状态$s_{t+1}$取决于用户反馈且是确定性的

- **折扣因子$Discount Factor (\gamma)$**：决定智能体对未来奖励相对于即时奖励的重视程度，$\gamma \in [0,1]$

基于以上定义，在推荐系统中应用RL进行MTF的目标可以定义为：给定推荐会话内RS与用户以MDP形式交互的历史，如何学习最优策略以最大化累积奖励。

# 3 提出的解决方案

## 3.1 奖励函数
在推荐会话中，RS在状态$s_t$采取动作$a_t$计算每个候选的最终得分，并向用户发送推荐列表，随后用户的多种反馈会上报至RS，如图1所示。为了评估即时奖励，我们定义如公式(2)所示的即时奖励函数：

$$
r(s_t,a_t) = \sum_{i=1}^k w_i \cdot v_i
$$

其中：

- $w_i$是行为$v_i$的权重。
- $v_1,...,v_k$：表示在我们的推荐场景中的用户行为，包括观看时长、有效消费（观看视频超过10秒）以及点赞、分享、收藏等交互行为。

通过分析不同用户行为与用户停留时长的相关性，我们为这些行为**设置了不同的权重**。

### 3.2 在线探索
在训练RL模型之前，首先需要收集大量探索数据，这对模型性能有关键影响。然而，传统探索策略面临两个挑战[15,32]：

- **低效率**：在实践中，使用传统探索策略在大规模RS中收集足够的探索数据通常需要很长时间。例如，在我们的平台上**使用动作噪声探索策略收集一次探索数据通常需要五天或更长时间**。这影响了模型迭代速度并意味着收入损失

- **对用户体验的负面影响**：传统探索策略生成的过多探索动作（包括异常动作）会对用户体验产生显著负面影响，甚至导致用户流失，这是不可接受的


为解决上述问题，我们首先在推荐场景数据集上，对新学习的RL策略与基线RL策略在相同状态下生成动作的绝对差值分布进行了分析。为简化分析，我们将动作各维度的取值范围归一化至$[-1,1]$区间，并选取**最重要的4维动作（包括有效消费、观看时长、播放完成率和正向行为率）**进行说明，如图2所示。我们观察到，对于相同状态，新学习RL策略生成的动作通常不会与基线RL策略生成的动作产生显著偏离，这一现象也与我们的直觉相符。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/1b2a3552cbda0de29ff23f5be45751194f09c0e299a1a2c5431cd6351a9ca4665f254fa6fb0ad1344f3bfe669bcc7527?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 在相同状态下，新学习的强化学习（RL）策略与基线RL策略生成的动作之间的绝对差异分布

基于此发现，我们提出了一种简单但极其高效的探索策略，如公式(3)所示，该策略根据基线策略为每个用户定义个性化的探索上下界：

$$
\mu_{ep} = \mu_{bp} + \epsilon,\quad \epsilon \sim \mathcal{U}(lower_b, upper_b)
$$

...(3)

探索动作由基线策略输出的动作加上由$lower_b$和$upper_b$定义的**均匀分布随机扰动**生成。我们通过统计分析精心选择了$lower_b$和$upper_b$的取值。该探索策略的核心思想是：**消除低价值探索空间，仅聚焦于探索潜在高价值的状态-动作对**，如图3所示。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/91258c3b58db5f3b60a40ef18b784ed437bdb5affe67f438570ce62077063f153b0665a1aa78692e9fe3a0846345e1bb?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3 我们的探索策略(exploration policy)与动作噪声探索策略（action-noise exploration policy）的动作分布

相较于传统探索策略（本文以常用于生成探索数据的动作噪声探索策略为例，如图3珊瑚色曲线所示），我们的策略展现出极高的效率。**在相同探索密度要求下，我们推荐场景中的探索策略效率约为动作噪声探索策略的210倍**（具体分析见第4节）。此外，相比动作噪声探索策略，我们的策略能减少数据分布对RL-MTF模型训练的干扰。第3节详述的渐进式训练模式进一步扩展了探索策略的探索空间，因此可设置更小的个性化探索空间上下界。

### 3.3 IntegratedRL-MTF：面向大规模推荐系统MTF定制的强化学习算法

为解决前文所述问题，我们提出名为**IntegratedRL-MTF**的新方法。下面将分别介绍其执Actor网络（Actor Network）、评价器网络（Critic Network）和渐进式训练模式。

#### 3.3.1 Actor网络(Actor Network)

执行器网络的目标是：为特定状态输出最优动作。遵循常规设置，我们在学习过程中构建两个执行器网络：

- 当前Actor网络 $\mu(s)$  
- 目标Actor网络 $\mu'(s)$  

$\mu(s)$ 通过将Actor网络与我们的探索策略相融合，实现了以下创新设计（如公式4-5所示）：  

1. **约束松弛机制**：通过整合在线探索策略的数据分布知识，放宽传统RL-MTF的严格约束条件  
2. **多评价器一致性惩罚项**：基于多个评价器输出的一致性引入额外惩罚项，有效缓解外推误差  

数学表达为：  

$$
\begin{align*}
\theta_{k + 1}&\leftarrow \underset{\theta_k}{\arg\min} E_{s_t\sim\mathcal{D},a\sim\mu(s_t|\theta_k)}\bigg[\\
&- \frac{1}{m} * \sum_{i = 1}^{m}Q_i(s_t,\mu(s_t|\theta_k))\\
&+ \eta * d(\mu(s_t|\theta_k),\mu_{bp}(s_t),\text{lower}_b,\text{upper}_b)\\
&+ \lambda * \sqrt{\frac{1}{m} * \sum_{i = 1}^{m}(Q_i - \text{mean}(Q))^2}\bigg]
\end{align*}
$$ 

$$
d(\mu(s_t|\theta_k),\mu_{bp}(s_t),\text{lower}_b,\text{upper}_b)=
\begin{cases}
0, &\mu(s_t|\theta_k)\in(\mu_{bp}(s_t)+\text{lower}_b,\mu_{bp}(s_t)+\text{upper}_b)\\
e^{\frac{\mu(s_t|\theta_k)-(\mu_{bp}(s_t)+\text{upper}_b)}{\beta*(\text{upper}_b - \text{lower}_b)}},&\mu(s_t|\theta_k)>\mu_{bp}(s_t)+\text{upper}_b \text{ (5)}\\
e^{\frac{(\mu_{bp}(s_t)+\text{lower}_b)-\mu(s_t|\theta_k)}{\beta*(\text{upper}_b - \text{lower}_b)}},&\mu(s_t|\theta_k)<\mu_{bp}(s_t)+\text{lower}_b
\end{cases}
$$

...(4)(5)

其中：

- $\lambda$为调节系数，$\xi$为探索噪声，$Q_j$表示第$j$个评价器网络。


在训练$\pi(s)$期间，如第3.2节所述，可以直接获取每个用户探索数据分布的上界和下界。因此，我们可以利用这一特性来简化过于严格的约束条件，并充分发挥$\pi(s)$的能力。如果$\pi(s)$在状态$s_t$生成的动作处于用户的上界和下界范围内，则公式4中第二项的值为零，即不施加惩罚以避免影响模型能力。否则，将根据超出用户上界或下界的偏差施加惩罚。通过这种方式，当前actor网络的性能相比现有方法得到显著提升，这一点在第4节的实验中得到验证。

此外，我们还引入了一个惩罚机制，该机制定义为多个独立critics[33]输出估计值的标准差，以减轻外推误差，这是公式4中的第三项。由于我们的探索策略具有极高的效率，在用户上下界范围内收集的探索动作与传统动作噪声探索策略相比具有显著更高的平均密度，这对模型优化极具价值。此外，与高斯扰动相比，个性化上下界内的随机扰动减轻了数据分布对模型训练的干扰。如果$\pi(s)$输出的动作处于用户的探索空间内，公式4中第三项的值会很小甚至可以忽略。否则，将施加相应的惩罚来减轻外推误差。

目标actor网络$\pi'(s)$是一个辅助网络，负责基于下一状态生成下一最优动作，以缓解由bootstrapping引起的过估计问题。其参数会使用当前actor网络进行周期性的软更新。

### 3.3.2 Critic网络

Critic网络$Q(s,a)$负责估计推荐会话中状态-动作对$(s,a)$的累积奖励。$Q(s,a)$还将critic网络与我们的探索策略相结合以避免外推误差。在我们的解决方案中，创建了多个独立的critic网络，这些网络被随机初始化并独立训练。每个critic网络的目标是最小化TD-error，如公式6所示。如果$\pi'(s)$在下一状态$s_{t+1}$生成的下一动作处于用户的上下界范围内，公式6中第二项的值为零。否则，将根据超出用户规定上下界的偏差施加惩罚。实践中，我们通常将critic网络数量设为24，这足以在我们的推荐场景中取得良好效果。为了获得更好的性能，我们为每个critic定义了一个目标网络，其参数会使用相应的critic网络进行周期性软更新。

### 3.3.3 渐进式训练模式

离线RL的一个严重缺点是当模型离线训练时，它仅依赖于之前收集的数据而不再与真实环境交互。离线训练期间缺乏实时交互会导致学习策略与实际环境之间的差异，这对离线RL算法的性能产生显著负面影响[15,26-31]。

为了在大规模RS中缓解这个问题，我们的解决方案采用渐进式训练模式，通过高效的探索策略进行多轮在线探索和离线模型训练来学习最优策略，使目标策略能够快速收敛到最优策略。由于我们的探索策略效率很高，我们将之前的单次数据探索和离线模型训练划分为五轮在线数据探索和离线模型训练。最新学习到的策略将作为下一轮在线探索的基线策略。通过迭代高效地探索环境，学习到的策略将不断改进，从而进一步提升我们RL模型的性能。

## 3.4 基于RL-MTF的推荐系统

我们在腾讯新闻短视频频道实现了IntegratedRL-MTF，如图4所示。我们的RL-MTF框架由两个组件组成：离线模型训练和在线模型服务。离线模型训练组件负责预处理探索数据和训练RL-MTF模型。在线模型服务组件主要负责在接收到用户请求时生成个性化最优动作，计算每个候选的最终得分。此外，在线模型服务组件还负责在线探索以收集训练数据。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/3d4c9ef0d313436d917029520379ca1a0c461ff64e8e87fce93ceeb9d17d72c50cf98dfe54de24c01f7704f827f10bcf?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=4.jpg&amp;size=750">

图4 我们推荐系统中的 RL-MTF 框架

# 4.实验

### 数据集描述  
以下数据集收集自腾讯新闻短视频频道，该频道服务数亿用户。我们使用四组用户收集探索数据，每组用户随机选取且数量相同（约200万），以确保公平比较。各数据集的在线探索策略定义如下：  

• **数据集1**：由动作噪声探索策略生成（如图3珊瑚色曲线所示），即在基线策略输出的动作上叠加高斯噪声。实验中采用的高斯分布均值为0.0，标准差为0.2，与此前推荐系统（RS）中使用的参数一致。  
• **数据集2**：由我们的探索策略生成（如图3青色曲线所示）。如前所述，将用户的动作上界设为基线策略输出的动作值加0.15，下界设为基线动作值减0.15。  
• **数据集3**：同样由我们的探索策略生成，但该数据集通过五轮在线探索收集。每轮使用当前探索数据完成模型训练后，新学习的强化学习（RL）策略将作为下一轮在线探索的基线策略。每轮数据采集时长为一天。  
• **测试数据集**：由基线策略生成，作为不同模型的统一测试数据。  


所有探索策略均进行为期5天的环境探索，每个数据集包含约680万次会话。离线实验中，我们使用不同数据集训练不同的RL模型，并采用自定义的新指标在同一测试集上评估模型性能。在线实验中，各模型在腾讯新闻短视频频道部署运行一周，进行A/B测试。  

---

### 4.2 实现细节  
在 **RL-MTF** 中，用户状态 $s$ 包括：  
• 用户画像特征（如年龄、性别、TOP K兴趣、刷新次数等）  
• 用户历史行为序列（最大长度限制为100）  

MTF模型生成的动作是一个10维向量，表示公式1中的融合权重。所有RL-MTF网络均为多层感知机（MLP），基于Adam优化器进行优化。参数设置如下：  
• 奖励折扣因子 $\gamma = 0.9$  
• 参数 $\alpha = 1.2$，$\beta = 0.3$，$\lambda = 0.2$  
• Critic网络数量：24  
• 目标网络软更新率 $w = 0.08$，延迟更新步长 $L = 15$  
• 小批量尺寸：256  
• 训练轮次：300,000  

---

### 4.3 评估设置  

#### 4.3.1 离线策略评估  
离线评估具有低成本且对用户无负面影响的优势。**Group Area Under the Curve (GAUC)** [34] 是推荐系统中常用的评估指标，用于衡量排序算法生成排名的质量 [4, 8, 16-18]。由于多任务学习框架（MTF）模型的目标是通过最终得分对候选项进行排名以最大化奖励，因此可以采用类似的评估指标。  

基于GAUC指标，我们设计了一种新的离线RL-MTF模型评估指标：  
1. **样本标注**：根据用户的观看时间定义每个用户-物品样本的标签。如果用户观看某个物品超过10秒（视为有效消费），则该用户-物品样本的标签为1；否则为0。  
2. **奖励定义**：根据公式2定义每个用户-物品样本的奖励值，样本权重为其奖励值加一个常数值（通常设为1）。  
3. **动作生成与归一化**：使用RL策略为每个用户-物品样本生成动作并计算其最终得分。将测试集中所有最终得分归一化到 $[0, 1]$ 区间，作为预测得分。  
4. **加权GAUC计算**：通过考虑每个用户-物品样本的奖励值，加权GAUC指标更全面地衡量了RL策略在最终排名和区分不同奖励方面的能力。  

相比现有评估指标 [15, 26]，我们的加权GAUC指标更简单、更准确，并在实践中表现良好，非常适合推荐系统中的RL-MTF评估。  

---

#### 4.3.2 在线A/B测试  
我们采用以下两个核心在线指标评估每个MTF模型的性能：  
• **用户有效消费**：一天内所有用户总有效消费次数的平均值。有效消费定义为用户观看视频超过10秒。  
• **用户观看时长**：一天内所有用户总观看时间的平均值。

### 4.4 对比方法  
我们将 **IntegratedRL-MTF** 与 **ES** 及其他先进的离线强化学习（RL）算法进行对比。此外，我们还设计了 **IntegratedRL-MTF** 的两个变体，分别用于验证 **离线 RL 模型与探索策略的整合** 和 **渐进式训练模式（PTM，Progressive Training Mode）** 对模型性能的影响：  

• **ES** [20-23]：以用户画像特征作为模型输入，生成个性化融合权重。由于其简单高效，本文将其作为基准方法。  
• **BatchRL-MTF** [15]：专为推荐系统中的多任务学习框架（MTF）设计，基于 **BCQ** [28] 框架生成动作。**BCQ** 已在腾讯多个推荐系统中部署多年，取得了显著改进。其他对比的 RL-MTF 方法均基于 **BatchRL-MTF** 框架实现。  
• **DDPG**（Deep Deterministic Policy Gradient）[27, 35]：一种经典的离策略 Actor-Critic 算法，适用于高维连续动作空间。  
• **CQL+SAC**（Conservative Q-Learning with Soft Actor-Critic）[30, 39]：通过正则化 OOD（Out-of-Distribution）动作-状态对的 Q 值，学习保守的 Q 函数下界，以减少外推误差。  
• **IQL**（Implicit Q-Learning）[31]：无需评估数据集外的动作，但通过泛化能力使学习到的策略显著优于数据集中最优行为。  
• **IntegratedRL-MTF（无 PTM）**：将离线 RL 模型与在线探索策略整合，放宽过于严格的约束条件（使用数据集 2 训练）。  
• **IntegratedRL-MTF**：结合探索策略与渐进式训练模式，通过多轮迭代优化策略（使用数据集 3 训练）。  

---

### 4.5 离线评估  
本节通过广泛的离线实验，验证 **IntegratedRL-MTF** 相较于其他 MTF 算法的显著性能提升。同时，还分析了我们在线探索策略的效率。  

#### 4.5.1 IntegratedRL-MTF 的有效性  
为比较上述算法的性能，我们分别训练各模型，并在相同测试集上使用加权 GAUC 指标进行评估：  
• **ES** 通过交替变异和选择过程更新模型参数，已长期部署于我们的推荐系统中，作为离线评估的基准。  
• **DDPG**、**CQL+SAC**、**BatchRL-MTF** 和 **IQL** 使用数据集 1 进行训练。  
• **IntegratedRL-MTF（无 PTM）** 使用数据集 2 进行训练。  
• **IntegratedRL-MTF** 使用数据集 3 并结合渐进式训练模式进行训练。  

所有模型均在相同测试集上进行测试，结果如表 1 所示：  

**表 1：各方法在相同测试集上的加权 GAUC 对比**  

在离线评估中，**IntegratedRL-MTF（无 PTM）** 的加权 GAUC 显著高于 **ES** 模型及其他现有离线 RL 模型。传统离线 RL 算法因严格的约束条件导致性能受限，而我们通过用户上下界信息简化约束，并将离线模型算法与高效探索策略整合，从而显著提升了模型性能。此外，**IntegratedRL-MTF** 进一步优于 **IntegratedRL-MTF（无 PTM）**，证明渐进式训练模式通过多轮迭代探索持续优化策略。  

---

#### 4.5.2 探索策略的效率  
我们的在线探索策略通过在基线策略输出的动作上叠加随机扰动生成动作，扰动范围必须位于用户的个性化上下界内（如图 3 青色曲线所示）。该方法的核心思想是排除低价值探索空间，专注于潜在高价值状态-动作对的探索。  

我们对不同 RL 策略在同一状态下生成的动作分布进行统计分析，并精心选择上下界值 $upperb$ 和 $lowerb$。为简化分析，本文假设探索策略每个维度的探索范围相同。通常，我们将 $upperb$ 和 $lowerb$ 分别设为 0.15 和 -0.15，这在推荐场景中已足够。  

此前推荐场景中使用的动作噪声探索策略（如图 3 珊瑚色曲线所示）具有均值 0.0 和标准差 0.2 的高斯分布。由于 RL-MTF 模型生成的动作是一个 10 维向量，在相同探索密度要求下，我们的探索策略效率约为动作噪声探索策略的 **210 倍**。  

在实际应用中，由于 **IntegratedRL-MTF** 采用渐进式训练模式，探索策略的上下界可以进一步缩小，从而提高效率。

[https://arxiv.org/pdf/2404.17589](https://arxiv.org/pdf/2404.17589)