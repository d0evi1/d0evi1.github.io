---
layout: post
title: STAR算法介绍
description: 
modified: 2021-09-04
tags: 
---


阿里在《One Model to Serve All: Star Topology Adaptive Recommender
for Multi-Domain CTR Prediction》中提出了一种思路来解决不同模块使用同一模型的思路：

# 1.介绍

传统CTR模型关注于single-domain的prediction，其中ctr模型会服务于单个业务domain，它基于从该domain中收集到的样本进行训练。每个业务domain是一个特定地点，items被展示给移动端app或PC 网站。在大的商业公司（比如：阿里和亚马逊），经常有许多业务domains需要进行CTR预估来增强用户满意度和提升商业回报。例如，在阿里，商业domains的范围有： 猜你喜欢、Banner、以及其它domains。图1展示了在阿里的一些业务domains。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/b537fa35871e112d976a5e070b848c9030eab4cf3a07f6316dff74d3ed51a11eb60c2bae76ccaeae21a4f1761aedf741?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=1.jpg&amp;size=750">

图一

- Banner：在banner中，会在taobao主页的top banner上展示items。这些item可以是一个商品、商店、或一个品牌。
- 猜你喜欢：在该模块中，items都是商品，在左或右列被展示给用户

由于不同业务domains会有重叠的用户组（user groups）和items，在这些domains间会存在共性，允许**信息共享**对于学习每个domain的CTR模型来说是有益的。然而，特定的user group可能会不同，用户行为也会在多个domains内变化。这些差异会导致**domain-specific数据分布**。**简单将所有data进行混合并训练单个共享的CTR模型不能很好地在所有domains上工作良好**。

除了**混合数据并训练一个shared model**外，另一种简单解法是，**为每个商业domain构建一个独立的模型**。这种方式也会有一些缺点：

- (1) 一些业务domains会比另一些domains具有更少的数据。将数据进行分割会忽略domain共性，并造成更少的训练数据，使得模型很难学习
- (2) 维护多个模型会造成资源大量消耗，并需要更多的人工开销。当商业domains的数目达到上百个时会相当麻烦

本paper的目标是学习一个有效和高效的CTR模型来同时处理多个domains。我们将multi-domain CTR prediction公式化成：recommender需要为M个商业domains $$D_1, D_2, \cdots, D_M$$作为CTR预测。该模型可以将input作为(x, y, p)，其中：

- x是公共特征（像：历史用户行为、用户profile特征、item feature、context feature等），会被多个商业domain使用
- $$y \in \lbrace 0, 1\rbrace$$是点击label
- p是domain indicator：它表示该样本是在哪个domain上被收集

注意：(x,y)从domain-specific分布$$D_p$$上抽取得到，分布会随着不同的domains有所不同。multi-domain CTR预测的目标是：构建一个有效且高效的模型，它会为每个domain给出精准的CTR预测，并在资源消耗上开销不大，该模型可以充分利用domain共性，并能捕捉domain间的差异。

一种用于提升学习的可能策略是，使用domain进行多任务学习。如图3所示，multi-domain CTR预测与多任务学习间的不同之处是：multi-domain CTR预测是在不同的domains上解决相同的任务（都是CTR 预测任务），不同domains的label spaces是相同的，数据分布有所不同。作为对比，大多数多任务学习方法则在相同的domain上解决不同的任务，其中label space会不同，例如：联合估计CTR和CVR。由于任务的异构性，已存在的多任务学习方法则关注于在bottom layers上的共享信息，但会在task-specific output layers上保持独立。直接在multi-domain CTR预测上采用multi-task方法可能不会充分利用上在label space上的domain关系，并且会忽略不同domains上不同数据分布。


<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/d6ca4057382ffe9e43a27b02061c01d86fcec79ea0918e318aa28a306d03161c741c3f9d3f057eee48ba6c22fcde2283?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=3.jpg&amp;size=750">

图3

为了充分利用domain关系，我们提出星形拓朴自适应推荐（STAR Topology Adaptive Recommender: STAR)来进行multi-domain CTR预估。**提出的STAR模型是星形拓朴，如图4所示**。STAR包含了共享的中心参数，以及domain-specific参数的多个集合。**每个domain的最终模型通过将共享中心参数（shared centerd params）和domain-specific参数进行组合来获得。中心参数（centered parameters）被用于学习在所有domains间的总行为，其中公共知识可以被学习到以及在所有domains间转移**。domain-specific参数会捕获在不同domains间的特定行为来提升更加refined的CTR预估。star topology会利用跨多个domains间的有效信息转换，来学习domain公共性和差异。**该paper会实现STAR模型，它使用在每个layer上对weights做element-wise product来作为组合策略**。由于embedding layers会在工业界推荐器上贡献大多数参数量，添加的domain-specific参数对于总参数量来说可被忽略。因此，使用STAR模型来serve多个domains只需要添加少量计算和内存开销，就能达到更好的效果。

主要的贡献如下：

- STAR:
- 不同domains具有不同的数据分布，当使用batch normalization时，这会生成不准确的统计。我们提出Partitioned Normalization(PN)，它会为不同domains上的样本进行独立normalization来解决该问题。PN会在domain内生成更准确的moments，它会提升model效果。
- 在mulit-domainCTR预测中，描绘domain信息的features是很重要的。我们提出一个auxiliary network，它会直接将domain indicator作为input，并学习描绘该domain的有embeddings。该embedding会feed给auxiliary network，它比原始network更简单。这会使得domain indicator以一种直接方式影响最终预测。
- 我们会在工业产品数据集上评估STAR，并将它部署在2020的阿里展示广告系统上。持续的收益验证了STAR的效果。直到现在，STAR的部署带来了6%的CTR和8%的RPM提升。它可以泛化到其它场景上。

# 2.相关工作

...

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/643d362c2a04ffe35e49f5ea5adfd2843e0da66f4805418616ca184f2d0615c71bf06985198572f7e5986d5f46f5741a?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=2.jpg&amp;size=750">

图2

# 3.提出的方法

在本节中，我们首先提出一个关于multi-domain CTR预估的简洁背景介绍。接下是提出方法multi-domain的CTR预估的架构总览。接着我们详细介绍STAR，包括提出的STAR topology network，partitioned normalization以及auxiliary network。

## 3.1 Multi-domain CTR预估

在序列推荐系统中，推荐会采用用户历史行为、user profile特征、target item feature以及其它features（比如：context feature）作为input。一个用户u在一个item m上点计的预估CTR（$$\hat{y}$$）可以计算如下：

$$
\hat{y} = f( E(u_1), \cdots, E(u_i); E(m_1), \cdots, E(m_j); E(c_j), \cdots, E(c_k))
$$

其中：

- $$\lbrace u_1, \cdots, u_i \rbrace$$是user features的集合，包括：用户历史行为，user pfofile feature。
- $$\lbrace m_1, \cdots, m_j \rbrace$$是target item feature的集合
- $$\lbrace c_1, \cdots, c_k \rbrace$$是其它features的集合
- $$E(\cdot) \in R^d$$表示embedding layer，它会将sparse IDs映射到可学习的dense vectors上

在将raw feartues映射到低维embeddings上后，惯例是将这些embeddings聚合来获取固定长度的vectors。可以部署不同类型的聚合方法（42, 43）来聚合这些embeddings来抽取用户兴趣并获取固定长度的presentation。获得的representation接着feed到下面的DNN中（例如：一个multi layer fully-connected network）来获得最终的CTR预测。

传统的CTR模型（6，13，23，42，43）通常从一个单一商业domain上获取数据进行训练。然而，真实推荐通常会处理不同的商业domains。推荐系统需要为M个domains $$D_1, D_2, \cdots, D_M$$同时作为CTR预测。该模型会将(x,y,p)作为input，其中：

- x是在多个domains中用到的公共featrure（比如：用户历史行为、user profile、target item feature）；
- $$y \in \lbrace 0, 1\rbrace$$是点击的label
- $$p \in \lbrace 1,2, \cdots, M\rbrace$$是domain indicator，它会表示样本来自哪个domain。

**注意(x,y)是从domain-specific分布$$D_p$$上抽样得到，该分布对于不同domains会不同**。multi-domain CTR预估的目标是：构建单个CTR模型，它可以给出准确的CTR预估，并以较低资源和开销进行serve。

## 3.2 架构总览

如上所示，忽略domain indicator p，学习单个共享CTR模型会忽略domain的差异性。这会导致次优的模型参数。另一方面，对于不同domain训练不同模型会更差，因为将domains进行分隔，每个模型会得到更少的数据。由于资源开销以及人力开销，在生产环境中为每个domain维护一个独立的模型是不可行的。

最后，我们提出STAR来进行multi-domain CTR预估，它可以更好使用不同domains间的相似性，并能捕获domain上的差异。如图4所示，STAR包含了三个主要部分：

- (1) partitioned normalization(PN)：它会为不同domains间的样本进行单独normalization
- (2) star topology FC network (star topology FCN)
- (3) auxiliary network：它会将domain indicator看成是input featrure，并能学到它的语义embeddings来捕获domain差异性


<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/f195c6b9eefafc617b8cfb4c62c9d9c078c9eb06655ba24334493497b415d6dfaa5b7a87d88923eff922423bd05a2be6?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=4.jpg&amp;size=750">

图4

在训练期间，domain indicator p会首先被抽样，接着会使用一个B个mini-batch实例：

$$
(x_1, p), (x_2, p), \cdots, (X_B, p)
$$

会从该domain中抽样。STAR会首先将这些input features通过一个embedding layer进行嵌入作为低维vectors。在工业推荐系统中，该模型通常会使用数十亿features（15）进行训练，embedding的参数通常要比其它部分的参数更多。这使得它在不同domains上使用有限数据来学习domain-specific embedding很难。例如：对于在日常任务中用到的模型，embeddings参数要比FC layers上超过10000倍。因此，在STAR模型中，我们将所有domains共享相同的embedding layer，例如：在不同domains上的相同ID features会共享相同的embedding。共享的embedding layer会跨不同的domains，可以极大减少计算和内存开销。

该embeddings接着被pooled和concatenated，来获得B个固定长度的reprensentations。在这之后，B个抽取的representations会通过PN（patitioned normalization） layer进行处理，接着为不同domains进行独立的normalization statistics。normalizated vectors接着作为input被feed到star topology FCN中来获取output。star topology FCN包含了共享的centered FCN以及多个domain-specific FCNs。每个domain的最终模型通过将shared centered FCN和domain-specific FCN进行组合获得。

在multi-domain CTR预估中，描述domain信息的features很重要。在STAR模型中，auxiliary network会将domain indicator作为input，并使用描述该domain的其它features来feed到auxiliary network中。auxiliary network的output 会被添加到star topology FCN的output中，来获取最终的prediction。我们让auxiliary network比star topoology FCN更简单，便得让模型以一个直接和简单方式来捕获domain差异。接着我们描述这些组件。

## 3.3 Partitioned Normalization

如上，raw featrures会首先转换成低维embeddings，接着进行pool和aggregation来获得中间表示。尽管一个实例的中间表示为z，为了训练deep networks更快和更稳定，一个标准的惯例是应用normalization layer到中间表示z上。在所有的normalization方法之间，batch normalization(BN)是一个表示方法，它对于训练非常深的DNN很重要（14，31）。BN会为所有样本使用一个全局的normalziation，它会累积normalization moments，并学习跨多个样本的共享参数。具体的，BN的训练归一化给定如下：

$$
z' = \gamma \frac{z-u}{\sqrt{\sigma^2 + \epsilon}} + \beta 
$$

其中：

- z'是output
- $$\gamma$$和$$\beta$$是可学习的scale和bias参数
- $$\mu, \sigma^2$$是当前mini-batch的均值（mean）和方差（variances）

在testing期间，在所有样本上的均值E和方差Var的移动平均统计，使用如下：

$$
z' = \gamma \frac{z-E}{\sqrt{Var + \epsilon}} + \beta 
$$

...(2)

换句话说，BN会假设：所有样本是独立同分布(i.i.d)的，并在所有训练样本上使用共享的statistics。

然而，在multi-domain CTR预估中，样本假设是在一个特定domain上是局部i.i.d的。在testing期间在BN layers上共享全局的monents和参数，会牺牲domain差异性，并导致模型效果的降级。为了捕获每个domain上唯一的数据特性，我们提出partitioned normalization(PN), 它会为不同domains上单独对statistics和parameters做normalization。具体的，在training期间，假设当前的mini-batch是会第p个domain上抽样得到，我们会计算当前mini-batch的均值（mean）和方差（variances），并将feature进行归一化：

$$
z' = (\gamma * \gamma_p) \frac{z - \mu}{\sqrt{\sigma^2 + \epsilon}} + (\gamma + \gamma_p)
$$

...(3)

其中：

- $$\gamma, \beta$$是全局的scale和bias
- $$\gamma_p, \beta_p$$是domain-specific scale和bias参数

对于每个mini-batch，它会接受最终scale，通过将共享的$$\gamma$$与domain-specific $$\gamma_p$$进行element-wise相乘作为final scale，例如：PN会根据domain indicator自适应地对representation进行缩放。相似的，PN的bias也可以根据domain自适应地计算，它可以通过global bias $$\beta$$和domain-specific bias $$\beta_p$$求和来实现。注意：通过对比BN，PN也会在training期间使用当前mini-batch的moments，但PN会引入domain-specific scale和bias $$\gamma_p, \beta_p$$来捕获domain差异。

除了在scale和bias上的修改外，PN也会让不同domains进累计domain-specific的均值$$E_p$$和方差$$Var_p$$的移动平均。在testing期间，PN会将第p个domain的实验z进行转换：

$$
z' = (\gamma * \gamma_p) \frac{z - E_p}{\Var_p+ \epsilon}} + (\gamma + \gamma_p)
$$

...(4)

从等式(4)来说，我们可以看到，PN会使用domain-specific的平均$$E_p$$和方差$$Var_p$$来归一化中间表示z。因而，PN会根据domain indicator为条件自适应更改中间表示来捕获差异特性。

## 3.4 Star Topology FCN

在PN layer之后，表示$$z^'$$会被feed作为input到下面的star topology multi-layer FCN上。如图5所示，提出的star topology FCN会为每个domain包含一个共享的centerd FCN和独立FCNs，因而，FCN的总数是M+1. 第p个domain的最终模型可以通过对shared centered FCN和domain-specific FCN组合来得到，其中，centered参数会学习在所有domains上的通用行为，domain-specific参数会捕获在不同domains上的指定行为，来帮助更多的fefined CTR预测。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/40f1890eba396cdf074a0463131896f6c34f755d9c5055ab53dc8ce2312aa8599f84954196b82241e1619838fb32cd84?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=5.jpg&amp;size=750">

图5

特别的，对于shared FCN，假设W和b分别是NN layer上的weights和bias。对于第p个在omain的specific FCN，假设：$$W_p$$和$$b_p$$是相应layer上的weights和bias。我们将input维度表示为c，output维度表示为d，（例如：$$W, W_p \in R^{c \times d}, b, b_p \in R^d$$），第p个domain的最终的weights $$W_i^*$$和bias $$b_i^*$$可以通过以下获得：

$$
W_p^* = W_p \otimes W, b_p^* = b_p + b
$$

...(5)

其中，$$\otimes$$表示element-wise乘法。假设：$$in_p \in R^{c \times 1}$$表示来自第p个domain该neural network layer的输入，最终的output $$out_p \in R^d \times 1$$给定如下：

$$
out_p = \phi((W_p^*)^T in_p + b_p^*)
$$

...(6)

其中，$$\phi$$表示该layer的activation function。shared param和在domain-specific param的组合可以在所有layers上使用。通过这种方式，STAR可以对它的参数基于domain为条件进行调节。

注意，我们会对shared centerd FCN和domain-specific FCN进行组合策略，它的实现是：将每个layer上的weights进行element-wise乘，将bias进行加得到；也可以尝试研究其它策略。**shared参数会通过对所有样本的梯度进行更新，而domain-specific参数则只会在使用该domain的样本时才会被更新。**如上所示，工业推荐系统的大多数参数，会由embedding layer来贡献，STAR只会增加M个FCNs，量级很少。

## 3.5 Auxiliary Network

在CTR建模的传统方式下，所有features会被同等看待，并被feed给复杂的模型。在multi-domain CTR预测时，对于模型来说自动学习domain差异是很难的。我们会讨论一个好的multi-domain CTR模型应该具有以下几个特性：

- (1) 具有考虑上domain特性的信息特征
- (2) 这些featrures可以很容易并直接影响final CTR预估

背后的直觉是，描会domains的features很重要，因为它可以减少模型难度来捕获domains间的不同。

最后，我们提出一个auxiliary network来学习domain差异。为了讨论与domain特性相关的信息特征，我们将domain features直接看成是ID feature input。domain indicator会首先映射到embedding vector上，并与其它features进行concate。auxiliary network接着会根据concatenated features分别计算forward pass，来获得一维output。我们将star topology FCN的一维output表示成$$s_m$$，auxiliary network的output表示成$$s_a$$。$$s_m$$和$$s_a$$会被相加来获得最终logit。接着使用sigmoid来获得CTR预测：

$$
Sigmoid(s_m + s_a)
$$

...(7)

在我们的实现中，auxiliary network会比主网络更简单，它是一个二层FCN。这种简单结构可以使得domain features可以直接影响final prediction。

$$\hat{y}_i^p$$表示在第p个domain的第i个样本上的预测概率，$$y_i^p \in \lbrace 0, 1\rbrace$$是ground truth。我们会在所有domains上对cross-entropy loss function进行最小化：

$$
min \sum\limits_{p=1}^M \sum\limits_{i=1}^{N_p} - y_i^p log(y_i^p) - (1 - y_i^p) log(1 - \hat{y}_i^p)
$$ 

...(8)

# 4.实验

略


# 参考



- 1.[https://arxiv.org/pdf/2101.11427.pdf](https://arxiv.org/pdf/2101.11427.pdf)