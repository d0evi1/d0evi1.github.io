---
layout: post
title: 推荐系统fairness
description: 
modified: 2019-07-21
tags: 
---

Alex Beutel等人在KDD 2019《Fairness in Recommendation Ranking through Pairwise Comparisons》中提出pairwise比较来看fairness。具体方法如下：

# 介绍

我们希望什么样的推荐系统？推荐器对于将用户连接到网络上的相关内容、items或者信息来说很重要，用户、内容提供商、零售商、信息提供商都依赖这些系统，我们需要明白谁应该是否被支持很重要。在本paper中，我们主要关注一个推荐系统在关于items的under-ranking groups上的风险。例如，一个under-ranked的社交网络通过一个给定的demographic group发表，会限定在该group范围内可见。

对于分类(classification)的公平性度量（fairness metrics）有许多研究，每种metric都是恰当的，但对于推荐系统来说，这方面考虑的较少。在推荐系统中，公平性的研究是很有挑战性的，它们很复杂。通常包含了多个模型，必须平衡多个目标，由于存在很大、倾斜的稀疏性以及许多动态性（dynamics）很难评估。所有这些要点在推荐系统社区中很难解决，在提升推荐公平性上提出了额外的挑战。

一个挑战性的划分是介于：将推荐看成是一个pointwise prediction问题、将这些预测应用于排序列表的构建。pointwise recommenders会为每个item做出一个关于用户兴趣的预测，接着基于这些预测决定一个推荐排序(ranking)。该setup在实际中很常见，但大多数研究都是深入去缩小pointwise predictions和ranking construction间的构建。公平性会陷入进退两难的境地。最近围绕pointwise accuracy为中心的关于fairness metrics的研究[8,49]，并不能表明产生的ranking是用户实际想看的。做为对比，[52,44,45,11]探索了什么是一个fair ranking，但都关注于非个性化排序（unpersonalized rankings），它们会考虑上items的相关度、并且大多数情况下需要使用上item group关系的一个后处理算法(post-processing algorithm)，这在实际中通常是不可行的[10]。

另外，推荐系统的评估是很难的，因为系统的动态变化性。一个用户昨天感兴趣的东西，在明天可能会不感兴趣，我们只能通过一个用户的偏好来推荐一个item给他们。因而，metrics通常是对之前的推荐系统(previous)[3]是有偏的(biased：在统计上)，大量研究会做无偏离线评估[43,42]，由于存在很大的item space、以及极其稀疏的feedback、users和items的不断演化，很难进行评估。当尝试衡量推荐系统的公平性时，这些issues只能变得更显著；而当你尝试评估complete rankings时更甚。

我们通过一个pairwise recommendation fairness metric来解决所有的这些挑战。通过使用易运行、随机化的实验，我们可以得到关于用户偏好的无偏估计（unbiased estimates）。基于这些观察到的(observed) pairwise偏好，我们甚至都可以对一个pointwise的推荐系统的公平性进行measure，我们展示了这些metrics与排序效果直接相关。另外，我们提供了一种新的正则项(regularization term)，结果表明它可以提升一个pointwise recommender的最终ranking fairness，如图1所示。我们在生产环境中的一个大规模推荐系统上进行测试，结果表明实际是有益的，并且理论上和经验上是可同时trade-offs的。我们的贡献有：

- Pairwise Fairness：
- Pairwise Regularization：
- 真实实验：

# 2.相关工作

**推荐系统**：。。。

**机器学习公平性(Machine Learning Fairness.)**：机器学习公平性主要关注分类问题的公平性，提出了许多定义。基于这些定义的group fairness，一个模型将两组样本进行比较，成为了最常见的结构，但研究者们展示了不同定义间的调节器。我们主要根据Hardt[23]的机会等式(equality of opportunity intuition)，其中我们会关注不同groups上在accuracy上的差异。我们的metric更接近于构建在一个AUC-based 分类和回归问题上的fairness metrics，它由Dixon[18]提出，并在[12]中展开可以作为不同的Mann-Whitney U-tests。

**Recommender System Fairness**。在ranking和recommendation上的fairness上，已经有一些研究，但这些研究都从不同的视角出发。Zehlike[52]从排序公平性的目标出发，但没有考虑推荐系统（它的数据是很稀疏的）。相似的，Singh[44]采用一个full-ranking的公平性视角，可以通过一个后处理算法(post-processing)
对模型预测应用于推荐系统；接着[45]将它移到模型训练中。所有这些工作[52,44,45,11]关注于一个非个性化的信息检索系统，其中主要关注每个item的相关labels；我们关于的个性化推荐场景存在：数据稀疏性和biases。[8,49]则关注于CF pointwise accuracy跨不同groups的差异，但没有将这些metrics连接到最终的rankings上。

更多研究在统计等价(statistical parity)，其中在一些应用上还有争议：items应跨不同groups以相同的rate被展示。Diversity、filter bubbles、feedback loops，以及机器学习的fairness，在本paper不是关注重点。

**Fairness Optimization.** 许多方法的提出是为了解决公平性问题。Post-processing可以提供优雅解法【23，44】，但通常需要已经对于所有样本的group memberships，这对于demographic数据来说几乎是未知的。然而，许多方法在分类训练期间来优化fairness metrics，比如：constriaint-based optimization、adversarial learning、以及通过模型预测的regularzation。我们构建了这些regularization方法来提升我们推荐系统的fairness属性。

# 3.推荐的pairwise fairness

我们考虑一个生产环境推荐系统，它会推荐一个关于K个items的个性化列表给用户。我们考虑一个cascading recommender，它会使用检索系统（retrieval systems）集合，后跟着一个排序系统(ranking system)。我们假设：retrival systems会从一个包含M个items的语料J中，返回一个关于$$M'$$的相关items的集合R，其中$$M \gg M' \geq K$$。排序模型（ranking model）接着必须进行打分，并对$$M'$$个items进行排序来得到最终的K items排序列表。这里，我们主要关注ranker的角色。

当做出一个推荐时，系统会为user i观察用户特征$$u_i$$，和一个上下文特征集合c（比如时序(timing)、或设备信息）；我们将它称为query：$$q=(u,c)$$。另外，对于每个item $$j \in J$$，我们观察到特征向量 $$v_j$$；这里，我们会包含对于item的稀疏表示或学到的embeddings，以及与该item相关的其它属性。ranker会基于user feedback（包含：clickes, ratings, 文上的停留时间，items的后续购买等）的估计执行ranking。对于我们的系统，我们会估计用户是否会在该item上的点击$$y \in {0, 1}$$，以及在点击该item上的用户参与度(user engagement)$$z \in R$$，比如：停留时间、购买、raitings。这样，我们的数据集包含了历史样本 $$D = \lbrace  (q,v,y,z) \rbrace$$。（注意，由于z是在一个点击之后的user engagement，如果没有点击发生，z=0）。D只包含了之前被推荐的样本。

ranker是一个模型$$f_{\theta}$$，参数为$$\theta$$；该模型被训练来预测用户参与度$$f_{\theta}(q,v) = (\hat{y},\hat{z}) \approx (y,z) $$。最终，一个items的ranking会通过打分函数$$g(\hat{y}, \hat{z})$$来生成，用户会从由g排序的相关items R中选取topK个items。

## 3.2 Fairness concerns的动机

在之前的讨论中，有许多公平性关注点在文献中有强调。在本paper中，我们主要关注items分组成为under-recommended的风险。例如，如果一个under-ranked的社交网络，通过一个给定的demographics group进行发表，它会限制分组在该服务上的可见性和参与度。如果一个网络的评估部分是个性化的，那么一个demographic group的用户评论也是under-ranked，接着该demographic会在该网络上有更少的话语权（voice）。在一个更抽象的层次上，我们假设，每个item j具有敏感属性$$s_j \in \lbrace 0, 1 \rbrace$$。我们会measure：来自一个group上的items是否在系统上是under-ranked。

尽管并非是我们的主要关注点，这些issues可以user group concerns并列，如果一个items的group是否被一个特定的user group更偏好。该框架会显式扩展到包含user groups。如果每个user具有一个敏感属性，我们可以通过每个user group来计算所有以下的metrics，并计算跨groups的性能比较。例如，如果我们关注的是，一个社交网络是under-ranking，特定主题的items只限定于特定的demographic人群，我们可以比较：跨demographic groups的主题内容的under-ranking的degree。

## 3.3 Pairwise Fairness Metric

上述fairness目标看起来很重要，对于一个"under-ranked"的item来说，我们必须准确搞清它的含义。这里我们吸收了[23]的思想：一个classifier的fairness通过比较它的false postive rate and/or false negative rate进行量化。不同的是，给定一个item的label是postive的，classifier预测它为positive的probability。在分类中，由于模型预测可以通过一个预测定阀值进行比较，这可以有效工作。

在推荐系统中，一个positive prediction是不明确的，即使人们将分析限制在clicks(y)和ignore engagement(z)中。例如，如果一个item被点击，y=1,那么被预测的点击概率为$$\hat{y}=0.6$$，这是一个positive prediction吗？它可以被看成是一个0.4的under-prediction，如果其它items都具有一个预测的$$\hat{y}<0.6$$它仍是top-ranked item。因而，理解在pointwise predictions中的errors需要对比对于相同query的items预测。

我们开始定义了一个pairwise accuracy：一个clicked item的概率被排在另一个相关uncliked item之上，对于一个相同的query有：

$$
PairwiseAccuarcy \triangleq P(g(f_{\theta} (q, v_j)) > g(f_{\theta}(q, v_j')) | y_{q,j'}, j, j' \in R_q)
$$

...(1)

有了该定义，我们可以知道ranking系统会对cliked item进行rank的频次。出于简洁，我们使用$$c_q(j, j') \triangleq 1 [g(f_{\theta}(q,v_j)) > g(f_{\theta}(q,v_{j'}))]$$来表示：对于query q，item j和$$j'$$间的预测比较；我们会隐掉$$j, j' \in R_q$$项，但我们只考虑对于所有以下定义相关items间的比较。

对于余下的fairness研究，我们会关注groups间的相对performance，而非绝对performance。因而，我们可以比较：

$$
P(c_q(j, j') | y_{q,j} > y_{q,j'}, s_j = 0) = P(c_q(j,j') | y_{q,j} > y_{q,j'}, s_j =1)
$$

也就是说，来自一个group $$S=0$$的items的PairwiseAccuarcy，要比来自另一个group $$S=1$$的PairwiseAccuarcy或高或低些。

这是一个直觉上的metric，这里还有疑问：它会忽略整个user engagement z，因而可能会有促进ckickbait的风险。

**定义1**(Pairwise Fairness)。一个具有ranking公式g的模型$$f_{\theta}$$，如果一个clicked item的likelihood被排到另一个相关的uncliked item之上（跨相同的groups），被认为是服从pairwise fairness，则被认为是服从pairwise fairness：

$$
P(c_q(j,j') | y_{q,j} > y_{q,j'}, s_j = 0, z_{q,j} = \bar{z}) = P(c_q(j, j') | y_{q,j} > y_{q,j'}, s_j=1, z_{q,j}=\bar{z}), \forall \bar{z}
$$

对于来自每个group的items，该定义给出了一个关于ranker accuracy的聚合概念。

由于它是valuable的，它不会区别来自mis-orderings types。对于来自一个group的under-exposing items可能是有问题的。为了说明，考虑以下两个示例：在两种情况下，每个group $$\lbrace A_j \rbrace_{j=1}^3 \cup \lbrace B_j \rbrace_{j=1}^3$$存在三个items，在第一个case中，系统给出了一个 ranking $$[ A_2, A_3, B_1, A_1, B_2, B_3 ]$$，在第二个case中，系统给出了$$[A_1, A_2, A_3, B_1, B_2, B_3]$$，我们可以看到，overall pairwise accuracy在两个cases中相同，$$\frac{2}{5}$$，但在第二个case中，当group B中的一个item有兴趣（clicked），所有group B items会排在group A items之下。两者在ranking中都有问题（排在clicked item之下），但第二个case有系统上更有问题，偏向于某个group，这独立于用户偏好。

为了解决该问题，我们可以将上述pairwise fairness定义分割成两个独立的criteria：在相同group中items间的pairwise accuracy，不同groups的items间的pairwise accuracy；我们将这些metrics称为："intra-group pairwise accuracy"和"inter-group pairwise accuracy"：

$$
Intra-Group \  Acc. \triangleq P(c_q(j, j') | y_{q,j} > y_{q,j'}, s_j = s_{j'}, z_{q,j} = \bar{z}) 
Inter-Group \  Acc. \triangleq P(c_q(j, j') | y_{q,j} > y_{q,j'}, s_j \neq s_{j'}, z_{q,j} = \bar{z}) 
$$

...(3) ...(4)


**定义2**

**定义3**

## 3.4 Measurement

在推荐系统中，users和items是高度动态的，我们通常只在之前的recommended items上观察user feedback，这会使得metrics容易偏偏于previous recommender system。

然而，对于上述给出的三个fairness定义，我们希望在item pairs间用户偏好的无偏估计。为了这样做，我们在小量queries上运行随机实验。







# 参考

- 1.[https://arxiv.org/pdf/1903.00780.pdf](https://arxiv.org/pdf/1903.00780.pdf)