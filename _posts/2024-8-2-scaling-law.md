---
layout: post
title: Meta推荐系统-scaling laws介绍
description: 
modified: 2024-8-1
tags: 
---

meta在《Understanding Scaling Laws for Recommendation Models》讨论了推荐系统中的scaling law问题。

# 摘要

**规模（scale）**一直是提高机器学习性能的主要驱动力，理解**规模法则（scaling laws）**对于可持续的模型质量性能增长的战略规划、长期资源规划以及开发支持大规模模型的高效系统基础设施至关重要。在本文中，我们研究了DLRM风格的推荐模型的经验规模法则，特别是点击率（CTR）。**我们观察到模型质量与模型大小、数据大小和训练所用计算量呈幂律加常数的规模**。我们通过比较这些轴上的不同规模方案，对数据、参数和计算三个不同的资源维度的规模效率进行了表征。我们展示了**参数规模对于所研究的模型架构已经力不从心，而在出现更高性能的模型架构之前，数据规模是前进的道路**。本研究解决的关键研究问题包括：

- 推荐模型是否如规模法则预测的那样可持续地规模？
- 我们是否远离规模法则的预测？
- 规模的极限是什么？
- 规模法则对长期硬件/系统开发有何影响？

# 1. 引言

在过去十年中，深度学习总体上，特别是基于深度学习的推荐模型（DLRM），在数据集规模、模型规模和系统资源方面经历了指数级的增长（Elkahky等人，2015年；Covington等人，2016年；Sullivan，2016年；Liu等人，2017年；Yi等人，2018年；Zhou等人，2019年；Zhao等人，2019年；Naumov等人，2020年；Zhao等人，2020年；Lui等人，2021年；Acun等人，2021年；Steck等人，2021年；Lian等人，2021年），将人工智能行业推向了**万亿参数时代**。实现万亿参数模型需要在人工智能系统基础设施上进行大量投资（Mudigere等人，2022年）。从系统设计的角度来看，主要问题/关注点是：

- 如何扩展？
- 哪种扩展方案提供更好的投资回报率（ROI）？
- 如何战略性地结合不同的扩展方案以提供更好的ROI？


图1显示了在5年时间（2016-2021）内，语言建模任务和DLRMs的模型规模增长了10000倍。这些结果只反映了已发布模型的增长。我们预计DLRMs的增长速度甚至更快。推荐系统是许多互联网公司的主要收入来源。因此，这些模型的细节通常是保密的。最近的研究表明，**在仅仅2年多的时间里（2019-2021），Facebook的推荐模型在参数数量上增长了20倍**，在训练集大小上增长了2.4倍，系统基础设施增长了2.5-2.9倍（Wu等人，2021年；Mudigere等人，2022年），并且超过50%的数据中心AI训练周期都致力于推荐模型（Acun等人，2021年）。**尽管它们很重要，但对于DLRM模型如何扩展，人们的认识有限**。识别和理解模型的扩展属性对于设计服务于这些模型的人工智能系统和基础设施至关重要。我们的论文是首次尝试解决这一差距。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/b96716e95cbf2a7badce255d9918434ea74917cbec7fd1cf7e2e6d883794bbb5a07f627e735b867369e6b470c925ebff?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 深度学习总体上，特别是基于深度学习的推荐模型近年来在参数规模上经历了指数级的增长（Sevilla等人，2021年；Mudigere等人，2022年；Lian等人，2021年）。请注意不同领域增长趋势的差异。

最近的工作（Hestness等人，2017年；Kaplan等人，2020年；Hernandez等人，2021年；Henighan等人，2020年；Gordon等人，2021年；Zhai等人，2021年；Brown等人，2020年；Hestness等人，2019年；Prato等人，2021年；Bahri等人，2021年）显示，在包括语言建模、机器翻译、视觉变换器、迁移学习和其他自回归模型在内的广泛领域中，高度可预测的扩展趋势。**然而，推荐系统如何扩展尚不清楚**。

此外，先前的研究在他们的扩展分析中没有包括embedding参数。**embedding参数占推荐模型容量的大部分（>90%）**，因此，研究它们对模型质量性能扩展的影响至关重要。

我们在这项工作中的目标是表征深度学习推荐模型的扩展规律，特别是点击率（CTR）预测模型。CTR模型是推荐系统中最重要的机器学习任务之一，为数十亿用户提供个性化体验。通过研究许多**不同模型规模N（跨越三个数量级）、计算预算C（跨越五个数量级）和数据集规模D（跨越三个数量级）**，我们展示了一个简单的幂律加常数可以解释CTR模型在一个周期内的性能与N、D和C之间的关系。

图11概述了一个典型的DLRM架构。在高层次上，有两个主要组件可以扩展：嵌入表和多层感知器（MLP）。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/026b8b5b5d6806ff960c2e8afe9c95cae71462d183bef69dd9fae81f419f6af73cfbb9f1ef0c67ce3749c31784cf7395?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=11.jpg&amp;size=750">

图11 深度学习模型架构的示意图。

- 嵌入表（embedding table）可以通过垂直扩展（增加每个表的嵌入行数）或水平扩展（扩展嵌入的维度）来扩展。
- MLP层可以通过加宽或加深层来扩展。

我们研究了在四种扩展方法上的推荐系统的经验扩展规律：扩展嵌入表（垂直和水平）、扩展顶层MLP层（我们称之为总架构层）以及扩展所有MLP层（包括通过增加宽度来扩展密集层、总架构层和密集-稀疏交互层）。

## 1.1 摘要

我们对CTR预测模型的关键发现如下：

**幂律加常数：**我们观察到，在训练一个周期后，推荐模型的性能（测试损失）与资源投入遵循幂律加常数关系（αx−β + γ）（见图2）。资源包括数据集大小、模型大小和计算浮点运算量。幂律加常数函数中的常数γ标识了扩展的极限：即我们假设可以无限扩展资源时能达到的最佳水平。表1显示了不同扩展方案和不同资源投入情景下经验收集的α、β和γ值。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/fb1dec4a829049f49873a28185e58d4fc616e304e558bb0303fac2ff9b8405677a5b14905af659396768fb8a81fffc36?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2 推荐系统的性能随着数据规模、模型规模以及训练计算量（FLOPs）的增加而呈现出幂律增长加上一个常数的特性：

- (a) 通过增加多层感知机（MLP）层的宽度来扩展模型规模。
- (b) 通过增加顶层网络层的宽度来扩展模型规模。
- (c) 通过增加嵌入表的维度来扩展模型规模。
- (d) 通过增加嵌入表中的行数来扩展模型规模。

**幂律函数的两个阶段：**如图3所示，幂律函数可以被一个高回报阶段和随后的低回报/饱和阶段所特征化。收益递减点是过渡发生的地方。如果使用幂律函数来比较两种扩展方案的效率，需要关注幂律函数的指数（β）以及操作阶段。指数较大且衰减更快的幂律函数更适合扩展。然而，处于饱和阶段的操作方案无论其指数如何，都不如非饱和方法。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/0fe02a38ce46f6c72fb5785f0638e0e091a9853b935aa0720d5ac90987b750508c9c78b9286d839f9a446fff7050dd09?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=3.jpg&amp;size=750">

图3 幂律函数特征曲线

**性能强烈依赖于数据集大小和计算能力，而与模型参数大小关系较弱：**模型性能强烈依赖于训练集中的样本数量（D）和计算浮点运算量（C），而与参数数量（P）关系较弱。

**扩展的极限：**幂律趋势中的常数（γ）捕获了不可减少的错误。这意味着通过扩展资源（模型参数、数据大小和/或计算浮点运算）到无限大所能达到的最佳归一化测试损失将饱和在0.98。

**数据扩展效率：**所有扩展方案的数据扩展效率相似（β在[0.09, 0.12]范围内），并且对模型大小不敏感。所有扩展方案都处于高回报阶段。根据图4中显示的幂律指数，可以看出垂直扩展嵌入表（V）比水平扩展嵌入表（H）更好，而水平扩展嵌入表又比顶层MLP层扩展（O）更好，后者又比MLP层扩展（M）在数据扩展效率方面更好。这意味着在固定参数预算下，通过同时扩展数据集大小和模型大小来扩展模型性能，对参数扩展方法有些敏感。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/9de762572782773fb584b1be16dd9e61cbac57c3e94d58300692d4b1cc626cd5ccc1fdea90c376590d2023acfa4bd453?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=4.jpg&amp;size=750">

图4 不同模型扩展方案中的数据扩展效率。尽管每条线显示了在固定模型规模下的数据扩展趋势，每个图表中的虚线及其对应的方程捕捉了帕累托最优曲线。如图所示，不论扩展方案如何，当模型和数据一起扩展时，所有模型或多或少具有相同的幂律扩展特性（幂指数为-0.1），这意味着所有模型扩展方案中的数据扩展效率是相同的。

**计算扩展效率：**所有扩展方案的计算扩展效率相似（β在[0.12, 0.15]范围内）。所有扩展方案都处于高回报阶段。根据图5中显示的幂律指数，可以看出MLP扩展比顶层扩展更计算效率高，顶层扩展又略比嵌入维度扩展更计算效率高。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/ba3a4fce0eed5979dfe0e2240310355d12b4c96d358a05f5226c5ecd126ed467a5ca62af0d41256d10a95b203894777a?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=5.jpg&amp;size=750">

图5 计算扩展效率 - 两种视角：(a) 同时扩展计算量（FLOPs）和数据集规模 (b) 同时扩展计算量（FLOPs）和模型规模。

**参数扩展效率：**不同扩展方案的参数扩展效率不同（α在[0.4, 7.6]范围内）。然而，所有扩展方案都处于饱和阶段（见图6）。对于一个工业级模型，所有参数扩展技术在参数扩展效率方面相似。这意味着在固定数据预算下，通过增加模型中的参数数量来扩展模型性能，对参数扩展方法不敏感。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/cc2254fe91ca12f469723b1ddbaba77305d9cc507a125cf0cd80211551eb6b6f518f5fca995274b7d04b6a76c5330e8b?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=6.jpg&amp;size=750">

图6 不同参数扩展方案中的参数扩展效率。在所有扩展方案中可见的模式是，准确性与参数规模之间的弱依赖性。

# 2. 扩展效率

在给定固定预算/资源的情况下，主要问题是哪种扩展方案可以提供更好的投资回报率（ROI）。我们针对三种不同的资源，即数据、参数和计算浮点运算量，对扩展效率进行了表征。我们展示了所有扩展方案在数据扩展和计算扩展效率上都相似，并且仍有改进空间。另一方面，参数扩展效率非常低，因为它已经超出了收益递减点。

## 2.1 数据扩展效率

为了研究数据扩展效率，我们在广泛范围内（三个数量级）扩展数据集大小，同时保持模型大小不变。从概念上讲，线的斜率捕捉了模型在面对问题时吸收新信息的有效性。结果如图4所示。每个图表捕捉了不同的模型扩展方案（垂直嵌入、水平嵌入、顶层和MLP扩展）。

正如所有扩展策略所示，推荐系统的性能强烈依赖于数据集大小，而与参数/模型大小关系较弱。这是违反直觉且非常有趣的。我们继续看到在过去5年中嵌入表的大小和嵌入表的数量不断增长。这些结果意味着工业级模型在过拟合范围内运行。

虽然图4中的每条线显示了固定模型大小的数据扩展趋势，但每个图表中的虚线捕捉了帕累托前沿线。如图所示，无论扩展方案如何，所有模型都有类似的幂律趋势。这意味着所有模型扩展方案的数据扩展效率相似。

**摘要** 推荐系统的性能强烈依赖于数据大小，而与参数/模型大小关系较弱。与大规模语言模型（Hestness等人，2017年；Kaplan等人，2020年）相比，其中性能与模型大小强烈相关，推荐系统对模型大小的敏感性较弱，这在设计下一代推荐系统时需要考虑。所有扩展方案的数据扩展效率相似。这意味着所研究的模型以相同的速率从新数据中吸收信息，无论其背后的扩展方案如何。输入粒度/词汇量大小对扩展趋势没有显著影响。

## 2.2 计算扩展效率

我们的目标是表征捕捉模型质量性能与计算浮点运算量之间关系的线的斜率。从概念上讲，线的斜率捕捉了模型在面对问题时对新计算浮点运算量吸收新信息的速度。在计算效率分析中，我们保持数据（或模型大小）不变，同时扩展模型大小（或数据大小）。当我们扩展模型大小或数据大小时，我们间接地增加了计算浮点运算量。还有另一种方法可以在不改变数据大小或模型大小的情况下扩展计算浮点运算量，那就是训练更长时间的模型。我们留待未来的工作。

图5显示了这种扩展的结果。每个图表捕捉了不同的模型扩展方案（水平嵌入、顶层和MLP扩展。注意我们没有显示垂直扩展的计算扩展，因为增加行数对计算浮点运算量没有任何影响。）如图所示，所有扩展策略中，推荐系统的性能强烈依赖于计算浮点运算量的数量。我们以两种不同的方式呈现相同的结果：（1）通过模型扩展增加计算浮点运算量，同时保持数据大小不变（图5，顶行）。（2）或者，我们通过数据扩展增加计算浮点运算量，同时保持模型大小不变（图5，底行）。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/e96e61172db6f47ff1aebe4bf03cbf4c0fe3ea361f133cbb3930d836dc63a830b6a9faaa199895269e57743009cd61f8?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=7.jpg&amp;size=750">

图7 何时选择垂直扩展（Vertical Scaling）与水平扩展（Horizontal Scaling）？

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/0e9532e9df8b6c3814b1637a1496e7c44d3b8a58ce87350ad13bdc1d6dadc3e61dd11a9ef72cf3f47bb7cdc456ea3393?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=8.jpg&amp;size=750">

图8 何时选择顶层网络扩展（Over-arch Scaling）与多层感知机扩展（MLP Scaling）？

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/7f64c7fa4777fb3d0d3ff5a666ad0cf7c5d8145332ef9126fdab2433e4ea5b06a80b14c09dc4dc3ec343964244cd92ff?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=9.jpg&amp;size=750">

图9 嵌入维度对表大小的敏感性：每条线展示了不同的垂直扩展因子（VSF）。大的蓝色圆圈显示了每条线的最小损失。然而，曲线的拐点在64处始终如一地出现。

**同时扩展计算和数据** 图5顶行显示了通过扩展模型大小对性能的计算浮点运算量扩展影响。在每条线内，我们保持数据大小不变，同时通过模型大小扩展增加计算浮点运算量。注意不同扩展方案的幂律方程的幂之间的轻微差异。看来，MLP扩展略优于顶层扩展，顶层扩展又略优于嵌入维度扩展，在相同增加的计算预算下提高模型准确性（0.15对-0.14对-0.12）。此外，如图所示，在固定的计算预算下，更大的数据集大小会带来更好的性能。同时，在固定的准确性目标下，更小的数据集大小更具计算效率。

**同时扩展计算和模型大小** 图5底行显示了通过扩展数据大小对性能的计算浮点运算量扩展影响。在每条线内，我们保持模型大小不变，同时通过扩展数据集大小增加计算浮点运算量。如图所示，在固定的计算预算下，更大的模型获得更低的性能。同时，在固定的准确性目标下，更小的模型大小更具计算效率。虚线捕捉了在每个计算预算下获得最佳性能的最佳模型大小。图5(a)和(b)基本上是同一组点，从两个不同的视角呈现（一次基于数据集大小对点进行分组，一次基于模型大小进行分组），因此，帕累托最优线（虚线）将是相同的。

**摘要** 在固定的计算预算下，需要在在更大的数据集大小上训练模型或训练具有更多参数的模型之间做出权衡。我们观察到，在固定的计算预算下，具有更多参数的模型显示出更低/更差的性能，而用更大的数据集大小训练的模型显示出更好的性能。从计算效率的角度来看，我们观察到，在第一个周期，MLP扩展优于顶层扩展，顶层扩展优于水平扩展嵌入表。注意，垂直扩展嵌入表对计算浮点运算量没有任何影响。

# 3. 敏感性分析

## 3.1 如何有效地按行数扩展嵌入维度？

图9展示了随着我们在表中增加行数（增加垂直扩展因子）时最佳嵌入维度的变化情况。如图所示，随着垂直扩展因子的增大，最佳嵌入维度趋于变小（对于0.125×和0.25×的垂直扩展因子，256是最佳嵌入维度，而对于0.5×、1×和2×的垂直扩展因子则是128）。然而，最佳性能和最具资源效率的嵌入维度并不一定是相同的。如图所示，曲线的拐点（收益递减点）对于所有表大小在嵌入维度=64左右开始出现。这意味着从资源效率的角度来看，嵌入维度的资源高效设计点对垂直扩展因子的依赖性较弱。这一结果暗示，从资源效率的角度来看，超过64将不会提供高投资回报率。

## 3.2 训练与测试

如图10所示，训练数据的学习曲线比测试数据的学习曲线更陡峭（-0.20对比-0.12）。两条曲线都捕捉了在相同数据上训练的相同模型的扩展，但在两个不同的数据集上进行了评估。左侧的曲线在训练集的数据点上进行了评估，右侧的模型在测试集上进行了评估。这种差距意味着模型从额外的训练点吸收的信息在预测来自相同分布（训练分布而非测试分布）的数据时更有效，这是意料之中的。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/e1303f0c8613f0566ede875d06781b871e5265e96396e984b80a0c4d02e65748759f4d481dbcc4c914714cf4116547d8?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=10.jpg&amp;size=750">

图10 training loss与testing loss上的数据扩展效率。请注意训练曲线和测试曲线之间幂律指数的差异。

# 4. 讨论

特征化不同扩展方案的幂律曲线提供了每种扩展技术的数据效率、参数效率和计算效率的见解。人们可以通过比较它们在三个不同轴（数据、计算、参数）上的幂律曲线，潜在地比较任何成对扩展技术的效率。表2显示了这种比较的结果。如图所示，没有单一的扩展技术在所有扩展效率维度上都脱颖而出。例如，水平嵌入扩展（H）在数据效率方面优于MLP扩展（M），但在计算效率方面则较差。

最近的分析显示，在短短5年多的时间里，工业级推荐模型增长了四个数量级（Mudigere等人，2022年；Lian等人，2021年）。幂律分析支持了过去的趋势。当按幂律趋势近似时，参数扩展的指数幅度最大。然而，工业级推荐模型已经过于庞大且饱和，因此进一步的参数增长不会从资源效率的角度提供高投资回报率。

与此同时，数据扩展和计算扩展仍然处于高收益递减的范围内。这意味着在更好的模型架构出现之前，应该将数据扩展视为一流的扩展方法。话虽如此，我们应该意识到，由于数据保留的限制，数据扩展从长远来看（以原始形式）并不是一种可持续的方法。

为了克服这一点，我们需要考虑替代方案。以下是一些建议，其中一些我们将作为下一步探索：(1) 记录更多数据，特别是通过记录更多负样本和减少正样本下采样；(2) 探索使用历史数据作为教师模型来训练模型，以合成从历史数据中学习到的有价值信息，供更近期的模型使用；(3) 水平扩展数据量而不是垂直扩展，即增加更多特征而不是增加更多行。

扩展法则也可以用来指导长期硬件开发。硬件设计通常提前3-5年开始，依靠对未来3-5年模型增长的准确预测。我们的分析表明，展望未来，硬件不需要增长来支持更大的模型。相反，我们需要设计硬件/系统来支持使用更大的数据集进行训练。

另一个关键的收获是，幂律加常数方程中的常数在0.98（以归一化熵度量的损失）处有界。这个常数捕获了在无限扩展极限下模型的准确性，可以用作衡量工业级模型与无限极限的距离的指南。在NLP领域的先前分析表明，模型架构的创新（例如，从LSTM过渡到Transformer）可以改善幂律的系数（即α.x−β + γ中的α），并向下移动曲线，但它们对幂律的指数（β）几乎没有影响（Hestness等人，2017年；Brown等人，2020年）。这表明模型架构探索是性能增长的短期解决方案。长期解决方案将需要改善幂律趋势的指数。至今，是什么控制了幂律的斜率仍然是一个开放的研究问题。幂律曲线的斜率似乎对每个领域都是独特的，与模型架构无关（Hestness等人，2017年；2019年）。先前的分析表明，改善数据分布可以改善幂律的指数（Bahri等人，2021年）。最近的工作表明，通过有效的数据修剪，我们可以打败幂律并实现指数级扩展（Sorscher等人，2022年）。

[https://arxiv.org/pdf/2208.08489](https://arxiv.org/pdf/2208.08489)