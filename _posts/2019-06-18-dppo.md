---
layout: post
title: PPO(deepmind版)介绍
description: 
modified: 2019-06-19
tags: 
---

deepmind在《Emergence of Locomotion Behaviours in Rich Environments》提出了PPO。我们来看下：

# 1.介绍

略

# 2.使用Distributed PPO进行大规模增强学习

我们在增强学习上关注的点是，在丰富的仿真环境上使用连续state和action spaces。我们的算法在多个任务变种上是健壮的，可以有效扩展到其它领域。我们将轮流介绍每个issues。

**使用PPO的Robust policy gradients**

深度学习算法基于大规模、高吞吐的optimization方法，在离散和低维action spaces中（比如：Atari games和三维导航(3D navigation)）可以产生state-of-the-art的结果。作为对比，许多之前的工作都是基于连续动作空间的，关注一些可对比的小问题。大规模、分布式的optimization较少广泛使用，相应的算法也很少被开发。我们发布了一个robust policy gradient算法，很适合**高维连续控制**的问题，可以使用分布式计算扩展到许多更大的领域上。

Policy gradient算法为连续控制（continuous control）提供了一个吸引人的范式。他们通过根据stochastic policy $$\pi_{\theta}(a \mid s)$$的参数$$\theta$$直接最大化rewards的期望和：

$$J(\theta) = E_{\rho_{\theta}(\tau)} [\sum_t \gamma^{t-1} r(s_t, a_t)]$$

该期望会对应于由policy $$\pi_{\theta}$$和系统动态性(dynamics) $$p(s_{t+1} \mid s_t, a_t)$$两者所联合生成的trajectories$$\tau=(s_0,a_0,s_1,\cdots)$$的分布：

$$
\rho_{\theta}(\tau) = p(s_0) \pi(a_0 | s_0) p(s_1 | s_0, a_0) ...
$$

$$\theta$$对应的目标函数的梯度是：

$$\nabla_{\theta} J = E_{\theta} [\sum_t \nabla_{\theta} log \pi_{\theta}(a_t | s_t)(R_t - b_t)]$$

其中：

- $$R_t = \sum_{t'=t} \gamma^{t'-t} r(s_{t'}, a_{t'})$$
- $$b_t$$是一个baseline（它不依赖于$$a_t$$或者future states和actions）。该baseline通常会选择$$b_t = V^{\theta} (s_t) = E_{\theta} [R_t \mid s_t]$$。

实际上，期望的future return通常近似于使用一个sample rollout，$$V^{\theta}$$通过一个学到的近似$$V_{\phi}(s)$$和参数$$\phi$$来替代。

Policy gradient的estimates可以有很高的variance，**算法对于超参数的设置很敏感**。有许多方法可以使得policy gradient算法更健壮。**一种有效的measure是：采用一个trust region constraint，可以限制policy update的任意更新量(amount)**。采用该思想的一种流行算法是：TRPO（trust region policy optimization）。在每个迭代中，给定当前参数$$\theta_{old}$$，TRPO会收集一个（相对较大）的batch数据，并优化surrogate loss：

$$
J_{TRPO}(\theta) = [ \sum_t \gamma^{t-1} \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)} A^{\theta_{old}}(a_t, s_t)]
$$

它会服从一个constraint：policy被允许更改多少，以KL divergence的方式表示：

$$
KL[\pi_{\theta_{old}} | \pi_{\theta}] < \delta
$$

$$A^{\theta}$$是advantage function，可以通过$$A^{\theta}(s_t, a_t) = E_{\theta}[R_t \mid s_t, a_t] - V^{\theta}(s_t)$$得到。

PPO算法可以被看成是一个依赖于一阶梯度的TRPO的近似版本，**它可以更方便地使用RNNs，并可以用在一个大规模分布式setting中**。trust region constraint通过一个正则项来实现。使用的正则项系数依赖于该是否先前违反了constraint。算法1展示了PPO算法的伪代码。

<img src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/0b2b391a78d84091d875cc56e4a3712cf854eccdb78d343d97319f9473470f67d0b09f7ec3ea2e1dcefd8f2eb0dfc38b?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=1.jpg&amp;size=750">

算法1  PPO

在算法1中，超参数$$KL_{target}$$是在每个iteration上policy的期望变更。如果在policy上的实际变更很低、或者大大超过target KL，归一化项$$\alpha > 1$$可以控制着KL-正则参数的调整（例如：落在区间$$[\beta_{low} KL_{target}, \beta_{high} KL_{target}]$$之外）。

**分布式PPO（DPPO）进行可扩展增强学习** 

为了在丰富的、仿真环境上达到较好性能，我们实现了一个分布式版本的PPO算法(DPPO)，数据的收集和梯度计算在workers间是分布式的。我们使用同步更新和异步更新进行实验，发现平均梯度和将以同步方式使用会导致更好的结果。

原始的PPO算法会使用完整的rewords求和来估计advantages。为了使用RNN及batch updates，同时支持可变长的episodes，我们会遵循与[2]相似的策略(strategy)，并使用一个长度为K的空间的截断BPTT（truncated backpropagation through time）。这使得它很自然地（虽然不是必须）使用K-step returns来估计advantage，例如：我们会在相同的K-step windows对rewards求和，并在K-steps后对value function进行bootstrap：$$\hat{A}_t = \sum_{i=1}^{K} \gamma^{i-1} r_{t+i} + \gamma^{K-1} V_{\phi}(s_{t+k} - V_{\phi}(s_t)$$

John Schulman[20]的公开提供的PPO实现对于核心算法增加了一些修改。它们包含了对inputs、rewards、以及在loss中的一个额外项（它会惩罚对于较大违反trust region constraint的情况）的normalization。我们采用在分布式setting中相似的augmentations，但发现在跨多个workers间对多个统计进行sharing和synchronization需要一些注意。我们的分布式实现(DPPO)采用tensorflow，参数在一个parameter server中，workers会在每个gradient step后同步它们的参数。伪码和详情在附件中提供。

## A.DPPO

**A.1 算法详情**

DPPO算法的伪码在算法2和算法3中提供。W是workers的数目；D会为workers的数目设置一个阀值，它们的gradients必须被提供来更新参数。M,B是在给定一个batch的datapoints，使用policy和baseline updates的sub-iterations的数目。T是在参数更新被计算之前每个worker收集的data points的数目。K是用于计算K-step returns和truncated BPTT的time steps的数目。

<img src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/5da230ca76b0191f04e304acd449b941cfaa584729e24bece04a0abb328ac4c11e93e0014dbf7f3aabbdd955db5085ee?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=2.jpg&amp;size=750">

算法2: 

<img src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/6bb723f181391395435745b78494ad988e8dbe19bb28d62ac3d7993fa9ec1c4191d1a9747bcff9e2688630b433f3653c?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=3.jpg&amp;size=750">

算法3: 

**normalization**

根据[20]我们会执行以下的normalization steps：

- 1.我们会通过减mean并除以标准差，将observations（或者 states $$s_t$$）进行归一化。。。
- 2.我们会通过一个正在运行的关于标准差的estimate对reward进行缩放（scale），在整个实验过程中进行聚合
- 3.我们会使用关于advantages的per-batch normalization

**跨workers共享算法参数**

在分布式setting中，我们发现，对于跨workers的data normalization来说共享相关统计很重要。Normalization在数据收集(collection)期间使用，统计会在每个environment step之后被本地更新。当一次迭代完成时，统计的本地变改（local changes）在数据收集(collection)后被应用到全局统计中。随时间变化（time-varying）的正则化参数$$\lambda$$也会跨workers共享，但更新会基于本地统计（它基于对每个worker在本地计算的平均KL）来决定，通过使用一个调整参数$$\hat{\alpha} = 1+(\alpha-1)/K$$由每个worker单独使用。

**额外的trust region constraint**

当KL超过期望变更的一个特定margin时（），我们也会采用一个额外的罚项。在我们的分布式实现中，该criterion会在一个per-worker basis上进行测试和应用。

# 参考

- 1.[https://arxiv.org/abs/1707.02286](https://arxiv.org/abs/1707.02286)
- 2.[https://github.com/joschu/modular_rl, 2016.](https://github.com/joschu/modular_rl)