---
layout: post
title: facebook DLRM介绍
description: 
modified: 2019-10-15
tags: 
---

# 介绍

facebook在2019在《Deep Learning Recommendation Model for
Personalization and Recommendation Systems》。

# 摘要

facebook开发了一种SOTA的深度学习推荐模型（DLRM）并提供了Pytroch和Caffe2的实现。另外，还设计了一种专门的并行化scheme利用在embedding tables上的模型并行机制来缓和内存限制，利用数据并行机制来从fully-connected layers中扩展（scale-out）计算。我们比较了DLRM与已存在推荐模型.

# 1.介绍

略

# 2.模型设计与架构

在本节中，我们会描述DLRM的设计。我们会从网络的高级别组件开始，并解释how和why它们以一种特别的方式组合在一起，对未来模型设计有启发，接着描述组成模型的低级别operators和primitives，用于未来的硬件和系统设计。

## 2.1 DLRM组件

通过回顾以往模型，DLRM的高级别组件可以很容易理解。我们会避免完整回顾，把精力集中在早期模型的4个技术上，它可以在DLRM中的高级组件中被解释。

### 2.1.1 Embeddings

为了处理类型化数据，embeddings可以将每个category映射到一个在抽象空间中的dense表示上。特别的，每个embedding lookup可以被解释成使用一个one-hot vector $$e_i$$来获得embedding table $$W \in R^{m \times d}$$相应的row vector：

$$
w_i^T = e_i^T W
$$

...(1)

在更复杂的情况下，一个embedding也可以表示成多个items的加权组合，它具有一个关于weights的multi-hot vector  $$a^T = [0, \cdots, a_{i_1}, \cdots, a_{i_k}, \cdots, 0]$$，其中，对于$$i=i_1, \cdots, i_k$$，元素$$a_i \neq 0 $$，否则为0 ，其中$$i=i_1, \cdots, i_k$$是相应的items。注意，t embedding lookups的一个mini-batch可以写成：

$$
S = A^T W
$$

...(2)

其中，sparse matrix为：$$A = [a_1, \cdots, a_t]$$。

DLRMs会使用embedding tables来将categorical features映射成dense representations。然而，在这些embeddings被设计后，如何利用它们来生成更精准的预测呢？我们先来回顾下latent factor。

### 2.1.2 Matrix Factorization

推荐问题的常用形式，我们给定一个集合S：用户会对一些商品进行评分。
。。。

$$
min \sum\limits_{(i,j) \in S}  r_{ij} - w_i^T v_j
$$

...(3)

### 2.1.3 Factorization Machine

$$
\hat{y} = b + w^T x + x^T upper(VV^T) x
$$

...(4)

### 2.1.4 MLP(Multilayer Perceptrons)

同时，在机器学习上的最近许多成功都归因于deep learning。DL最基础的模型是：MLP。预测函数由一串交替的FC layers和activation function $$\sigma: R \rightarrow R$$组成：

$$
\hat{y} = W_k \sigma(W_{k-1} \sigma(W_1 x + b_1) \cdots) + b_{k-1}) + b_k)
$$

...(5)

该方法被用于捕获更复杂的交叉。例如，给定足够参数，MLP会具有够深和够宽，可以拟合任意想预测的数据。这些方法的变种被广告用于CV和NLP中。例如：NCF被用于MLPerf benchmark的一部分，它使用MLP，而非dot product来计算MF中embeddings间的交叉。

## 2.2 DLRM架构

我们已经描述了在RS中不同的模型。我们将这些想法进行组合来构建SOTA的个性化模型。

假设用户和商品通过许多连续型特征（continuous features）和类别型特征（categorical features）进行描述。为了处理categorical features，每个categorical feature可以通过一个相同维度的embedding vector表示，即MF中latent factors。为了处理continous features，会通过一个MLP来进行转换，它会生成和embedding vectors相同长度的dense representation。

我们将根据FMs提供的处理sparse data的方式，将它们传给MLPs，显式地(explicitly)计算不同特征间的二阶交叉（second-order interaction）。这可以通过使用所有embedding vectors的pairs和dense features间的dot product来做到。这些dot products可以使用另一个MLP(top 或 output MLP)将original-processed dense features和post-processed一起concatenated，接着被feed到一个sigmoid function来提供一个概率。

<img src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/97d8441dcc3ffcd6175c13a9510b9c7c46aa8e9f04317925cbf2ede503382bf1dbd315a33dee5ba9212dbaaef7594bea?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=f1.jpg&amp;size=750">


我们将产生的模型称为：DLRM。如图1所示，并在表1中展示了PyTroch和Caffe2的DLRM所用到的一些operators。

<img src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/dc4ec4819daf1e4e3b7b0019d13fcd14ef6f9d67f7a7e2a21093cfb00c6cfa7e3c7a7e061367e97e10e6d5600818d3fc?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=f2.jpg&amp;size=750">

表1

## 2.3 与之前的模型比较

许多deep learning-based的推荐模型，使用相似的底层思想来生成高阶项来处理sparse features。Wide&Deep, Deep&Cross, DeepFM, xDeepFM网络，例如，设计专有网络来有系统的构建高阶交叉。这些网络接着将来自这些专有模型和MLP的结果进行求和（sum），将它传给一个linear layer及sigmoid activation来生成一个最终概率。DLRM以一种结构化的方式与embeddings交互，通过只考虑由final MLP中embeddings pairs间的dot-product生成的cross-terms，模拟了FM对模型进行极大地降维。对于在其它网络的二阶交叉外的更高阶交叉，使用额外的计算/内存开销并不值。

# 3.并行化（Parallelism）

模型个性化和推荐系统，需要大且复杂的模型来估计大量数据上的价值。DLRMs特别包含了许多数目的参数，多阶的幅度要超过其它常见的deep learning模型（比如：CNN），transformer、RNN、GAN。这会导致训练时间上常达许周或更久。因此，对这些模型进行高效并行化，以便解决在实际规模中的问题。

如前面章节所示，DLRMs会以成对（coupled）的方式，同时处理categorical features（使用embeddings）以及continuous features（使用bottom MLP）。Embeddings会占据参数的大部分，一些tables每个都需要超过多个GBs的内存，使得DLRM对内存容量和带宽很敏感。embeddings的size使得它禁止使用数据并行化（data parallelism），因为它需要在每个设备上复制很大的embeddings。在许多cases中，这种内存限制需要模型分布跨多个设备，以便能满足内存容量需求。

在另一方面，MLP参数在内存上是更小的，但需要大量计算。因此，data-parallelism对MLPs更好，因为它可以让不同devices上的samples并发处理，只需要在当累积更新（accumulating updates）时需要通信。我们的并行化DLRM会使用一个embeddings的模型并行化(model parallelism)以及MLPs的数据并行化(data parallelism)的组合，来减缓由embeddings生成的内存瓶颈，而MLPs上的forward和backward propagations并行化。通过将model和data parallelism进行组合，是DLRM的唯一需求，因为它的架构和大模型size所导致；这样的组合并行化在Caffe2或PyTroch中并不支持（以及其它流行的DL框架），因此，我们设计了一种定制实现。我们计划在将来提供它的详细效果研究。

在我们的setup中，top MLP和interaction operator需要访问部分来自bottom MLP的mini-batch以及和所有embeddings。由于模型并行化已经被用于跨devices分布embeddings，这需要一个个性化的all-to-all通信。在embedding lookup的尾部，对于在mini-batch中的所有samples（必须根据mini-batch维度进行分割、以及与相应devieces进行通信）、对于在这些devices上的embedding tables，每个device都具有一个vector，如图2所示。Pytorch或Caffe2都不会提供model parallelism的原生支持；因此，我们通过显式将embedding operators（PyTorch的nn.EmbeddingBag, Caffe2的SparseLengthSum）映射到不同devices上来实现它。个性化的all-to-allcwpwy使用butterfly shuffle operator来实现，它可以将生成的embedding vectors进行切片（slices），并将它们转移到目标设备（target devices）上。在当前版本，这些transfers是显式的copies，但我们希望后续使用提供的通信原语（比如：all-gather以及send-recv）进一步optimize。

我们注意到，对于数据并行化MLPs，在backward pass中的参数更新会使用一个allreduce进行累积（accumulated），并以一种同步方式将它用在每个device的参数复制上，确保在每个device上的参数更新在每轮迭代上是一致的。在Pytorch中，data parallelism可以通过nn.DistributedDataParallel和nn.DataParallel模块来开启，将在每个device上的model复制，使用必要的依赖插入allreduce。在Caffe2中，我们会在梯度更新前手工插入allreduce。

# 4.数据

为了measure模型的acuracy，并测试它的整体效果，并将单独operators特征化，我们需要为我们的实现创建或获得一个dataset。我们模型的当前实现支持三种类型的datasets：random、synthetic、public datasets。

前两个dataset对于从系统角度实验我们的模型很有用。特别的，它允许我们通过生成即时数据，并移除数据存储依赖，来测试不同的硬件属性及瓶颈。后一个dataset允许我们执行真实数据的实验，并measure模型的accuracy。

## 4.1 Random

回顾DLRM，它接收continuous和categorical features作为inputs。前者可以通过生成一个随机数目的vector，通过使用一个uniform/normal(Gaussian)分布（numpy.random rand/randm缺省参数）。接着通过生成一个matrix来获得mini-batch inputs，其中每行对应在mini-batch中的一个element。

为了生成categorical features，我们需要决定在一个给定multi-hot vector中具有多少非零元素。benchmark允许该数字可以是fixed或在一个[1,k]的范围内random。接着，我们生成整型indices的相应数字，范围在[1,m]中，其中，m是在embedding W中的rows数目(2)。最后，为了创建一个mini-batch的lookups，我们将以上indices进行concatenate，并将每个单独的lookup使用lengths和offsets进行描述。

## 4.2 Synthetic

对应于categorical features，有许多理由支持定制索引的生成。例如，如果我们的应用使用一个特定dataset，但我们不希望出于私人目的共享它，那么我们可以选择通过distributions来表示categorical features。这可以潜在作为一种隐私保护技术的可选方法（用于联邦学习(federated learning)）。同时，如果我们希望练习系统组件（比如：学习内存行为）。。。

## 4.3 Public






# 参考

- 1.[https://arxiv.org/pdf/1906.00091.pdf](https://arxiv.org/pdf/1906.00091.pdf)