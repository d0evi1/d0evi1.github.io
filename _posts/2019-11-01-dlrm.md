---
layout: post
title: facebook DLRM介绍
description: 
modified: 2019-11-01
tags: 
---

# 介绍

facebook在2019在《Deep Learning Recommendation Model for
Personalization and Recommendation Systems》。

# 摘要

facebook开发了一种SOTA的深度学习推荐模型（DLRM）并提供了Pytorch和Caffe2的实现。另外，还设计了一种专门的并行化scheme利用在embedding tables上的模型并行机制来缓和内存限制，利用数据并行机制来从fully-connected layers中扩展（scale-out）计算。我们比较了DLRM与已存在推荐模型.

# 1.介绍

在大型互联网公司中的许多任务上，部署了个性化和推荐系统，包括：CTR预估和rankings。尽管这些方法具有很长的历史，这些方法最近才拥抱神经网络。对于个性化和推荐，朝着深度学习模型架构设计方向贡献了两个主要视角。

第一个视角来自于**推荐系统**。这些系统最初部署了content filtering，其中：运营人员会将products按categories分类，而用户选择它们喜欢的categories，因而可以基于它们的偏好进行match[22]。该领域接着演化成使用collaborative filtering，基于用户过往行为（比如：用户对商品的评分）进行推荐。最近邻方法[21]通过将users和products进行分组（grouping）在一起来提供推荐，latent factor方法通过MF技术以及特定隐式factors将users和products进行特征化，并成功部署。

第二个视角来自**预测分析(predictive analytics)**，它依赖于统计模型根据给定数据来对events进行分类（classify）或预测（predict）。预测模型从简单模型（比如：linear或logistic regression）转向到深度网络上来建模。为了处理类别型数据，这些模型采用了embeddings，它会将one-hot和multi-hot vectors转化成在一个抽象空间中的dense表示。该抽象空间可以被解释成由推荐系统发现的latent factors空间。

在本paper中，我们引入了一个个性化模型，它可以通过将上述两个视角进行联合来表示。模型会：

- 1.**使用embeddings来处理稀疏特征（sparse features）（它可以表示categorical data）**
- 2.**使用一个multilayer perceptron（MLP）来处理dense features**

接着使用[24]中的统计技术将这些features进行显式交叉。最终，它会**使用另一个MLP来post-processing交叉**来寻找event  probability。我们将该模型称为：DLRM（深度学习推荐模型）。见图1。该模型的PyTorch和Caffe2实现已公开。

# 2.模型设计与架构

在本节中，我们会描述DLRM的设计。我们会从网络的high-level组件开始，并解释how和why它们以一种特别的方式组合在一起，对未来模型设计有启发，接着描述组成模型的low-level operators和primitives，用于未来的硬件和系统设计。

## 2.1 DLRM组件

通过回顾以往模型，DLRM的high-level组件可以很容易理解。我们会避免完整回顾，把精力集中在早期模型的4个技术上，它可以在DLRM中的高级组件中被解释。

### 2.1.1 Embeddings

为了处理类型化数据，**embeddings可以将每个category映射到一个在抽象空间中的dense表示**上。特别的，每个embedding lookup可以被解释成使用一个one-hot vector $$e_i$$来获得embedding table $$W \in R^{m \times d}$$相应的row vector：

$$
w_i^T = e_i^T W
$$

...(1)

在更复杂的情况下，一个embedding也可以表示成多个items的加权组合**，它具有一个关于weights的multi-hot vector：

  $$a^T = [0, \cdots, a_{i_1}, \cdots, a_{i_k}, \cdots, 0]$$
  
其中，

- 对于$$i=i_1, \cdots, i_k$$，元素$$a_i \neq 0 $$，否则为0 ，其中$$i=i_1, \cdots, i_k$$是相应的items。

注意，t embedding lookups的一个mini-batch可以写成：

$$
S = A^T W
$$

...(2)

其中，sparse matrix为：$$A = [a_1, \cdots, a_t]$$。

**DLRMs会使用embedding tables来将categorical features映射成dense representations**。然而，在这些embeddings被设计后，如何利用它们来生成更精准的预测呢？我们先来回顾下latent factor。

### 2.1.2 Matrix Factorization

推荐问题的常用形式，我们给定一个集合S：用户会对一些商品进行评分。我们通过两个vector：

- $$w_i \in R^d, i=1,\cdots, n$$来表示第i个商品，
- $$v_j \in R^d, j=1, \cdots, m$$来表示第j个user

以便寻找所有的ratings，其中n和m各表示products和users的总数。更严格的，当第i个商品已经被第j个user评分时，集合S包含了(i,j) tuples。

MF方法通过最小化下面的等式来求解该问题：

$$
min \sum\limits_{(i,j) \in S}  r_{ij} - w_i^T v_j
$$

...(3)

其中：

- $$r_{ij} \in R$$是第j个user对第i个product的rating，$$i=1, \cdots, m; j = 1, \cdots, n$$。

接着，假设：$$W^T = [w_1, \cdots, w_m]$$和$$V^T = [v_1, \cdots, v_n]$$，我们希望将full matrix的ratings $$R=[r_{ij}]$$近似为矩阵乘法 $$R \approx W V^T$$。注意，W和V可以被解释成两个embedding tables，其中每一行表示在latent factor space中的一个user/product。**这些embedding vectors的dot product会生成后续rating的一个有意义的预测，这对于FM和DLRM的设计来说是一个key observation**。

### 2.1.3 Factorization Machine

在经典问题中，我们希望定义一个预测函数：$$\phi: R^n \rightarrow T$$，从一个输入数据点$$x \in R^n$$到一个target label $$y \in T$$上的预测。作为示例，我们可以通过定义 $$T = \lbrace +1, -1 \rbrace$$预测CTR，其中：+1表示点击，-1表示未点击。

FM使用categorical data，通过定义以下形式的模型，来将二阶交叉并入到一个线性模型中：

$$
\hat{y} = b + w^T x + x^T upper(VV^T) x
$$

...(4)

其中：

- 1.$$V \in R^{n \times d}$$
- 2.$$w \in R^n$$
- 3.$$b \in R$$
- 4.$$d << n$$的参数
- 5.**upper会严格选择该矩阵的上三角部分**【24】。

FM与SVM和polynomial kernels有明显区别，因为它们将二阶交叉矩阵分解成latent factors（或embedding vectors）（和MF很像），它能更有效地处理稀疏数据。**通过只捕获不同embedding vectors pairs间的交叉，这可以极大减小二阶交叉的复杂度，生成线性的计算复杂度**。

### 2.1.4 MLP(Multilayer Perceptrons)

同时，在机器学习上的最近许多成功都归因于deep learning。**DL最基础的模型是：MLP**。预测函数由一串交替的FC layers和activation function $$\sigma: R \rightarrow R$$组成：

$$
\hat{y} = W_k \sigma(W_{k-1} \sigma(W_1 x + b_1) \cdots) + b_{k-1}) + b_k)
$$

...(5)

其中：

- $$W_l \in R^{n_l \times n_{l-1}}$$是weight matrix
- $$b_l \in R^{n_l}$$：表示对于$$layer \ l=1,\cdots,k$$的bias

该方法被用于捕获更复杂的交叉。例如，给定足够参数，MLP会具有够深和够宽，可以拟合任意想预测的数据。这些方法的变种被广告用于CV和NLP中。例如：NCF被用于MLPerf benchmark的一部分，它使用MLP，而非dot product来计算MF中embeddings间的交叉。

## 2.2 DLRM架构

我们已经描述了在RS中不同的模型。我们将这些想法进行组合来构建SOTA的个性化模型。

假设用户和商品通过许多**连续型特征（continuous features）**和**类别型特征（categorical features）**进行描述。

- 为了处理categorical features，每个categorical feature可以通过一个相同维度的embedding vector表示，即MF中latent factors。
- 为了处理continous features，会通过一个MLP来进行转换，**它会生成和embedding vectors相同长度的dense representation**。

**我们将根据FMs提供的处理sparse data的方式，将它们传给MLPs，显式地(explicitly)计算不同特征间的二阶交叉（second-order interaction）**。这可以通过使用所有embedding vectors的pairs和dense features间的dot product来做到。用**这些dot products可以使另一个MLP(top 或 output MLP)将original-processed dense features和post-processed一起concatenated**，接着被feed到一个sigmoid function来提供一个概率。

<img src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/97d8441dcc3ffcd6175c13a9510b9c7c46aa8e9f04317925cbf2ede503382bf1dbd315a33dee5ba9212dbaaef7594bea?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=f1.jpg&amp;size=750">


我们将产生的模型称为：DLRM。如图1所示，并在表1中展示了PyTorch和Caffe2的DLRM所用到的一些operators。

<img src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/dc4ec4819daf1e4e3b7b0019d13fcd14ef6f9d67f7a7e2a21093cfb00c6cfa7e3c7a7e061367e97e10e6d5600818d3fc?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;uin=402636034&amp;fname=f2.jpg&amp;size=750">

表1

## 2.3 与之前的模型比较

许多deep learning-based的推荐模型，使用相似的底层思想来生成高阶项来处理sparse features。Wide&Deep, Deep&Cross, DeepFM, xDeepFM网络，例如，设计专有网络来有系统的构建高阶交叉。这些网络接着将来自这些专有模型和MLP的结果进行求和（sum），将它传给一个linear layer及sigmoid activation来生成一个最终概率。DLRM以一种结构化的方式与embeddings交互，通过只考虑由final MLP中embeddings pairs间的dot-product生成的cross-terms，模拟了FM对模型进行极大地降维。对于在其它网络的二阶交叉外的更高阶交叉，使用额外的计算/内存开销并不值。

DLRM和其它网络之间的一个关键不同点是：**这些网络是如何对待embedded feature vectors和它们的cross-terms的**。**DLRM(以及XDeepFM)会将每个feature vector看成单个unit来表示单个category**，其它像DCN(Deep&Cross)网络会将feature vector中的每个element看成是一个新的unit，这会生成不同的cross-terms。因此，Deep&Cross网络不仅会生成不同feature vectors的elements间的cross-terms（这和DLRM通过dot product方式一样），也会生成在相同feature vector的elements间的cross-terms，从而生成更高的维度。

# 3.并行化（Parallelism）

模型个性化和推荐系统，需要大且复杂的模型来估计大量数据上的价值。DLRMs特别包含了许多数目的参数，多阶的幅度要超过其它常见的deep learning模型（比如：CNN），transformer、RNN、GAN。这会导致训练时间上常达数周或更久。因此，对这些模型进行高效并行化，以便解决在实际规模中的问题。

如前面章节所示，**DLRMs会以成对（coupled）的方式，同时处理categorical features（使用embeddings）以及continuous features（使用bottom MLP）**。Embeddings会占据参数的大部分，一些tables每个都需要超过多个GBs的内存，使得DLRM对内存容量和带宽很敏感。embeddings的size使得它禁止使用数据并行化（data parallelism），因为它需要在每个设备上复制很大的embeddings。在许多cases中，这种内存限制需要模型分布跨多个设备，以便能满足内存容量需求。

在另一方面，MLP参数在内存上是更小的，但需要大量计算。因此，data-parallelism对MLPs更好，因为它可以让不同devices上的samples并发处理，只需要在当累积更新（accumulating updates）时需要通信。我们的并行化DLRM会使用一个embeddings的模型并行化(model parallelism)以及MLPs的数据并行化(data parallelism)的组合，来减缓由embeddings生成的内存瓶颈，而MLPs上的forward和backward propagations并行化。通过将model和data parallelism进行组合，是DLRM的唯一需求，因为它的架构和大模型size所导致；这样的组合并行化在Caffe2或PyTroch中并不支持（以及其它流行的DL框架），因此，我们设计了一种定制实现。我们计划在将来提供它的详细效果研究。

在我们的setup中，top MLP和interaction operator需要访问部分来自bottom MLP的mini-batch以及和所有embeddings。由于模型并行化已经被用于跨devices分布embeddings，这需要一个个性化的all-to-all通信。在embedding lookup的尾部，对于在mini-batch中的所有samples（必须根据mini-batch维度进行分割、以及与相应devieces进行通信）、对于在这些devices上的embedding tables，每个device都具有一个vector，如图2所示。Pytorch或Caffe2都不会提供model parallelism的原生支持；因此，我们通过显式将embedding operators（PyTorch的nn.EmbeddingBag, Caffe2的SparseLengthSum）映射到不同devices上来实现它。个性化的all-to-allcwpwy使用butterfly shuffle operator来实现，它可以将生成的embedding vectors进行切片（slices），并将它们转移到目标设备（target devices）上。在当前版本，这些transfers是显式的copies，但我们希望后续使用提供的通信原语（比如：all-gather以及send-recv）进一步optimize。

我们注意到，对于数据并行化MLPs，在backward pass中的参数更新会使用一个allreduce进行累积（accumulated），并以一种同步方式将它用在每个device的参数复制上，确保在每个device上的参数更新在每轮迭代上是一致的。在Pytorch中，data parallelism可以通过nn.DistributedDataParallel和nn.DataParallel模块来开启，将在每个device上的model复制，使用必要的依赖插入allreduce。在Caffe2中，我们会在梯度更新前手工插入allreduce。

# 4.数据

为了measure模型的acuracy，并测试它的整体效果，并将单独operators特征化，我们需要为我们的实现创建或获得一个dataset。我们模型的当前实现支持三种类型的datasets：random、synthetic、public datasets。

前两个dataset对于从系统角度实验我们的模型很有用。特别的，它允许我们通过生成即时数据，并移除数据存储依赖，来测试不同的硬件属性及瓶颈。后一个dataset允许我们执行真实数据的实验，并measure模型的accuracy。

## 4.1 Random

回顾DLRM，它接收continuous和categorical features作为inputs。前者可以通过生成一个随机数目的vector，通过使用一个uniform/normal(Gaussian)分布（numpy.random rand/randm缺省参数）。接着通过生成一个matrix来获得mini-batch inputs，其中每行对应在mini-batch中的一个element。

为了生成categorical features，我们需要决定在一个给定multi-hot vector中具有多少非零元素。benchmark允许该数字可以是fixed或在一个[1,k]的范围内random。接着，我们生成整型indices的相应数字，范围在[1,m]中，其中，m是在embedding W中的rows数目(2)。最后，为了创建一个mini-batch的lookups，我们将以上indices进行concatenate，并将每个单独的lookup使用lengths和offsets进行描述。

## 4.2 Synthetic

对应于categorical features，有许多理由支持定制索引的生成。例如，如果我们的应用使用一个特定dataset，但我们不希望出于私人目的共享它，那么我们可以选择通过distributions来表示categorical features。这可以潜在作为一种隐私保护技术的可选方法（用于联邦学习(federated learning)）。同时，如果我们希望练习系统组件（比如：学习内存行为）。。。

## 4.3 Public






# 参考

- 1.[https://arxiv.org/pdf/1906.00091.pdf](https://arxiv.org/pdf/1906.00091.pdf)
- 2.[https://github.com/facebookresearch/dlrm](https://github.com/facebookresearch/dlrm)