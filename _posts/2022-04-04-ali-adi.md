---
layout: post
title: ADIN multi-domain推荐算法介绍
description: 
modified: 2022-04-04
tags: 
---


阿里在《Adaptive Domain Interest Network for Multi-domain Recommendation》中提出了一种思路来解决multi-domain推荐问题：

# 摘要

工业界推荐系统通常会持有来自多个商业场景的数据，并同时为这些场景提供推荐服务。在检索阶段，topK的高质量items会从大量corpus中选出，通常对于多个场景来说是多样的。以alibaba展示广告系统为例，不仅因为淘宝用户的行为模式很多样，同时广告主对场景的竞价分配也非常多样。传统方法会针对不同场景独立训练模型，忽略掉user groups和items的cross-domain overlapping，或者简单将所有样本进行混合并使用一个共享模型（它很难捕获不同场景上的多样性）。在本paper中，我们提出Adaptive Domain Interest network，它会自适应处理不同场景的共性和差异，在训练期充分利用多场景数据。接着，**在在线inference期间，通过对不同场景给出不同的topK候选，提出的方法能够提升每个business domain的效果**。特别的，我们提出的ADI会通过**共享网络以及domain-specific networks**来建模不同domains的共性和差异。另外，我们会应用domain-specific batch normalization并设计domain interest adaptation layer进行**feature-level domain adaptation**。一个自训练策略（self training strategy）也会被包含进来捕获**跨domains的label-level connections**。ADI已经被部署到Alibaba的展示广告系统中，并且获得了1.8%的提升。

# 1.介绍

# 2.相关工作

# 3.前提

## 3.1 问题公式

在本节中，我们会对multi-domain retrieval任务进行公式化定义。**Multi-domain retrieval task的目标是：从一个非常大的corpus中，为multiple domains检索high-quality items**。更特别的，online multi-domain retrieval任务可以被公式化：

$$
S_{u,d} = \underset{v \in V}{arg \ Topk} \  f_{\theta}(v | u, d) 
$$

...(1)

其中：

- U和V: 分别表示user set 和**item set**
- d: 表示domain indicator
- $$f_{\theta}(v \mid u, d)$$: 是使用可训练参数$$\theta$$的estimated matching function，给定user u和domain indicator d后，用于measuring u到V的quality
- $$S_{u,d}$$: 是一个set，它包含了对应于$$f_{\theta}(v \mid u, d)$$的topK items的set

在neural-based retrieval模型中，学习这样的一个模型 $$f_{\theta}(v \mid u, d)$$可以看成是一个instance-level的分类问题。从V中胜出的postive item v的分布基于softmax function：

$$
s_{\theta}(v | u, d) = \frac{exp(f_{\theta}(v |u, d))}{\sum_{v' \in V} exp(f_{\theta}(v' \mid u, d))}
$$

...(2)

接着$$\theta$$会训练在训练数据上用来最小化negative log likelihood：$$log s_{\theta}(v \mid u, d)$$：

$$
\theta^{*} = \underset{\theta}{argmin} \sum\limits_d \sum\limits_u \sum\limits_{v \in B_{u,d}} - log s_{\theta}(v \mid u, d)
$$

...(3)

其中：

- $$B_{u,d}$$是在给定user u和domain indicator d后，与u的交叉items的集合

实际上，由于V通常相当大，sub-sampling被广泛用来减小等式(2)分母的计算复杂度。根据[3,6]，**我们会使用sampled softmax loss[33]**，并将等式(2)中的$$f_{\theta}(v \mid u, d)$$替换成：

$$
\bar{f}_{\theta} (v | u, d) = f_{\theta}(v | u, d) - log Q(v)
$$

...(4)

有了sub-sampling，我们有等式(5)。$$N_{u,d}$$是不相关items的集合，它从V中根据分布$$Q: V \rightarrow R$$进行采样，以便它的size可以满足$$\mid N_{u,d} \mid << \mid V \mid$$。

$$
\theta^{*} = argmin_{\theta} \sum_{d,u,v \in B_{u,d}} - \bar{f}_{\theta}(v | u, d) + log(exp(\bar{\theta}(v | u, d)) + \sum_{v' \in N_{u,t}} exp(\bar{f}_{\theta} (v' |u, t))
$$

...(5)

# 4.方法论

在本节中，我们会引入我们提出的方法来解决multi-domain retrieval问题。整个模型结构如图1所示。总模型结构被设计成：对于来自三个角度(angles)的不同domains的共性和不同。

- 首先，**后端网络(backbone network)**会从来自不同domains收集到的数据来抽取参数级共性（parameter-level）和多样性
- 第二，**domain adaptation方法**会学到feature-level diversities
- 第三，**self-training策略**会捕获label-level的共性

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/89e2def53cc1e0f89ab39510a87c9c44d20b7543b160866c1c2b54fbdb9af95280daacd77c4858686c00fccba16fb3b4?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=1.jpg&amp;size=750">

图1 ADI的整体架构展示。根据灰色箭头，一个样本会首先进行embedded，接着feed到Domain Interest Adaptation Layer，Shared Domain-Specific Network, Fusion Layer和Domain-Specific Forward Network。在通过user/item tower获得user/ item representations后，inner product会被生成，在最后会计算sampled softmax loss。domain indicator会被用来选择：使用哪个domain-related network

## 4.1 Backbone Network

为了有效学习来自不同domains的数据分布的共性与不同，我们会在底部使用设计shared networks和domain-specific networks，在顶部使用domain-specific forward networks。当处理multi-domain retrieval问题时，对比起普通DNN【3】、share-bottom network【28】以及MMoE【29】，这样的架构效果更好。它会在下面实验中证明。

### 4.1.1 Shared Embedding Layer

如表1所示，training/testing samples包含了丰富的feature信息。因此，第一阶段是，将这样的高维稀疏one-hot vectors转化成低维embedding vectors，所有domains会共享相同的embedding layers。

$$
F_i = EMBED(f_i) \\
F = concat(F_1 | \cdots | F_n)
$$

...(6)(7)

其中：

- $$F_i$$表示第i个embedded feature，F表示user/item inputs。

### 4.1.2 Shared Network & Domain-Specific Network

在获得encoded user representations和item representations后，我们会引入shared network和domain-specific network，如图1所示。受[18]启发，我们设计了shared network来学习由所有domains共享的representations，以及domain-specific network来在每个domain上学习domain-specific representations：

$$
\begin{align}
& a_k  = \frac{W_{shared}^k (f_{domain}) + b_{shared}^k}{\sum\limits_{n=1}^K (w_{shared}^n(f_{domain}) + b_{shared}^n)} \\
& E_{shared}  = \sum\limits_{k=1}^K a_k MLP_{shared}^k (F) \\
& E_{spec}^{(d)} = MLP_{spec}^{(d)} (F^{(d)})
\end{align}
$$

...(8)(9)(10)

其中：

- MLP表示multilayer perceptron，
- $$f_{domain}, F^{(d)}$$表示domain相关的features，数据从domain d中收集到。在我们的实践中，我们会使用domain indicator embedding作为$$f_{domain}$$。 
- $$W_{shared}^n, b_{shared}$$：是一个one-layer shallow neural network的weights和bias。

从所有domains中的数据会feed到shared networks中，而从domain d中的数据会feed到第d个domain-specific network中。更特别的，假设，存在来自D个domains的训练数据，我们会构建K个shared network以及D个specific network。FCs的总数是D+K

### 4.1.3 Fusion Layer

fusion layer的目标是学习一个来自Domain-Specific Network和Shared Network的最优化组合，它可以描述如下：

$$
\beta_1^{(d)} = \sigma (W_{fusion_spec}^{(d)} (f_{domain})) \\
\beta_2^{(d)} = \sigma (W_{fusion_shared}^{(d)} (f_{domain})) \\
E_{fusion}^{(d)} = concat(\beta_1^{(d)} E_{spec}^{(d)} | \beta_{1}^{(d)} \odot \beta_2^{(d) E_{shared} | \beta_2^{(d)} E_{shared}})
$$

...(11)(12)(13)

其中：

- $$\sigma$$表示sigmoid函数
- $$\odot$$表示hadamard product
- $$\beta_1^{(d)}, \beta_2^{(d)}$$表示分配给$$E_{spec}^{(d)}, E_{shared}$$feature weights。

我们将提出的fusion layer命名为：CONCAT version。因此，shared和specific network会为每个domain生成domain-related $$E_{fusion}^{(d)}$$。另外，我们会引入两个变种，它们是由MMoE、SAR-NET使用的SUM version，以及由STAR【11】提出的Network-Mul version。对于SUM version，我们会使用MMoE的gating network作为fusion layer。$$W_{gate}, b_{gate}$$表示gating network的weights和bias：

$$
a^{(d)} = \sigma(W_{gate}^{(d)}(f_{domain}) + b_{gate}) \\
E_{fusion}^{(d)} = \alpha^{(d)} E_{spec}^{(d)} + (1 - \alpha^{(d)}) E_{shared} 
$$

...(14)(15)

对于Network-Mul version，我们使用STAR-Topology FCN作为fusion layer。$$W_{shared}, b_{shared}, W_{spec}^{(d)}, b_{spec}^{(d)}$$分别表示在$$FC_{shared}$$和$$FC_{spec}$$中的参数：

$$
FC_{Net-Mul}(X) = (W_{shared} \odot W_{spec}^{(d)}) \cdot X + b_{shared} + b_{spec}^{(d)} \\
E_{fusion}^{(d)} = FC_{Net-Mul} (F^{(d)})
$$

在第5.3.1节中的实验表明，我们提出的CONCAT version会达到最好的效果，它会被用作fusion layer。

### 4.1.4 Domain-Specific Forward Network

在获得domain-related $$E_{fusion}^{(d)}$$后，最后outputs会feed到domain-related forward network中，描述如下：

$$
E = FC_{forward}^{(d)} (E_{fusion}^{(d)})
$$

...(18)

由user tower以及item tower生产的output E会被用作随后的inner product和sampled softmax计算。

## 4.2 Domain Adaptation

在multi-domain推荐任务中，我们提供了两种方法来解决domain adaptation问题：domain-specific batch normalization和domain intereset adaptation layer。

<img alt="图片名称" src="https://picabstract-preview-ftn.weiyun.com/ftn_pic_abs_v3/0e95d78eb11381e3d20215624a2d1f556ff8ae68dd22079c9a959f420b8ec2cd1d15d44caedaad233bb24b175a6632ed?pictype=scale&amp;from=30113&amp;version=3.3.3.3&amp;fname=2.jpg&amp;size=750">

图2

### 4.2.1 Domain-Specific Batch Normalization

batch normalization技术已经广泛被用于训练非常深的neural network。假设：$$\mu$$表示input X的均值，而$$\sigma^2$$表示方差。batch normalization方法可以描述如下：

$$
\hat{X} = \alpha \frac{X-\mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

...(19)

其中：$$\alpha$$和$$\beta$$是可学习的参数，$$\epsilon$$是非常小的quantity，以避免分母为0. BN假设input X会满足i.i.d的假设，它会在单个场景上有效。然而，multi-domain retrieval问题会面临着一个混合数据分布的问题。计算全局的BN参数，并忽略在不同domain间的静态差异，可能会损伤最终效果。受[35]的启发，我们会使用domain-specific batch normalization（DSBN）来解决提到的问题：

$$
\hat{X}^{(d)} = \alpha^{(d)} \frac{X^{(d)} - \mu^{(d)}}{\sqrt{(\sigma^{(d)})^2 + \epsilon}} + \beta^{(d)}
$$

...(20)

其中：

- $$X^{(d)} \in X$$ 表示：来自domain d的样本。

通过估计在batch normalization中的domain-specific batch统计：$$\mu^{(d)}, (\sigma^{(d)})^2, \alpha^{(d)}, \beta^{(d)}$$，我们相信：该模型可以捕获domain-specific信息。

### 4.2.2 Domain Interst Adaptation Layer

domain interest apaptation layer来自直觉，不同domains假设只关注在raw features的不同部分。我们实现了三种类型的domain interest adaptation layer：linear domain transformation, vanilla domain attention，以及SE-Block-based domain attention：

Linear domain transformation：[14]使用的Linear domain transformation会将original features映射到domain-related features中。假如：$$F_i^{(d)}$$表示来自domain d的embedded input的第i个feature，$$W^{(d)}, b^{(d)}$$共享着与input $$F^{(d)}$$相同的维度。Linear domain transformation方法的描述如下：


$$
\alpha_i^{(d)} = \sigma(Q_i^{(d)} F_i^{(d)}) \\
\hat{F}^{(d)} = concat(\alpha_1^{(d)} F_1^{(d)} | \cdots | \alpha_n^{(d)} F_N^{(d)})
$$

...(23)(24)

SE-Block based domain attention：Squeeze-and-Excitation Network (SE-Net) 【36】在许多计算机视觉任务上达到了SOTA的结果。我们会讨论SE-Block是另一种形式的attention机制，它可以捕获在不同domains下的特征重要性差异。$$F_{se}$$表示一个(FC, Relu, FC) block和$$F_{avg}$$表示average pooling操作符。$$\alpha^{(d)}$$表示domain d下的N维的SE attention scores vector。


$$
F^{(d)} = concat(F_1^{(d)} | \cdots | F_N^{(d)}) \\
\hat{F}^{(d)} = \alpha^{(d)} \odot concat(F_1^{(d)} | \cdots | F_N^{(d)})
$$

...(25)(26)

SE-Block based domain adaptation layer会为不同domains学习不同domain attention weights，并以一种轻量且高效的方式来迁移cross-domain知识。

通过添加domain interest adaptation layer给backbone network，raw features会迁移到domain-related features中。在第5.3节中的实验和可视化表明：提出的domain interest adaptation layer。

## 4.3 Self Training

self training方法已经证明是一种有效的学习策略，可以在模型训练期利用unlabeled data。我们会应用该技术在multi-domain推荐的retrieval step中，有两个原因：

- 1) 在训练数据中，当在domains间存在数据重合时，有一个潜在的label-level connection。为了更准确，在一个domain中的与user交叉的一个item，在另一个domain中仍会被相同的一个user进行交叉。该假设是有效的，特别是当更大的domain会帮助那些labeled data有限的小domains或新domains。
- 2) 添加pseudo-labeld data到训练中，必然会变更原始的数据分布，然而，我们会讨论我们提出的self-training方法更适合于召回模型(retrieval models)，而非ranking models. 在广告系统中的ranking models需要预测准确的CTR scores】【38】，添加额外的pseudo-labeled data可能会导致未知的效果，因为数据分布已经变化，CTR模型对数据分布非常敏感。然而，广告系统的retrieval models的目标是提供candidates set给下流任务。换句话说，对于retrieval models不需要精准的CTR score，因为多个candidates会平等的生成。因此，额外的潜在兴趣信号可以添加到model中，即使对于生成高质量topK candidates来说数据分布会发生微弱变化。已经存在的方法主要关注sample-level【32】，feature level【14】，parameter level【11】转换，从而忽略label-level transferring。因此，我们提出这种有效的self training方法，通过domains来挖掘潜在的label-level transferring knowledge，它被证明是有效的。

给定一个item v与user u在domain d上交叉，self training方法遵循以下两个steps：

- a) 对于在除了domain d外的其它domains v，freeze住模型来生成pseudo-labels
- b) freeze住pseduo-labels，接着fine-tune模型。

根据算法 1，对于每个step，我们会选择具有最高置信分的pseudo-labels，并且在训练期间选择部分（selection portion）会渐近增加。最后对于在其它domains中的v获得pseudo-labels，等式(3)中的$$\theta$$被训练来最小化在训练数据和pseudo-labeled data上的negative log likelihood：$$log s_{\theta}(v \mid u, d)$$：

$$
\theta^* = argmin_{\theta} \sum_d \sum_u \sum_{v \in B_{u,d}} - (log s_{\theta} (v | u, d) + log s_{\theta}(\bar{v} | u, d))
$$

...(27)

其中，$$\bar{v}$$是在给定user u和domain d下，选中的潜在postive pseudo-items。



表3 

# 5.实验

略



- 1.[https://arxiv.org/pdf/2206.09672.pdf](https://arxiv.org/pdf/2206.09672.pdf)